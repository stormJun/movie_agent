# çŸ¥è¯†å›¾è°±æ„å»º

---

## ğŸ“‹ å…ƒä¿¡æ¯

- **ç›®æ ‡è¯»è€…**ï¼šå¼€å‘è€…ã€æ¶æ„å¸ˆ
- **é˜…è¯»æ—¶é—´**ï¼š60åˆ†é’Ÿ
- **éš¾åº¦**ï¼šâ­â­â­
- **å‰ç½®çŸ¥è¯†**ï¼šNeo4jã€å›¾æ•°æ®åº“ã€LLMã€å‘é‡åµŒå…¥
- **æœ€åæ›´æ–°**ï¼š2026-01-04

---

## ğŸ“– æœ¬æ–‡å¤§çº²

- [ç³»ç»Ÿæ¦‚è§ˆ](#ç³»ç»Ÿæ¦‚è§ˆ)
- [å®Œæ•´æ„å»ºæµç¨‹](#å®Œæ•´æ„å»ºæµç¨‹)
- [æ–‡æ¡£æ‘„å–ä¸åˆ†å—](#æ–‡æ¡£æ‘„å–ä¸åˆ†å—)
- [å®ä½“å…³ç³»æå–](#å®ä½“å…³ç³»æå–)
- [å®ä½“æ¶ˆæ­§æœºåˆ¶](#å®ä½“æ¶ˆæ­§æœºåˆ¶)
- [å®ä½“å¯¹é½æœºåˆ¶](#å®ä½“å¯¹é½æœºåˆ¶)
- [Neo4j å›¾è°±å­˜å‚¨](#neo4j-å›¾è°±å­˜å‚¨)
- [ç¤¾åŒºæ£€æµ‹](#ç¤¾åŒºæ£€æµ‹)
- [å‘é‡ç´¢å¼•æ„å»º](#å‘é‡ç´¢å¼•æ„å»º)
- [å¢é‡ vs å…¨é‡æ„å»º](#å¢é‡-vs-å…¨é‡æ„å»º)
- [é…ç½®å‚æ•°è¯¦è§£](#é…ç½®å‚æ•°è¯¦è§£)
- [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
- [ç›¸å…³æ–‡æ¡£](#ç›¸å…³æ–‡æ¡£)

---

## ç³»ç»Ÿæ¦‚è§ˆ

### æ ¸å¿ƒç†å¿µ

çŸ¥è¯†å›¾è°±æ„å»ºæ˜¯æœ¬é¡¹ç›®çš„æ ¸å¿ƒåŸºç¡€è®¾æ–½ï¼Œè´Ÿè´£å°†éç»“æ„åŒ–æ–‡æ¡£è½¬æ¢ä¸ºç»“æ„åŒ–çš„çŸ¥è¯†å›¾è°±ã€‚

**æ ¸å¿ƒç›®æ ‡**ï¼š
1. **é«˜è´¨é‡å®ä½“æå–**ï¼šå‡†ç¡®è¯†åˆ«æ–‡æ¡£ä¸­çš„å®ä½“å’Œå…³ç³»
2. **æ¶ˆé™¤æ­§ä¹‰**ï¼šé€šè¿‡å¤šé˜¶æ®µæ¶ˆæ­§ï¼Œå°† mention æ˜ å°„åˆ°è§„èŒƒå®ä½“
3. **å†²çªè§£å†³**ï¼šæ£€æµ‹å¹¶è§£å†³å®ä½“å†²çªï¼Œä¿è¯å›¾è°±ä¸€è‡´æ€§
4. **å¯æ‰©å±•æ€§**ï¼šæ”¯æŒå¢é‡æ›´æ–°ï¼Œé€‚åº”å¤§è§„æ¨¡çŸ¥è¯†åº“

### æ¶æ„å±‚çº§

```mermaid
graph TB
    subgraph æ•°æ®å±‚[æ•°æ®å±‚]
        F[æ–‡æ¡£æ–‡ä»¶<br/>TXT/PDF/MD/DOCX]
    end

    subgraph æ‘„å–å±‚[æ‘„å–å±‚ Ingestion]
        DP[DocumentProcessor<br/>æ–‡æ¡£è§£æ]
        TC[TextChunker<br/>æ–‡æœ¬åˆ†å—]
    end

    subgraph æå–å±‚[æå–å±‚ Extraction]
        EE[EntityExtractor<br/>å®ä½“å…³ç³»æå–]
        ED[EntityDisambiguator<br/>å®ä½“æ¶ˆæ­§]
        EA[EntityAligner<br/>å®ä½“å¯¹é½]
    end

    subgraph å­˜å‚¨å±‚[å­˜å‚¨å±‚ Storage]
        GW[GraphWriter<br/>å›¾è°±å†™å…¥]
        Neo[(Neo4j<br/>å›¾æ•°æ®åº“)]
    end

    subgraph ç´¢å¼•å±‚[ç´¢å¼•å±‚ Indexing]
        EI[EntityIndex<br/>å®ä½“å‘é‡ç´¢å¼•]
        CI[ChunkIndex<br/>å—å‘é‡ç´¢å¼•]
        CD[CommunityDetection<br/>ç¤¾åŒºæ£€æµ‹]
    end

    F --> DP
    DP --> TC
    TC --> EE
    EE --> ED
    ED --> EA
    EA --> GW
    GW --> Neo

    Neo --> EI
    Neo --> CI
    Neo --> CD

    style æ‘„å–å±‚ fill:#e3f2fd
    style æå–å±‚ fill:#fff3e0
    style å­˜å‚¨å±‚ fill:#e8f5e9
    style ç´¢å¼•å±‚ fill:#fce4ec
```

---

## å®Œæ•´æ„å»ºæµç¨‹

### ä¸»æµç¨‹å›¾

```mermaid
graph TB
    Start([å¼€å§‹]) --> Init[åˆå§‹åŒ–ç»„ä»¶]

    Init --> DP[æ–‡æ¡£å¤„ç†<br/>Document Processing]
    DP --> TC[æ–‡æœ¬åˆ†å—<br/>Text Chunking]

    TC --> EE[å®ä½“å…³ç³»æå–<br/>Entity Extraction]
    EE --> Cache{ç¼“å­˜æ£€æŸ¥}
    Cache -->|å‘½ä¸­| Skip[è·³è¿‡LLMè°ƒç”¨]
    Cache -->|æœªå‘½ä¸­| LLM[LLMæå–]

    Skip --> GS[å›¾ç»“æ„æ„å»º]
    LLM --> GS

    GS --> GW[å†™å…¥Neo4j]
    GW --> ED[å®ä½“æ¶ˆæ­§<br/>Disambiguation]

    ED --> EA[å®ä½“å¯¹é½<br/>Alignment]
    EA --> CD[ç¤¾åŒºæ£€æµ‹<br/>Community Detection]

    CD --> EI[å®ä½“ç´¢å¼•æ„å»º<br/>Entity Index]
    EI --> CI[Chunkç´¢å¼•æ„å»º<br/>Chunk Index]

    CI --> End([å®Œæˆ])

    style DP fill:#e3f2fd
    style EE fill:#fff3e0
    style ED fill:#ffe1f5
    style CD fill:#e8f5e9
```

### å…³é”®é˜¶æ®µæ—¶åº

```mermaid
sequenceDiagram
    participant U as ç”¨æˆ·
    participant KB as KnowledgeGraphBuilder
    participant DP as DocumentProcessor
    participant EE as EntityExtractor
    participant DB as Neo4j
    participant ED as EntityDisambiguator
    participant CD as CommunityDetector

    U->>KB: å¯åŠ¨æ„å»º
    KB->>DP: è¯»å–æ–‡æ¡£

    DP-->>KB: è¿”å›æ–‡æ¡£+Chunks

    KB->>EE: æ‰¹é‡æå–å®ä½“å…³ç³»
    Note over EE: å¹¶è¡Œå¤„ç† (MAX_WORKERS)
    EE-->>KB: è¿”å›å®ä½“å…³ç³»å›¾

    KB->>DB: æ‰¹é‡å†™å…¥å›¾è°±
    Note over DB: Document/Chunk/Entity/Relationship

    KB->>ED: å®ä½“æ¶ˆæ­§
    Note over ED: å­—ç¬¦ä¸²å¬å›â†’å‘é‡é‡æ’â†’NILæ£€æµ‹
    ED-->>DB: æ›´æ–° canonical_id

    KB->>DB: å®ä½“å¯¹é½
    Note over DB: å†²çªæ£€æµ‹ä¸è§£å†³

    KB->>CD: ç¤¾åŒºæ£€æµ‹
    Note over CD: Leiden/SLLPA ç®—æ³•
    CD-->>DB: å†™å…¥ç¤¾åŒºç»“æ„

    KB->>DB: æ„å»ºå‘é‡ç´¢å¼•
    Note over DB: å®ä½“ç´¢å¼• + Chunkç´¢å¼•

    DB-->>U: æ„å»ºå®Œæˆ
```

### æ•°æ®æµè½¬

```mermaid
graph LR
    subgraph åŸå§‹æ•°æ®[åŸå§‹æ•°æ®]
        F1[document1.pdf]
        F2[document2.txt]
    end

    subgraph Chunks[åˆ†å—æ•°æ®]
        C1[Chunk 1<br/>512 tokens]
        C2[Chunk 2<br/>512 tokens]
        C3[Chunk 3<br/>512 tokens]
    end

    subgraph æå–ç»“æœ[æå–ç»“æœ]
        E1[å®ä½“: å­¦ç”Ÿ]
        E2[å®ä½“: å¥–å­¦é‡‘]
        R1[å…³ç³»: ç”³è¯·]
    end

    subgraph å›¾è°±èŠ‚ç‚¹[å›¾è°±èŠ‚ç‚¹]
        D[DocumentèŠ‚ç‚¹]
        CH[ChunkèŠ‚ç‚¹]
        EN[EntityèŠ‚ç‚¹]
        CO[CommunityèŠ‚ç‚¹]
    end

    F1 --> C1
    F1 --> C2
    F2 --> C3

    C1 --> E1
    C1 --> R1
    C2 --> E2

    E1 --> EN
    E2 --> EN
    R1 --> EN

    C1 --> CH
    C2 --> CH
    C3 --> CH

    CH --> D
    EN --> CO

    style åŸå§‹æ•°æ® fill:#e3f2fd
    style Chunks fill:#fff3e0
    style æå–ç»“æœ fill:#ffe1f5
    style å›¾è°±èŠ‚ç‚¹ fill:#e8f5e9
```

---

## æ–‡æ¡£æ‘„å–ä¸åˆ†å—

### DocumentProcessor

**æ ¸å¿ƒåŠŸèƒ½**ï¼šæ”¯æŒå¤šæ ¼å¼æ–‡æ¡£è§£æå’Œæ–‡æœ¬æå–ã€‚

**æ”¯æŒçš„æ–‡ä»¶æ ¼å¼**ï¼š
- TXT
- PDF
- Markdown (MD)
- Word (DOCX, DOC)
- CSV
- JSON
- YAML

**å¤„ç†æµç¨‹**ï¼š

```mermaid
graph LR
    F[æ–‡ä»¶] --> DT{æ£€æµ‹ç±»å‹}

    DT -->|.txt| TXT[æ–‡æœ¬è¯»å–å™¨]
    DT -->|.pdf| PDF[PDFè§£æå™¨]
    DT -->|.md| MD[Markdownè§£æå™¨]
    DT -->|.docx| DOCX[Wordè§£æå™¨]
    DT -->|.csv| CSV[CSVè¯»å–å™¨]
    DT -->|.json| JSON[JSONè§£æå™¨]

    TXT --> Clean[æ–‡æœ¬æ¸…æ´—]
    PDF --> Clean
    MD --> Clean
    DOCX --> Clean
    CSV --> Clean
    JSON --> Clean

    Clean --> Output[æ ‡å‡†åŒ–æ–‡æœ¬]

    style DT fill:#fce4ec
    style Clean fill:#e8f5e9
```

**æ ¸å¿ƒä»£ç **ï¼š

```python
class DocumentProcessor:
    def __init__(self, files_dir: str, chunk_size: int = 512, overlap: int = 50):
        self.files_dir = files_dir
        self.chunk_size = chunk_size
        self.overlap = overlap
        self.chunker = TextChunker(chunk_size, overlap)

    def process_all_files(self) -> List[Tuple]:
        """å¤„ç†æ‰€æœ‰æ–‡ä»¶"""
        files = self._list_files()
        results = []

        for file_path in files:
            # è¯»å–æ–‡ä»¶å†…å®¹
            content = self._read_file(file_path)

            # åˆ†å—
            chunks = self.chunker.chunk(content)

            # ç”Ÿæˆå”¯ä¸€ID
            doc_id = generate_hash(file_path)

            results.append((doc_id, file_path, chunks))

        return results

    def _read_file(self, file_path: str) -> str:
        """æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©è¯»å–å™¨"""
        ext = os.path.splitext(file_path)[1].lower()

        if ext == '.pdf':
            return self._read_pdf(file_path)
        elif ext in ['.docx', '.doc']:
            return self._read_word(file_path)
        elif ext == '.txt':
            return self._read_text(file_path)
        elif ext == '.md':
            return self._read_markdown(file_path)
        elif ext == '.csv':
            return self._read_csv(file_path)
        elif ext == '.json':
            return self._read_json(file_path)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {ext}")
```

### TextChunker

**åˆ†å—ç­–ç•¥**ï¼šæ»‘åŠ¨çª—å£ + é‡å 

```mermaid
graph TB
    Text[åŸå§‹æ–‡æœ¬<br/>3000 tokens]

    Text --> C1[Chunk 1<br/>0-512]
    Text --> C2[Chunk 2<br/>462-974<br/>Overlap: 50]
    Text --> C3[Chunk 3<br/>924-1436<br/>Overlap: 50]
    Text --> C4[Chunk 4<br/>1386-1898<br/>Overlap: 50]

    style C1 fill:#e3f2fd
    style C2 fill:#fff3e0
    style C3 fill:#e8f5e9
    style C4 fill:#fce4ec
```

**æ ¸å¿ƒä»£ç **ï¼š

```python
class TextChunker:
    def __init__(self, chunk_size: int = 512, overlap: int = 50):
        self.chunk_size = chunk_size
        self.overlap = overlap

    def chunk(self, text: str) -> List[str]:
        """åˆ†å—æ–‡æœ¬"""
        # ä½¿ç”¨ tiktoken è¿›è¡Œ token çº§åˆ«çš„åˆ†å—
        encoding = tiktoken.get_encoding("cl100k_base")
        tokens = encoding.encode(text)

        chunks = []
        start = 0

        while start < len(tokens):
            end = min(start + self.chunk_size, len(tokens))
            chunk_tokens = tokens[start:end]

            # è§£ç ä¸ºæ–‡æœ¬
            chunk_text = encoding.decode(chunk_tokens)
            chunks.append(chunk_text)

            # ä¸‹ä¸€ä¸ªå—çš„èµ·å§‹ä½ç½®ï¼ˆè€ƒè™‘é‡å ï¼‰
            start += self.chunk_size - self.overlap

        return chunks
```

**é…ç½®å‚æ•°**ï¼š
```env
# .env
CHUNK_SIZE=512          # æ¯ä¸ªå—çš„å¤§å°ï¼ˆtokenæ•°ï¼‰
OVERLAP=50              # å—ä¹‹é—´çš„é‡å ï¼ˆtokenæ•°ï¼‰
```

**é‡å çš„ä½œç”¨**ï¼š
- é¿å…å®ä½“è¢«åˆ‡æ–­
- ä¿ç•™ä¸Šä¸‹æ–‡è¿è´¯æ€§
- æé«˜æå–å‡†ç¡®ç‡

---

## å®ä½“å…³ç³»æå–

### LLM æç¤ºå·¥ç¨‹

**ç³»ç»Ÿæç¤ºæ¨¡æ¿**ï¼š

```python
system_template_build_graph = """
ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†å›¾è°±æ„å»ºä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯ä»ç»™å®šæ–‡æœ¬ä¸­æå–å®ä½“å’Œå…³ç³»ã€‚

**å®ä½“ç±»å‹**ï¼š{entity_types}
**å…³ç³»ç±»å‹**ï¼š{relationship_types}

**è¾“å‡ºæ ¼å¼**ï¼š
æ¯è¡Œä¸€ä¸ªä¸‰å…ƒç»„ï¼Œæ ¼å¼ä¸ºï¼šå®ä½“1 : å…³ç³» : å®ä½“2

**ç¤ºä¾‹**ï¼š
å­¦ç”Ÿ : ç”³è¯· : å¥–å­¦é‡‘
å¥–å­¦é‡‘ : ç”±...è¯„é€‰ : è¯„å®¡å§”å‘˜ä¼š

**è¦æ±‚**ï¼š
1. ä»…æå–æ˜ç¡®å­˜åœ¨çš„å®ä½“å’Œå…³ç³»
2. å®ä½“åç§°ä½¿ç”¨è§„èŒƒåŒ–å½¢å¼
3. é¿å…é‡å¤æå–
4. æ¯è¡Œä»…åŒ…å«ä¸€ä¸ªä¸‰å…ƒç»„
"""
```

**äººç±»æç¤ºæ¨¡æ¿**ï¼š

```python
human_template_build_graph = """
è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–å®ä½“å’Œå…³ç³»ï¼š

{text}

è¯·æŒ‰ç…§æŒ‡å®šæ ¼å¼è¾“å‡ºã€‚
"""
```

### æ‰¹é‡å¹¶è¡Œæå–

**æ¶æ„è®¾è®¡**ï¼š

```mermaid
graph TB
    Chunks[æ‰€æœ‰Chunks<br/>1000ä¸ª] --> Batcher[æ‰¹å¤„ç†å™¨<br/>Batch Size: 5]

    Batcher --> B1[Batch 1<br/>5 chunks]
    Batcher --> B2[Batch 2<br/>5 chunks]
    Batcher --> B3[Batch 3<br/>5 chunks]

    subgraph çº¿ç¨‹æ± [çº¿ç¨‹æ±  ThreadPoolExecutor]
        W1[Worker 1]
        W2[Worker 2]
        W3[Worker 3]
        W4[Worker 4]
    end

    B1 --> W1
    B2 --> W2
    B3 --> W3

    W1 --> LLM1[LLMè°ƒç”¨]
    W2 --> LLM2[LLMè°ƒç”¨]
    W3 --> LLM3[LLMè°ƒç”¨]

    LLM1 --> Result[æå–ç»“æœ]
    LLM2 --> Result
    LLM3 --> Result

    style Batcher fill:#fff3e0
    style çº¿ç¨‹æ±  fill:#e3f2fd
```

**æ ¸å¿ƒä»£ç **ï¼š

```python
class EntityRelationExtractor:
    def __init__(self, llm, system_template, human_template,
                 entity_types, relationship_types,
                 max_workers=4, batch_size=5):
        self.llm = llm
        self.max_workers = max_workers
        self.batch_size = batch_size

        # åˆ›å»ºæç¤ºæ¨¡æ¿
        self.chat_prompt = ChatPromptTemplate.from_messages([
            SystemMessagePromptTemplate.from_template(system_template),
            MessagesPlaceholder("chat_history"),
            HumanMessagePromptTemplate.from_template(human_template)
        ])

        self.chain = self.chat_prompt | self.llm

    def process_chunks(self, file_contents: List[Tuple],
                      progress_callback=None) -> List[Tuple]:
        """å¹¶è¡Œå¤„ç†æ‰€æœ‰æ–‡ä»¶çš„æ‰€æœ‰ chunks"""
        t0 = time.time()
        total_chunks = sum(len(fc[2]) for fc in file_contents)

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = []

            for doc_id, file_name, chunks in file_contents:
                for chunk_index, chunk in enumerate(chunks):
                    future = executor.submit(
                        self._extract_from_chunk,
                        chunk
                    )
                    futures.append((doc_id, file_name, chunk_index, future))

            # æ”¶é›†ç»“æœ
            results = []
            for doc_id, file_name, chunk_index, item in futures:
                result = item.result()

                results.append((doc_id, file_name, chunk_index, result))

                if progress_callback:
                    progress_callback()

        print(f"æå–å®Œæˆï¼Œè€—æ—¶: {time.time() - t0:.2f}ç§’")

        return results

    @retry(max_attempts=3, delay=2)
    def _extract_from_chunk(self, chunk: str) -> str:
        """ä»å•ä¸ª chunk æå–å®ä½“å…³ç³»"""
        # è°ƒç”¨ LLMï¼ˆå¸¦è¶…æ—¶ï¼‰
        with _alarm_timeout(OPENAI_REQUEST_TIMEOUT_SECONDS):
            response = self.chain.invoke({
                "text": chunk,
                "chat_history": []
            })

        result = response.content

        return result
```

### è§£ææå–ç»“æœ

**åŸå§‹ LLM è¾“å‡º**ï¼š
```
å­¦ç”Ÿ : ç”³è¯· : å›½å®¶å¥–å­¦é‡‘
å›½å®¶å¥–å­¦é‡‘ : è¯„é€‰ : è¯„å®¡å§”å‘˜ä¼š
å­¦ç”Ÿ : è¿å : å­¦æ ¡è§„å®š
å­¦æ ¡è§„å®š : ç®¡ç† : å­¦ç”Ÿè¡Œä¸º
```

**è§£ææµç¨‹**ï¼š

```python
def parse_llm_output(output: str) -> Dict[str, List]:
    """è§£æ LLM è¾“å‡ºä¸ºç»“æ„åŒ–æ•°æ®"""
    entities = set()
    relationships = []

    lines = output.strip().split('\n')

    for line in lines:
        parts = line.split(' : ')
        if len(parts) != 3:
            continue  # è·³è¿‡æ ¼å¼é”™è¯¯çš„è¡Œ

        entity1, relation, entity2 = [p.strip() for p in parts]

        # æ·»åŠ å®ä½“
        entities.add(entity1)
        entities.add(entity2)

        # æ·»åŠ å…³ç³»
        relationships.append({
            'source': entity1,
            'target': entity2,
            'type': relation
        })

    return {
        'entities': list(entities),
        'relationships': relationships
    }
```

### æ€§èƒ½ä¼˜åŒ–

æœ¬é¡¹ç›® v3 strict é˜¶æ®µä¸å†æä¾›å›¾è°±æ„å»ºçš„æœ¬åœ°è½ç›˜ç¼“å­˜ï¼›æ€§èƒ½ä¸»è¦ä¾èµ–å¹¶å‘ã€æ‰¹å¤„ç†ä¸è¶…æ—¶æ§åˆ¶ã€‚

**æ‰¹å¤„ç†é…ç½®**ï¼š
```env
MAX_WORKERS=4          # å¹¶è¡Œçº¿ç¨‹æ•°
LLM_BATCH_SIZE=5       # æ¯æ‰¹å¤„ç†çš„ chunk æ•°
BATCH_SIZE=100         # æ•°æ®åº“æ‰¹å¤„ç†å¤§å°
```

**è¶…æ—¶ä¿æŠ¤**ï¼š
```python
@contextmanager
def _alarm_timeout(seconds: float):
    """é€šè¿‡ SIGALRM å¼ºåˆ¶é™åˆ¶é˜»å¡è°ƒç”¨çš„æœ€é•¿è€—æ—¶"""
    def _handler(signum, frame):
        raise LlmInvokeTimeoutError(f"LLM invoke è¶…æ—¶ï¼ˆ>{seconds}sï¼‰")

    signal.signal(signal.SIGALRM, _handler)
    signal.setitimer(signal.ITIMER_REAL, float(seconds))
    try:
        yield
    finally:
        signal.setitimer(signal.ITIMER_REAL, 0)
```

---

## å®ä½“æ¶ˆæ­§æœºåˆ¶

### ä¸‰é˜¶æ®µæ¶ˆæ­§æµç¨‹

```mermaid
graph TB
    M["Mention<br/>ä¼˜ç§€å­¦ç”Ÿå¥–å­¦é‡‘"] --> S1["é˜¶æ®µ1 - å­—ç¬¦ä¸²å¬å›<br/>String Recall"]

    S1 --> C1["å€™é€‰1: ä¼˜ç§€å­¦ç”Ÿ<br/>similarity: 0.85"]
    S1 --> C2["å€™é€‰2: å›½å®¶å¥–å­¦é‡‘<br/>similarity: 0.75"]
    S1 --> C3["å€™é€‰3: ä¼˜ç§€å­¦ç”Ÿå¥–å­¦é‡‘<br/>similarity: 1.0"]

    C1 --> S2["é˜¶æ®µ2 - å‘é‡é‡æ’<br/>Vector Rerank"]
    C2 --> S2
    C3 --> S2

    S2 --> R1["å€™é€‰3: ä¼˜ç§€å­¦ç”Ÿå¥–å­¦é‡‘<br/>combined: 0.95<br/>vec: 0.92 + str: 0.85"]
    S2 --> R2["å€™é€‰1: ä¼˜ç§€å­¦ç”Ÿ<br/>combined: 0.72<br/>vec: 0.65 + str: 0.85"]

    R1 --> S3["é˜¶æ®µ3 - NILæ£€æµ‹<br/>NIL Detection"]
    R2 --> S3

    S3 --> Check{"æ£€æŸ¥æœ€ä½³å€™é€‰åˆ†æ•°"}
    Check -->|combined >= 0.6| Match["åŒ¹é…æˆåŠŸ<br/>canonical_id = ä¼˜ç§€å­¦ç”Ÿå¥–å­¦é‡‘"]
    Check -->|combined < 0.6| NIL["NILåˆ¤å®š<br/>åˆ›å»ºæ–°å®ä½“"]

    style M fill:#e8f5e9
    style S1 fill:#e3f2fd
    style S2 fill:#fff3e0
    style S3 fill:#ffe1f5
    style Match fill:#c8e6c9
    style NIL fill:#ffcdd2
    style Check fill:#fff9c4
```

### é˜¶æ®µ1ï¼šå­—ç¬¦ä¸²å¬å›

**ç›®æ ‡**ï¼šå¿«é€Ÿå¬å›å¯èƒ½åŒ¹é…çš„å€™é€‰å®ä½“ã€‚

**ç®—æ³•**ï¼šLevenshtein ç¼–è¾‘è·ç¦»

#### ç®—æ³•åŸç†

**å®šä¹‰**ï¼šLevenshtein è·ç¦»è¡¡é‡ä¸¤ä¸ªå­—ç¬¦ä¸²ä¹‹é—´çš„"ç¼–è¾‘è·ç¦»"ï¼Œå³å°†ä¸€ä¸ªå­—ç¬¦ä¸²è½¬æ¢ä¸ºå¦ä¸€ä¸ªå­—ç¬¦ä¸²æ‰€éœ€çš„æœ€å°‘å•å­—ç¬¦ç¼–è¾‘æ“ä½œæ¬¡æ•°ã€‚

**å…è®¸çš„æ“ä½œ**ï¼š
1. **æ’å…¥**ï¼ˆInsertionï¼‰ï¼šåœ¨ä»»æ„ä½ç½®æ’å…¥ä¸€ä¸ªå­—ç¬¦
2. **åˆ é™¤**ï¼ˆDeletionï¼‰ï¼šåˆ é™¤ä»»æ„ä¸€ä¸ªå­—ç¬¦
3. **æ›¿æ¢**ï¼ˆSubstitutionï¼‰ï¼šå°†ä¸€ä¸ªå­—ç¬¦æ›¿æ¢ä¸ºå¦ä¸€ä¸ªå­—ç¬¦

**æ•°å­¦å…¬å¼**ï¼š

ç»™å®šä¸¤ä¸ªå­—ç¬¦ä¸² $a$ å’Œ $b$ï¼Œé•¿åº¦åˆ†åˆ«ä¸º $|a|$ å’Œ $|b|$ï¼ŒLevenshtein è·ç¦» $\text{lev}(a, b)$ å®šä¹‰ä¸ºï¼š

$$
\text{lev}(a, b) =
\begin{cases}
|a| & \text{if } |b| = 0 \\
|b| & \text{if } |a| = 0 \\
\text{lev}(a[0:], b[0:]) & \text{if } a[0] = b[0] \\
1 + \min
\begin{cases}
\text{lev}(a[1:], b) & \text{åˆ é™¤ } a[0] \\
\text{lev}(a, b[1:]) & \text{æ’å…¥ } b[0] \\
\text{lev}(a[1:], b[1:]) & \text{æ›¿æ¢ } a[0] \text{ ä¸º } b[0]
\end{cases} & \text{otherwise}
\end{cases}
$$

**ç›¸ä¼¼åº¦è½¬æ¢**ï¼š

åœ¨å®ä½“æ¶ˆæ­§ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨**ç›¸ä¼¼åº¦**è€Œéè·ç¦»ï¼Œè½¬æ¢å…¬å¼ä¸ºï¼š

$$
\text{similarity}(a, b) = 1 - \frac{\text{lev}(a, b)}{\max(|a|, |b|)}
$$

å…¶ä¸­ï¼š
- $\text{lev}(a, b)$ï¼šç¼–è¾‘è·ç¦»
- $\max(|a|, |b|)$ï¼šä¸¤ä¸ªå­—ç¬¦ä¸²çš„æœ€å¤§é•¿åº¦
- $\text{similarity} \in [0, 1]$ï¼š1 è¡¨ç¤ºå®Œå…¨ç›¸åŒï¼Œ0 è¡¨ç¤ºå®Œå…¨ä¸åŒ

#### è®¡ç®—ç¤ºä¾‹

**ç¤ºä¾‹ 1**ï¼šå®Œå…¨åŒ¹é…
```
a = "ä¼˜ç§€å­¦ç”Ÿ"
b = "ä¼˜ç§€å­¦ç”Ÿ"

lev(a, b) = 0
similarity = 1 - \frac{0}{4} = 1.0
```

**ç¤ºä¾‹ 2**ï¼šå•å­—ç¬¦å·®å¼‚
```
a = "ä¼˜ç§€å­¦ç”Ÿ"
b = "ä¼˜ç§€ç”Ÿ"

ç¼–è¾‘æ­¥éª¤ï¼š
1. åˆ é™¤ "å­¦" (1æ¬¡æ“ä½œ)
lev(a, b) = 1
similarity = 1 - \frac{1}{4} = 0.75
```

**ç¤ºä¾‹ 3**ï¼šå¤šä¸ªå·®å¼‚
```
a = "å›½å®¶å¥–å­¦é‡‘"
b = "å›½å®¶åŠ±å¿—å¥–å­¦é‡‘"

ç¼–è¾‘æ­¥éª¤ï¼š
1. æ’å…¥ "åŠ±å¿—" (2æ¬¡æ“ä½œ)
lev(a, b) = 2
similarity = 1 - \frac{2}{6} = 0.667
```

**ç¤ºä¾‹ 4**ï¼šå®Œå…¨ä¸åŒ
```
a = "å¥–å­¦é‡‘"
b = "å¤„åˆ†"

ç¼–è¾‘æ­¥éª¤ï¼š
1. æ›¿æ¢ "å¥–" â†’ "å¤„"
2. æ›¿æ¢ "å­¦" â†’ "åˆ†"
lev(a, b) = 2
similarity = 1 - \frac{2}{2} = 0.0
```

#### åŠ¨æ€è§„åˆ’å®ç°

**çŠ¶æ€è½¬ç§»çŸ©é˜µ**ï¼š

ä»¥ `"kitten"` â†’ `"sitting"` ä¸ºä¾‹ï¼š

```
      ""  s  i  t  t  i  n  g
   ""  0  1  2  3  4  5  6  7
   k   1  1  2  3  4  5  6  7
   i   2  2  1  2  3  4  5  6
   t   3  3  2  1  2  3  4  5
   t   4  4  3  2  1  2  3  4
   e   5  5  4  3  2  2  3  4
   n   6  6  5  4  3  3  2  3
```

**è½¬ç§»æ–¹ç¨‹**ï¼š

$$
\text{dp}[i][j] =
\begin{cases}
j & \text{if } i = 0 \\
i & \text{if } j = 0 \\
\text{dp}[i-1][j-1] & \text{if } a[i] = b[j] \text{ (å­—ç¬¦ç›¸åŒ)} \\
1 + \min
\begin{cases}
\text{dp}[i-1][j] & \text{(åˆ é™¤)} \\
\text{dp}[i][j-1] & \text{(æ’å…¥)} \\
\text{dp}[i-1][j-1] & \text{(æ›¿æ¢)}
\end{cases} & \text{otherwise}
\end{cases}
$$

#### ç®—æ³•å¤æ‚åº¦

| å¤æ‚åº¦ç±»å‹ | å€¼ | è¯´æ˜ |
|-----------|-----|------|
| **æ—¶é—´å¤æ‚åº¦** | O(m Ã— n) | m, n åˆ†åˆ«ä¸ºä¸¤ä¸ªå­—ç¬¦ä¸²çš„é•¿åº¦ |
| **ç©ºé—´å¤æ‚åº¦** | O(m Ã— n) | éœ€è¦å­˜å‚¨æ•´ä¸ª dp çŸ©é˜µ |
| **ä¼˜åŒ–ç©ºé—´** | O(min(m, n)) | åªä¿ç•™å‰ä¸€è¡Œå’Œå½“å‰è¡Œ |

#### åœ¨å®ä½“æ¶ˆæ­§ä¸­çš„åº”ç”¨

**ä¸ºä»€ä¹ˆé€‰æ‹© Levenshtein è·ç¦»**ï¼Ÿ

1. **æ¨¡ç³ŠåŒ¹é…èƒ½åŠ›å¼º**ï¼šèƒ½å¤„ç†æ‹¼å†™é”™è¯¯ã€ç¼©å†™ã€åŒä¹‰è¯å˜ä½“
   - `"ä¼˜ç§€å­¦ç”Ÿ"` â†” `"ä¼˜ç§€ç”Ÿ"` â†’ similarity = 0.75
   - `"å›½å®¶å¥–å­¦é‡‘"` â†” `"å›½å®¶å¥–åŠ©å­¦é‡‘"` â†’ similarity = 0.83

2. **å¯¹é¡ºåºæ•æ„Ÿ**ï¼šä¿ç•™å­—ç¬¦çš„ç›¸å¯¹ä½ç½®ä¿¡æ¯
   - `"å­¦ç”Ÿä¼˜ç§€"` å’Œ `"ä¼˜ç§€å­¦ç”Ÿ"` çš„è·ç¦» â‰  0ï¼ˆé¡ºåºä¸åŒï¼‰
   - è¿™ç¬¦åˆè¯­ä¹‰å·®å¼‚

3. **è®¡ç®—é«˜æ•ˆ**ï¼šå¯¹äºçŸ­æ–‡æœ¬ï¼ˆå®ä½“åç§°é€šå¸¸ < 20 å­—ç¬¦ï¼‰ï¼Œé€Ÿåº¦éå¸¸å¿«
   - å¹³å‡è€—æ—¶ï¼š< 1ms per comparison
   - å¯ä»¥ä½¿ç”¨ Neo4j APOC è¿‡ç¨‹å¹¶è¡ŒåŠ é€Ÿ

**Neo4j APOC å®ç°**ï¼š

```cypher
// apoc.text.levenshteinSimilarity å†…éƒ¨å®ç°
RETURN apoc.text.levenshteinSimilarity(
    "ä¼˜ç§€å­¦ç”Ÿ",      // string1
    "ä¼˜ç§€ç”Ÿ",        // string2
    0.7              // ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆå¯é€‰ï¼‰
) AS similarity
// ç»“æœï¼š0.75
```

**ä¼˜åŒ–çš„å¬å›ç­–ç•¥**ï¼š

```cypher
// å®Œæ•´å¬å›æµç¨‹
MATCH (e:__Entity__)
WHERE e.id IS NOT NULL
// 1. è½¬å°å†™ï¼ˆå¤§å°å†™ä¸æ•æ„Ÿï¼‰
WITH e,
     apoc.text.levenshteinSimilarity(
         toLower($mention),
         toLower(e.id)
     ) AS similarity
// 2. è¿‡æ»¤ä½ç›¸ä¼¼åº¦ï¼ˆæå‰ç»ˆæ­¢ï¼‰
WHERE similarity >= $threshold  // DISAMBIG_STRING_THRESHOLD = 0.7
// 3. æŒ‰ç›¸ä¼¼åº¦æ’åºï¼ˆTop-Kï¼‰
RETURN e.id AS entity_id,
       e.description AS description,
       similarity
ORDER BY similarity DESC
LIMIT $top_k  // DISAMBIG_TOP_K = 10
```

#### å¬å›æ•ˆæœç¤ºä¾‹

**è¾“å…¥ mention**ï¼š`"ä¼˜ç§€å­¦ç”Ÿå¥–å­¦é‡‘"`

**å¬å›å€™é€‰**ï¼ˆTop-5ï¼‰ï¼š

| æ’å | å®ä½“ID | ç›¸ä¼¼åº¦ | è¯´æ˜ |
|------|--------|--------|------|
| 1 | ä¼˜ç§€å­¦ç”Ÿ | 0.85 | é«˜ç›¸ä¼¼åº¦ï¼Œéƒ¨åˆ†åŒ¹é… |
| 2 | å›½å®¶å¥–å­¦é‡‘ | 0.75 | ä¸­ç­‰ç›¸ä¼¼åº¦ï¼Œä¸»é¢˜ç›¸å…³ |
| 3 | ä¼˜ç§€å­¦ç”Ÿå¥–å­¦é‡‘ | 1.0 | å®Œå…¨åŒ¹é… |
| 4 | ä¼˜ç§€å­¦ç”Ÿå¹²éƒ¨ | 0.82 | é«˜ç›¸ä¼¼åº¦ï¼Œç•¥å¾®ä¸åŒ |
| 5 | ä¼˜ç§€ç”Ÿ | 0.88 | é«˜ç›¸ä¼¼åº¦ï¼Œç¼©å†™å½¢å¼ |

**è¿‡æ»¤å**ï¼ˆ`threshold >= 0.7`ï¼‰ï¼š
- ä¿ç•™å…¨éƒ¨ 5 ä¸ªå€™é€‰
- è¿›å…¥ä¸‹ä¸€é˜¶æ®µï¼ˆå‘é‡é‡æ’ï¼‰

---

```cypher
MATCH (e:`__Entity__`)
WHERE e.id IS NOT NULL
WITH e,
     apoc.text.levenshteinSimilarity(toLower($mention), toLower(e.id)) AS similarity
WHERE similarity >= $threshold
RETURN e.id AS entity_id,
       e.description AS description,
       similarity
ORDER BY similarity DESC
LIMIT $top_k
```

**é…ç½®å‚æ•°**ï¼š
```env
DISAMBIG_STRING_THRESHOLD=0.7    # å­—ç¬¦ä¸²ç›¸ä¼¼åº¦é˜ˆå€¼
DISAMBIG_TOP_K=10                # å¬å›å€™é€‰æ•°é‡
```

**æ ¸å¿ƒä»£ç **ï¼š

```python
class EntityDisambiguator:
    def string_recall(self, mention: str, top_k: int = 10) -> List[Dict]:
        """å­—ç¬¦ä¸²å¬å›å€™é€‰å®ä½“"""
        query = """
        MATCH (e:`__Entity__`)
        WHERE e.id IS NOT NULL
        WITH e,
             apoc.text.levenshteinSimilarity(toLower($mention), toLower(e.id)) AS similarity
        WHERE similarity >= $threshold
        RETURN e.id AS entity_id,
               e.description AS description,
               similarity
        ORDER BY similarity DESC
        LIMIT $top_k
        """

        results = self.graph.query(query, params={
            'mention': mention,
            'threshold': DISAMBIG_STRING_THRESHOLD,
            'top_k': top_k
        })

        return results
```

### é˜¶æ®µ2ï¼šå‘é‡é‡æ’

**ç›®æ ‡**ï¼šä½¿ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦é‡æ–°æ’åºå€™é€‰ã€‚

**ç®—æ³•**ï¼šCosine ç›¸ä¼¼åº¦

#### ç®—æ³•åŸç†

**å®šä¹‰**ï¼šä½™å¼¦ç›¸ä¼¼åº¦è¡¡é‡ä¸¤ä¸ªå‘é‡ä¹‹é—´çš„å¤¹è§’ä½™å¼¦å€¼ï¼Œåæ˜ å®ƒä»¬åœ¨æ–¹å‘ä¸Šçš„ç›¸ä¼¼ç¨‹åº¦ï¼Œè€Œä¸è€ƒè™‘å‘é‡çš„å¤§å°ï¼ˆé•¿åº¦ï¼‰ã€‚

**æ•°å­¦å…¬å¼**ï¼š

ç»™å®šä¸¤ä¸ª $d$ ç»´å‘é‡ $\vec{A} = (a_1, a_2, \ldots, a_d)$ å’Œ $\vec{B} = (b_1, b_2, \ldots, b_d)$ï¼Œä½™å¼¦ç›¸ä¼¼åº¦å®šä¹‰ä¸ºï¼š

$$
\text{cosine}(\vec{A}, \vec{B}) = \frac{\vec{A} \cdot \vec{B}}{\|\vec{A}\| \times \|\vec{B}\|} = \frac{\sum_{i=1}^{d} a_i b_i}{\sqrt{\sum_{i=1}^{d} a_i^2} \times \sqrt{\sum_{i=1}^{d} b_i^2}}
$$

å…¶ä¸­ï¼š
- $\vec{A} \cdot \vec{B}$ï¼šå‘é‡çš„ç‚¹ç§¯ï¼ˆå†…ç§¯ï¼‰
- $\|\vec{A}\|$ï¼šå‘é‡ $\vec{A}$ çš„ L2 èŒƒæ•°ï¼ˆæ¬§å‡ é‡Œå¾—é•¿åº¦ï¼‰
- $\text{cosine}(\vec{A}, \vec{B}) \in [-1, 1]$ï¼š
  - $1$ï¼šå®Œå…¨åŒå‘ï¼ˆæ–¹å‘å®Œå…¨ç›¸åŒï¼‰
  - $0$ï¼šæ­£äº¤ï¼ˆæ— å…³ï¼‰
  - $-1$ï¼šå®Œå…¨åå‘ï¼ˆæ–¹å‘å®Œå…¨ç›¸åï¼‰

**å‡ ä½•è§£é‡Š**ï¼š

ä½™å¼¦ç›¸ä¼¼åº¦ç­‰äºä¸¤ä¸ªå‘é‡å¤¹è§’çš„ä½™å¼¦å€¼ï¼š

$$
\text{cosine}(\vec{A}, \vec{B}) = \cos(\theta)
$$

å…¶ä¸­ $\theta$ æ˜¯å‘é‡ $\vec{A}$ å’Œ $\vec{B}$ ä¹‹é—´çš„å¤¹è§’ã€‚

```
     B
     â†‘
     |\
     | \
     |  \ Î¸ = 60Â°
     |   \
     |____\
    O â†’   A
```

å¯¹äº $\theta = 60^\circ$ï¼š
$$
\cos(60^\circ) = 0.5
$$

#### è®¡ç®—ç¤ºä¾‹

**ç¤ºä¾‹ 1**ï¼šå®Œå…¨ç›¸åŒ
```
vec1 = [1, 2, 3]
vec2 = [1, 2, 3]

ç‚¹ç§¯: 1Ã—1 + 2Ã—2 + 3Ã—3 = 14
èŒƒæ•°1: âˆš(1Â² + 2Â² + 3Â²) = âˆš14 â‰ˆ 3.74
èŒƒæ•°2: âˆš(1Â² + 2Â² + 3Â²) = âˆš14 â‰ˆ 3.74

cosine = 14 / (3.74 Ã— 3.74) = 1.0
```

**ç¤ºä¾‹ 2**ï¼šæ–¹å‘ç›¸åŒï¼Œå¤§å°ä¸åŒ
```
vec1 = [1, 2, 3]
vec2 = [2, 4, 6]  # vec1 çš„ 2 å€

ç‚¹ç§¯: 1Ã—2 + 2Ã—4 + 3Ã—6 = 28
èŒƒæ•°1: âˆš14 â‰ˆ 3.74
èŒƒæ•°2: âˆš56 â‰ˆ 7.48

cosine = 28 / (3.74 Ã— 7.48) = 1.0
```

**ç¤ºä¾‹ 3**ï¼šæ­£äº¤ï¼ˆæ— å…³ï¼‰
```
vec1 = [1, 0, 0]
vec2 = [0, 1, 0]

ç‚¹ç§¯: 1Ã—0 + 0Ã—1 + 0Ã—0 = 0
èŒƒæ•°1: âˆš1 = 1
èŒƒæ•°2: âˆš1 = 1

cosine = 0 / (1 Ã— 1) = 0.0
```

**ç¤ºä¾‹ 4**ï¼šéƒ¨åˆ†ç›¸ä¼¼
```
vec1 = [1, 2, 3]
vec2 = [2, 3, 4]

ç‚¹ç§¯: 1Ã—2 + 2Ã—3 + 3Ã—4 = 20
èŒƒæ•°1: âˆš14 â‰ˆ 3.74
èŒƒæ•°2: âˆš29 â‰ˆ 5.39

cosine = 20 / (3.74 Ã— 5.39) â‰ˆ 0.99
```

#### å‘é‡è¡¨ç¤º

**æ–‡æœ¬å‘é‡åŒ–ï¼ˆEmbeddingï¼‰**ï¼š

åœ¨å®ä½“æ¶ˆæ­§ä¸­ï¼Œæˆ‘ä»¬éœ€è¦å°†æ–‡æœ¬ï¼ˆå®ä½“åç§°ã€æè¿°ç­‰ï¼‰è½¬æ¢ä¸ºæ•°å€¼å‘é‡ã€‚

**å¸¸è§ Embedding æ¨¡å‹**ï¼š

| æ¨¡å‹ | ç»´åº¦ | ç‰¹ç‚¹ | åº”ç”¨åœºæ™¯ |
|------|------|------|----------|
| **text-embedding-3-large** | 3072 | é«˜ç²¾åº¦ï¼Œå¤šè¯­è¨€ | é€šç”¨è¯­ä¹‰æ£€ç´¢ |
| **text-embedding-3-small** | 1536 | å¹³è¡¡æ€§èƒ½ä¸é€Ÿåº¦ | å®æ—¶åº”ç”¨ |
| **text-embedding-ada-002** | 1536 | ç¨³å®šå¯é  | é—ç•™ç³»ç»Ÿ |
| **sentence-transformers** | 384-768 | è½»é‡çº§ï¼Œå¯æœ¬åœ°éƒ¨ç½² | è¾¹ç¼˜è®¡ç®— |

**æ–‡æœ¬ â†’ å‘é‡ç¤ºä¾‹**ï¼š

```python
from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(model="text-embedding-3-large")

# å®ä½“åç§°å‘é‡åŒ–
entity_name = "å›½å®¶å¥–å­¦é‡‘"
entity_vec = embeddings.embed_query(entity_name)
# è¾“å‡º: [0.0123, -0.0456, 0.0789, ..., 0.0321]  # 3072 ç»´å‘é‡

# Mention å‘é‡åŒ–
mention = "å›½å®¶å¥–å­¦é‡‘ç”³è¯·"
mention_vec = embeddings.embed_query(mention)
# è¾“å‡º: [0.0135, -0.0442, 0.0801, ..., 0.0298]  # 3072 ç»´å‘é‡

# è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
similarity = cosine_similarity(entity_vec, mention_vec)
# è¾“å‡º: 0.89  # é«˜ç›¸ä¼¼åº¦
```

#### ç®—æ³•å¤æ‚åº¦

| å¤æ‚åº¦ç±»å‹ | å€¼ | è¯´æ˜ |
|-----------|-----|------|
| **æ—¶é—´å¤æ‚åº¦** | O(d) | d ä¸ºå‘é‡ç»´åº¦ |
| **ç©ºé—´å¤æ‚åº¦** | O(d) | éœ€è¦å­˜å‚¨ä¸¤ä¸ªå‘é‡ |

å…¶ä¸­ï¼š
- ç‚¹ç§¯è®¡ç®—ï¼šO(d)
- èŒƒæ•°è®¡ç®—ï¼šO(d)
- é™¤æ³•è¿ç®—ï¼šO(1)

**ä¼˜åŒ–æŠ€å·§**ï¼š

```python
# 1. å‘é‡å½’ä¸€åŒ–ï¼ˆæå‰è®¡ç®—èŒƒæ•°ï¼‰
def normalize(vec):
    norm = np.linalg.norm(vec)
    return vec / norm

# 2. ç›¸ä¼¼åº¦è®¡ç®—ç®€åŒ–ä¸ºç‚¹ç§¯
def cosine_similarity_normalized(vec1_norm, vec2_norm):
    return np.dot(vec1_norm, vec2_norm)

# ä½¿ç”¨æ–¹å¼
entity_vec_norm = normalize(entity_vec)  # é¢„è®¡ç®—ä¸€æ¬¡
mention_vec_norm = normalize(mention_vec)  # é¢„è®¡ç®—ä¸€æ¬¡
similarity = np.dot(entity_vec_norm, mention_vec_norm)  # O(d)
```

#### åœ¨å®ä½“æ¶ˆæ­§ä¸­çš„åº”ç”¨

**ä¸ºä»€ä¹ˆé€‰æ‹©ä½™å¼¦ç›¸ä¼¼åº¦**ï¼Ÿ

1. **å¤§å°æ— å…³ï¼Œæ–¹å‘æ•æ„Ÿ**ï¼š
   - åªå…³æ³¨è¯­ä¹‰æ–¹å‘ï¼Œå¿½ç•¥æ–‡æœ¬é•¿åº¦å·®å¼‚
   - `"å›½å®¶å¥–å­¦é‡‘"` (4å­—) å’Œ `"å›½å®¶å¥–å­¦é‡‘ç”³è¯·"` (7å­—) ä»èƒ½é«˜ç›¸ä¼¼åº¦

2. **è¯­ä¹‰ç†è§£èƒ½åŠ›å¼º**ï¼š
   - èƒ½æ•æ‰åŒä¹‰è¯ã€è¿‘ä¹‰è¯çš„è¯­ä¹‰ç›¸ä¼¼æ€§
   - `"ä¼˜ç§€å­¦ç”Ÿ"` å’Œ `"æ°å‡ºå­¦ç”Ÿ"` çš„å‘é‡è·ç¦»å¾ˆè¿‘

3. **è®¡ç®—é«˜æ•ˆ**ï¼š
   - å¯¹äºé«˜ç»´å‘é‡ï¼ˆd = 3072ï¼‰ï¼Œå•æ¬¡è®¡ç®— < 0.1ms
   - å¯åˆ©ç”¨ NumPy/BLAS åŠ é€ŸçŸ©é˜µè¿ç®—

4. **é€‚åˆ Top-K æ£€ç´¢**ï¼š
   - å¯ä½¿ç”¨å‘é‡ç´¢å¼•ï¼ˆå¦‚ FAISSã€HNSWï¼‰åŠ é€Ÿå¤§è§„æ¨¡æ£€ç´¢
   - Neo4j åŸç”Ÿæ”¯æŒå‘é‡ç´¢å¼•

**åŠ æƒç»„åˆç­–ç•¥**ï¼š

```python
combined_score = (
    0.4 * string_similarity +   # Levenshtein ç›¸ä¼¼åº¦
    0.6 * vector_similarity      # ä½™å¼¦ç›¸ä¼¼åº¦
)
```

**æƒé‡è®¾è®¡ç†ç”±**ï¼š

| ç›¸ä¼¼åº¦ç±»å‹ | æƒé‡ | ä¼˜åŠ¿ | åŠ£åŠ¿ |
|-----------|------|------|------|
| **å­—ç¬¦ä¸²ç›¸ä¼¼åº¦** | 0.4 | ç²¾ç¡®åŒ¹é…å¼ºï¼Œé€Ÿåº¦å¿« | è¯­ä¹‰ç†è§£å¼± |
| **å‘é‡ç›¸ä¼¼åº¦** | 0.6 | è¯­ä¹‰ç†è§£å¼ºï¼Œæ³›åŒ–èƒ½åŠ›å¥½ | å¯èƒ½è¯¯åŒ¹é… |

**å®é™…æ•ˆæœ**ï¼š

```
è¾“å…¥: "å›½å®¶å¥–å­¦é‡‘"

å€™é€‰1: "å›½å®¶å¥–å­¦é‡‘"
  - å­—ç¬¦ä¸²ç›¸ä¼¼åº¦: 1.0 (å®Œå…¨åŒ¹é…)
  - å‘é‡ç›¸ä¼¼åº¦: 1.0 (å®Œå…¨ç›¸åŒ)
  - combined: 0.4 Ã— 1.0 + 0.6 Ã— 1.0 = 1.0 âœ“

å€™é€‰2: "å›½å®¶åŠ±å¿—å¥–å­¦é‡‘"
  - å­—ç¬¦ä¸²ç›¸ä¼¼åº¦: 0.667 (éƒ¨åˆ†åŒ¹é…)
  - å‘é‡ç›¸ä¼¼åº¦: 0.92 (è¯­ä¹‰é«˜åº¦ç›¸ä¼¼)
  - combined: 0.4 Ã— 0.667 + 0.6 Ã— 0.92 = 0.82 âœ“

å€™é€‰3: "å¥–å­¦é‡‘"
  - å­—ç¬¦ä¸²ç›¸ä¼¼åº¦: 0.5 (å­ä¸²åŒ¹é…)
  - å‘é‡ç›¸ä¼¼åº¦: 0.85 (è¯­ä¹‰ç›¸å…³)
  - combined: 0.4 Ã— 0.5 + 0.6 Ã— 0.85 = 0.71 âœ“

é‡æ’åé¡ºåº: å€™é€‰1 > å€™é€‰2 > å€™é€‰3
```

#### é‡æ’æ•ˆæœç¤ºä¾‹

**è¾“å…¥ mention**ï¼š`"ä¼˜ç§€å­¦ç”Ÿå¥–å­¦é‡‘"`

**é˜¶æ®µ1å¬å›ï¼ˆå­—ç¬¦ä¸²ç›¸ä¼¼åº¦æ’åºï¼‰**ï¼š

| æ’å | å®ä½“ID | å­—ç¬¦ä¸²ç›¸ä¼¼åº¦ | è¯´æ˜ |
|------|--------|------------|------|
| 1 | ä¼˜ç§€ç”Ÿ | 0.88 | ç¼©å†™ï¼Œé«˜å­—ç¬¦ä¸²ç›¸ä¼¼åº¦ |
| 2 | ä¼˜ç§€å­¦ç”Ÿ | 0.85 | éƒ¨åˆ†åŒ¹é… |
| 3 | ä¼˜ç§€å­¦ç”Ÿå¥–å­¦é‡‘ | 1.0 | å®Œå…¨åŒ¹é… |

**é˜¶æ®µ2é‡æ’ï¼ˆåŠ æƒç»„åˆåˆ†æ•°æ’åºï¼‰**ï¼š

| æ’å | å®ä½“ID | å­—ç¬¦ä¸² | å‘é‡ | combined | æ–°æ’å |
|------|--------|--------|------|----------|--------|
| ä¼˜ç§€å­¦ç”Ÿå¥–å­¦é‡‘ | 1.0 | 1.0 | **1.0** | 1 â†‘ |
| ä¼˜ç§€å­¦ç”Ÿ | 0.85 | 0.92 | **0.89** | 2 â†‘ |
| ä¼˜ç§€ç”Ÿ | 0.88 | 0.75 | **0.80** | 3 â†“ |

**å…³é”®å‘ç°**ï¼š
- `"ä¼˜ç§€å­¦ç”Ÿå¥–å­¦é‡‘"` è™½ç„¶å­—ç¬¦ä¸²å®Œå…¨åŒ¹é…ï¼Œä½†å‘é‡ç›¸ä¼¼åº¦éªŒè¯äº†è¯­ä¹‰ä¸€è‡´æ€§
- `"ä¼˜ç§€å­¦ç”Ÿ"` è¯­ä¹‰é«˜åº¦ç›¸å…³ï¼Œæ’åä¸Šå‡
- `"ä¼˜ç§€ç”Ÿ"` å­—ç¬¦ä¸²ç›¸ä¼¼åº¦é«˜ä½†è¯­ä¹‰ç•¥ä½ï¼Œæ’åä¸‹é™

---

```python
def vector_rerank(self, mention: str, candidates: List[Dict]) -> List[Dict]:
    """å‘é‡é‡æ’å€™é€‰å®ä½“"""
    # è®¡ç®— mention çš„ embedding
    mention_vec = self.embeddings.embed_query(mention)

    # è·å–å€™é€‰å®ä½“çš„ embedding
    entity_ids = [c['entity_id'] for c in candidates]
    embeddings_map = self._fetch_entity_embeddings(entity_ids)

    # è®¡ç®—å‘é‡ç›¸ä¼¼åº¦å¹¶é‡æ’
    reranked = []
    for candidate in candidates:
        entity_id = candidate['entity_id']
        if entity_id in embeddings_map:
            entity_vec = embeddings_map[entity_id]
            vec_sim = self._cosine_similarity(mention_vec, entity_vec)

            # åŠ æƒç»„åˆåˆ†æ•°
            combined_score = (
                0.4 * candidate['similarity'] +  # å­—ç¬¦ä¸²ç›¸ä¼¼åº¦
                0.6 * vec_sim                    # å‘é‡ç›¸ä¼¼åº¦
            )

            reranked.append({
                **candidate,
                'vector_similarity': vec_sim,
                'combined_score': combined_score
            })

    return sorted(reranked, key=lambda x: x['combined_score'], reverse=True)

def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
    """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
    vec1 = np.array(vec1)
    vec2 = np.array(vec2)
    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))
```

**é…ç½®å‚æ•°**ï¼š
```env
DISAMBIG_VECTOR_THRESHOLD=0.8    # å‘é‡ç›¸ä¼¼åº¦é˜ˆå€¼
```

### é˜¶æ®µ3ï¼šNIL æ£€æµ‹

**ç›®æ ‡**ï¼šåˆ¤æ–­æ˜¯å¦ä¸ºæœªç™»å½•å®ä½“ï¼ˆä¸åœ¨çŸ¥è¯†åº“ä¸­çš„æ–°å®ä½“ï¼‰ã€‚

```python
def nil_detection(self, mention: str, candidates: List[Dict]) -> Tuple[bool, Optional[str]]:
    """NIL æ£€æµ‹"""
    if not candidates:
        return True, None  # æ— å€™é€‰ï¼Œåˆ¤å®šä¸º NIL

    # æ£€æŸ¥æœ€ä½³å€™é€‰çš„åˆ†æ•°
    best_candidate = candidates[0]
    if best_candidate.get('combined_score', 0) < DISAMBIG_NIL_THRESHOLD:
        return True, None  # åˆ†æ•°è¿‡ä½ï¼Œåˆ¤å®šä¸º NIL

    return False, best_candidate['entity_id']  # åŒ¹é…æˆåŠŸ
```

**é…ç½®å‚æ•°**ï¼š
```env
DISAMBIG_NIL_THRESHOLD=0.75      # NIL æ£€æµ‹é˜ˆå€¼
```

### å®Œæ•´æ¶ˆæ­§æµç¨‹

```python
def disambiguate(self, mention: str) -> Dict[str, Any]:
    """å®Œæ•´çš„æ¶ˆæ­§æµç¨‹"""
    # 1. å­—ç¬¦ä¸²å¬å›
    candidates = self.string_recall(mention)

    if not candidates:
        return {
            'mention': mention,
            'canonical_id': None,
            'is_nil': True,
            'candidates': []
        }

    # 2. å‘é‡é‡æ’
    reranked = self.vector_rerank(mention, candidates)

    # 3. NIL æ£€æµ‹
    is_nil, canonical_id = self.nil_detection(mention, reranked)

    return {
        'mention': mention,
        'canonical_id': canonical_id,
        'is_nil': is_nil,
        'candidates': reranked[:3]  # è¿”å›å‰3ä¸ªå€™é€‰
    }
```

### åº”ç”¨åˆ°å›¾è°±

#### canonical_id çš„ä½œç”¨

**å®šä¹‰**ï¼š`canonical_id`ï¼ˆè§„èŒƒIDï¼‰æ˜¯æŒ‡å‘çŸ¥è¯†åº“ä¸­**è§„èŒƒå®ä½“**ï¼ˆCanonical Entityï¼‰çš„å”¯ä¸€æ ‡è¯†ç¬¦ã€‚

**æ ¸å¿ƒç›®çš„**ï¼š
- æ¶ˆé™¤é‡å¤å®ä½“ï¼Œå»ºç«‹"ä¸»å®ä½“-åˆ«åå®ä½“"çš„æ˜ å°„å…³ç³»
- ç»Ÿä¸€å®ä½“è®¿é—®å…¥å£ï¼Œç¡®ä¿æŸ¥è¯¢æ—¶å§‹ç»ˆè¿”å›æœ€å®Œæ•´çš„å®ä½“ä¿¡æ¯
- æ”¯æŒå®ä½“åˆå¹¶ï¼Œé¿å…æ•°æ®å†—ä½™å’Œå†²çª

**å…·ä½“ä½œç”¨**ï¼š

1. **å®ä½“æ¶ˆæ­§é˜¶æ®µ**ï¼ˆæ¶ˆæ­§åè®¾ç½®ï¼‰ï¼š
   ```cypher
   // WCC åˆ†ç»„åï¼Œä¸ºæ¯ä¸ªç»„é€‰æ‹©åº¦æ•°æœ€å¤§çš„å®ä½“ä½œä¸º canonical
   MATCH (e:__Entity__)
   WHERE e.wcc = 123  // åŒä¸€ä¸ª WCC åˆ†ç»„
   AND e.degree = 15  // åº¦æ•°æœ€å¤§
   RETURN e.id AS canonical_id  // "ä¼˜ç§€å­¦ç”Ÿ"

   // å…¶ä»–å®ä½“æŒ‡å‘å®ƒ
   MATCH (other:__Entity__)
   WHERE other.id IN ["ä¼˜ç§€ç”Ÿ", "ä¼˜ç§€å­¦ç”Ÿå¹²éƒ¨"]
   SET other.canonical_id = "ä¼˜ç§€å­¦ç”Ÿ"
   ```

2. **å®ä½“å¯¹é½é˜¶æ®µ**ï¼ˆæŒ‰ canonical_id åˆ†ç»„åˆå¹¶ï¼‰ï¼š
   ```cypher
   // æŸ¥æ‰¾æ‰€æœ‰æŒ‡å‘åŒä¸€ canonical_id çš„å®ä½“
   MATCH (e:__Entity__)
   WHERE e.canonical_id = "ä¼˜ç§€å­¦ç”Ÿ"
   RETURN collect(e.id)
   // ç»“æœ: ["ä¼˜ç§€å­¦ç”Ÿ", "ä¼˜ç§€ç”Ÿ", "ä¼˜ç§€å­¦ç”Ÿå¹²éƒ¨"]

   // æ£€æµ‹å†²çªå¹¶åˆå¹¶
   ```

3. **æŸ¥è¯¢é˜¶æ®µ**ï¼ˆä¼˜å…ˆè¿”å› canonical å®ä½“ï¼‰ï¼š
   ```cypher
   // ç”¨æˆ·æŸ¥è¯¢"ä¼˜ç§€ç”Ÿ"æ—¶ï¼Œè‡ªåŠ¨è¿”å› canonical å®ä½“çš„å®Œæ•´ä¿¡æ¯
   MATCH (e:__Entity__ {id: "ä¼˜ç§€ç”Ÿ"})
   OPTIONAL MATCH (canonical:__Entity__ {id: e.canonical_id})
   RETURN coalesce(canonical.description, e.description) AS description
   ```

**æ•°æ®ç»“æ„ç¤ºä¾‹**ï¼š

```json
// åœºæ™¯ï¼šçŸ¥è¯†åº“ä¸­å­˜åœ¨ 3 ä¸ªç›¸ä¼¼çš„å®ä½“èŠ‚ç‚¹
{
  "å®ä½“1": {
    "id": "ä¼˜ç§€å­¦ç”Ÿ",
    "canonical_id": "ä¼˜ç§€å­¦ç”Ÿ",  // æŒ‡å‘è‡ªå·±ï¼ˆä¸»å®ä½“ï¼‰
    "degree": 15,
    "description": "å“å­¦å…¼ä¼˜çš„å­¦ç”Ÿç§°å·..."
  },
  "å®ä½“2": {
    "id": "ä¼˜ç§€ç”Ÿ",
    "canonical_id": "ä¼˜ç§€å­¦ç”Ÿ",  // æŒ‡å‘ä¸»å®ä½“
    "degree": 8,
    "description": "ä¼˜ç§€å­¦ç”Ÿç§°å·çš„ç®€ç§°..."
  },
  "å®ä½“3": {
    "id": "ä¼˜ç§€å­¦ç”Ÿå¹²éƒ¨",
    "canonical_id": "ä¼˜ç§€å­¦ç”Ÿ",  // æŒ‡å‘ä¸»å®ä½“
    "degree": 5,
    "description": "æ—¢æ˜¯ä¼˜ç§€å­¦ç”Ÿåˆæ˜¯å­¦ç”Ÿå¹²éƒ¨..."
  }
}
```

**å¯è§†åŒ–è¡¨ç¤º**ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     WCC åˆ†ç»„ #123                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚
â”‚   â”‚ ä¼˜ç§€å­¦ç”Ÿ    â”‚ â† canonical_id = "ä¼˜ç§€å­¦ç”Ÿ" (è‡ªå·±)         â”‚
â”‚   â”‚ degree: 15  â”‚ â† ä¸»å®ä½“ï¼ˆåº¦æ•°æœ€å¤§ï¼‰                       â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
â”‚          â”‚                                                   â”‚
â”‚          â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚
â”‚          â†“                  â†“                                â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚   â”‚ ä¼˜ç§€ç”Ÿ      â”‚   â”‚ ä¼˜ç§€å­¦ç”Ÿå¹²éƒ¨    â”‚                     â”‚
â”‚   â”‚ degree: 8   â”‚   â”‚ degree: 5       â”‚                     â”‚
â”‚   â”‚ canonical:  â”‚   â”‚ canonical:      â”‚                     â”‚
â”‚   â”‚ "ä¼˜ç§€å­¦ç”Ÿ"  â”‚   â”‚ "ä¼˜ç§€å­¦ç”Ÿ"      â”‚                     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**å…³é”®ç‰¹æ€§**ï¼š

| ç‰¹æ€§ | è¯´æ˜ | ç¤ºä¾‹ |
|------|------|------|
| **ä¼ é€’æ€§** | canonical_id å¿…é¡»æŒ‡å‘å·²å­˜åœ¨çš„å®ä½“ | `"ä¼˜ç§€ç”Ÿ".canonical_id = "ä¼˜ç§€å­¦ç”Ÿ"` |
| **å”¯ä¸€æ€§** | ä¸€ä¸ªå®ä½“åªèƒ½æœ‰ä¸€ä¸ª canonical_id | æ¯ä¸ªå®ä½“æœ€å¤šæŒ‡å‘ä¸€ä¸ªä¸»å®ä½“ |
| **ç¯çŠ¶é¿å…** | canonical_id ä¸èƒ½å½¢æˆç¯ | Aâ†’Bâ†’Câ†’A æ˜¯éæ³•çš„ |
| **è‡ªæŒ‡å‘** | ä¸»å®ä½“çš„ canonical_id æŒ‡å‘è‡ªå·± | `"ä¼˜ç§€å­¦ç”Ÿ".canonical_id = "ä¼˜ç§€å­¦ç”Ÿ"` |
| **å¯ç©ºæ€§** | æ–°å®ä½“æˆ–æœªæ¶ˆæ­§å®ä½“ canonical_id = null | å°šæœªå¤„ç†çš„å®ä½“ |

**æŸ¥è¯¢ä¼˜åŒ–**ï¼š

```cypher
// åˆ›å»ºç´¢å¼•åŠ é€Ÿ canonical_id æŸ¥è¯¢
CREATE INDEX entity_canonical IF NOT EXISTS
FOR (e:__Entity__) ON (e.canonical_id);

// æŸ¥è¯¢æ—¶ä¼˜å…ˆä½¿ç”¨ canonical å®ä½“
MATCH (e:__Entity__)
WHERE e.canonical_id IS NOT NULL  // å·²æ¶ˆæ­§
WITH coalesce(
  (MATCH (c:__Entity__ {id: e.canonical_id}) RETURN c),
  e
) AS canonical_entity
RETURN canonical_entity.id,
       canonical_entity.description
```

**ä¸å®ä½“å¯¹é½çš„å…³ç³»**ï¼š

```
å®ä½“æ¶ˆæ­§ï¼ˆè®¾ç½® canonical_idï¼‰
    â†“
å®ä½“å¯¹é½ï¼ˆæŒ‰ canonical_id åˆ†ç»„ï¼‰
    â†“
å†²çªæ£€æµ‹ï¼ˆæ£€æŸ¥åŒä¸€ canonical_id ä¸‹çš„å®ä½“ï¼‰
    â†“
å®ä½“åˆå¹¶ï¼ˆä¿ç•™ä¸»å®ä½“ï¼Œåˆ é™¤å…¶ä»–å®ä½“ï¼‰
```

---

```python
def apply_to_graph(self) -> int:
    """
    å°†æ¶ˆæ­§ç»“æœåº”ç”¨åˆ°å›¾è°±
    æ ¸å¿ƒæ€è·¯ï¼šæ‰¾åˆ°å·²åˆå¹¶çš„å®ä½“ç»„ï¼Œä¸ºç»„å†…å…¶ä»–å®ä½“æŒ‡å‘ä¸»å®ä½“ä½œä¸º canonical_id
    """
    total_updated = 0
    batch_size = 500

    while True:
        # æŸ¥è¯¢æœªå¤„ç†çš„å®ä½“ç»„
        query = """
        MATCH (e:`__Entity__`)
        WHERE e.wcc IS NOT NULL
        AND e.embedding IS NOT NULL
        AND e.canonical_id IS NULL
        WITH e.wcc AS community, collect(e) AS entities
        WHERE size(entities) >= 2
        WITH community, entities
        ORDER BY community
        LIMIT $limit
        UNWIND entities AS entity
        WITH community, entity, COUNT { (entity)--() } AS degree
        WITH community, collect({
            id: entity.id,
            description: entity.description,
            degree: degree
        }) AS entity_info
        RETURN community, entity_info
        """

        groups = self.graph.query(query, params={'limit': batch_size})

        if not groups:
            break  # æ— æ›´å¤šæœªå¤„ç†çš„ç»„

        for group in groups:
            entities = group['entity_info']

            # é€‰æ‹©åº¦æ•°æœ€å¤§çš„å®ä½“ä½œä¸ºä¸»å®ä½“
            main_entity = max(entities, key=lambda e: e['degree'])

            # æ›´æ–°å…¶ä»–å®ä½“çš„ canonical_id
            for entity in entities:
                if entity['id'] != main_entity['id']:
                    self.graph.query("""
                        MATCH (e:`__Entity__` {id: $entity_id})
                        SET e.canonical_id = $canonical_id
                    """, params={
                        'entity_id': entity['id'],
                        'canonical_id': main_entity['id']
                    })
                    total_updated += 1

    return total_updated
```

---

## å®ä½“å¯¹é½æœºåˆ¶

### å†²çªæ£€æµ‹

**æ£€æµ‹è§„åˆ™**ï¼š
1. åŒä¸€è§„èŒƒå®ä½“ä¸‹çš„æ‰€æœ‰å®ä¾‹
2. å®ä¾‹é—´å­˜åœ¨çŸ›ç›¾çš„å±æ€§æˆ–å…³ç³»

```cypher
MATCH (e:`__Entity__`)
WHERE e.canonical_id IS NOT NULL
WITH e.canonical_id AS canonical, collect(e) AS instances
WHERE size(instances) > 1
RETURN canonical, instances
```

### å†²çªè§£å†³ç­–ç•¥

```mermaid
graph TD
    Conflict{æ£€æµ‹åˆ°å†²çª} --> Strategy{è§£å†³ç­–ç•¥}

    Strategy -->|manual_first| Manual[ä¼˜å…ˆä¿ç•™äººå·¥å®¡æ ¸çš„]
    Strategy -->|auto_first| Auto[ä¼˜å…ˆä¿ç•™è‡ªåŠ¨æå–çš„]
    Strategy -->|merge| Merge[åˆå¹¶æ‰€æœ‰å±æ€§]

    Manual --> Result[æ›´æ–°è§„èŒƒå®ä½“]
    Auto --> Result
    Merge --> Result

    style Conflict fill:#ffebee
    style Strategy fill:#fff3e0
    style Result fill:#e8f5e9
```

**æ ¸å¿ƒä»£ç **ï¼š

```python
class EntityAligner:
    def __init__(self, strategy: str = "merge"):
        self.strategy = strategy  # manual_first / auto_first / merge
        self.graph = get_graph()

    def align_entities(self) -> int:
        """å®ä½“å¯¹é½"""
        # 1. æŸ¥æ‰¾å†²çª
        conflicts = self._find_conflicts()

        # 2. è§£å†³å†²çª
        resolved = 0
        for conflict in conflicts:
            self._resolve_conflict(conflict)
            resolved += 1

        return resolved

    def _find_conflicts(self) -> List[Dict]:
        """æŸ¥æ‰¾å†²çªå®ä½“"""
        query = """
        MATCH (e:`__Entity__`)
        WHERE e.canonical_id IS NOT NULL
        WITH e.canonical_id AS canonical, collect(e) AS instances
        WHERE size(instances) > 1
        RETURN canonical, instances
        """
        return self.graph.query(query)

    def _resolve_conflict(self, conflict: Dict):
        """è§£å†³å•ä¸ªå†²çª"""
        canonical = conflict['canonical']
        instances = conflict['instances']

        if self.strategy == "merge":
            # åˆå¹¶æ‰€æœ‰å±æ€§
            merged_desc = self._merge_descriptions([i['description'] for i in instances])

            # æ›´æ–°è§„èŒƒå®ä½“
            self.graph.query("""
                MATCH (e:`__Entity__` {id: $canonical})
                SET e.description = $description
            """, params={
                'canonical': canonical,
                'description': merged_desc
            })

            # ä¿ç•™æ‰€æœ‰å…³ç³»ï¼ˆä¸åˆ é™¤å®ä¾‹èŠ‚ç‚¹ï¼‰
```

**é…ç½®å‚æ•°**ï¼š
```env
GRAPH_CONFLICT_STRATEGY=merge    # manual_first / auto_first / merge
```

---

## Neo4j å›¾è°±å­˜å‚¨

### èŠ‚ç‚¹ç±»å‹

```mermaid
graph LR
    subgraph æ ¸å¿ƒèŠ‚ç‚¹[æ ¸å¿ƒèŠ‚ç‚¹ç±»å‹]
        D[__Document__<br/>æ–‡æ¡£èŠ‚ç‚¹]
        C[__Chunk__<br/>å—èŠ‚ç‚¹]
        E[__Entity__<br/>å®ä½“èŠ‚ç‚¹]
        CO[__Community__<br/>ç¤¾åŒºèŠ‚ç‚¹]
    end

    D -->|CONTAINS| C
    C -->|MENTIONS| E
    E -->|IN_COMMUNITY| CO

    style D fill:#e3f2fd
    style C fill:#fff3e0
    style E fill:#e8f5e9
    style CO fill:#fce4ec
```

**èŠ‚ç‚¹å±æ€§è®¾è®¡**ï¼š

```python
# __Document__ èŠ‚ç‚¹
{
    "id": "doc_hash_123",
    "file_name": "student_handbook.pdf",
    "file_path": "/files/student_handbook.pdf",
    "created_at": "2026-01-04T10:00:00"
}

# __Chunk__ èŠ‚ç‚¹
{
    "id": "chunk_hash_456",
    "text": "å­¦ç”Ÿç”³è¯·å¥–å­¦é‡‘éœ€è¦æ»¡è¶³ä»¥ä¸‹æ¡ä»¶...",
    "index": 0,
    "embedding": [0.1, 0.2, ...]  # å‘é‡åµŒå…¥
}

# __Entity__ èŠ‚ç‚¹
{
    "id": "å›½å®¶å¥–å­¦é‡‘",
    "description": "å›½å®¶çº§åˆ«çš„æœ€é«˜å¥–å­¦é‡‘...",
    "type": "å¥–å­¦é‡‘ç±»å‹",
    "embedding": [0.3, 0.4, ...],
    "canonical_id": "å›½å®¶å¥–å­¦é‡‘",  # è§„èŒƒID
    "wcc": 123  # è¿é€šåˆ†é‡ID
}

# __Community__ èŠ‚ç‚¹
{
    "id": "community_1",
    "level": 0,
    "summary": "æœ¬ç¤¾åŒºåŒ…å«å­¦ç”Ÿå¥–å­¦é‡‘ç›¸å…³å®ä½“...",
    "full_content": "...",
    "weight": 50,  # ç¤¾åŒºé‡è¦æ€§
    "community_rank": 1
}
```

### å…³ç³»ç±»å‹

```mermaid
graph LR
    D[Document] -->|CONTAINS| C[Chunk]
    C -->|MENTIONS| E1[Entity 1]
    C -->|MENTIONS| E2[Entity 2]

    E1 -->|ç”³è¯·| E2
    E1 -->|IN_COMMUNITY| CO[Community]
    E2 -->|IN_COMMUNITY| CO

    style D fill:#e3f2fd
    style C fill:#fff3e0
    style E1 fill:#e8f5e9
    style E2 fill:#e8f5e9
    style CO fill:#fce4ec
```

**å…³ç³»å±æ€§è®¾è®¡**ï¼š

```python
# å®ä½“å…³ç³»
{
    "type": "ç”³è¯·",
    "description": "å­¦ç”Ÿå¯ä»¥ç”³è¯·å¥–å­¦é‡‘...",
    "weight": 10,  # å…³ç³»æƒé‡ï¼ˆå‡ºç°æ¬¡æ•°ï¼‰
    "source_chunks": ["chunk_1", "chunk_2"]  # æ¥æºå—
}

# MENTIONS å…³ç³»
{
    "count": 5  # è¯¥å®ä½“åœ¨å—ä¸­å‡ºç°æ¬¡æ•°
}
```

### æ‰¹é‡å†™å…¥ä¼˜åŒ–

```python
class GraphWriter:
    def __init__(self, batch_size: int = 100):
        self.batch_size = batch_size
        self.graph = get_graph()

    def write_entities(self, entities: List[Dict]):
        """æ‰¹é‡å†™å…¥å®ä½“"""
        for i in range(0, len(entities), self.batch_size):
            batch = entities[i:i + self.batch_size]

            query = """
            UNWIND $entities AS entity
            MERGE (e:`__Entity__` {id: entity.id})
            SET e.description = entity.description,
                e.type = entity.type,
                e.embedding = entity.embedding
            """

            self.graph.query(query, params={'entities': batch})

    def write_relationships(self, relationships: List[Dict]):
        """æ‰¹é‡å†™å…¥å…³ç³»"""
        for i in range(0, len(relationships), self.batch_size):
            batch = relationships[i:i + self.batch_size]

            query = """
            UNWIND $rels AS rel
            MATCH (e1:`__Entity__` {id: rel.source})
            MATCH (e2:`__Entity__` {id: rel.target})
            MERGE (e1)-[r:RELATES_TO {type: rel.type}]->(e2)
            SET r.description = rel.description,
                r.weight = coalesce(r.weight, 0) + 1
            """

            self.graph.query(query, params={'rels': batch})
```

---

## ç¤¾åŒºæ£€æµ‹

### ä¸¤ç§ç®—æ³•

```mermaid
graph TB
    Graph[çŸ¥è¯†å›¾è°±] --> Choice{ç®—æ³•é€‰æ‹©}

    Choice -->|é»˜è®¤| Leiden[Leidenç®—æ³•<br/>å±‚æ¬¡åŒ–ç¤¾åŒºæ£€æµ‹]
    Choice -->|å¤‡é€‰| SLLPA[SLLPAç®—æ³•<br/>æ ‡ç­¾ä¼ æ’­]

    Leiden --> Check{æ˜¯å¦å‘ç°ç¤¾åŒº?}
    Check -->|æ˜¯| Result[ç¤¾åŒºç»“æ„]
    Check -->|å¦| SLLPA

    SLLPA --> Result

    style Leiden fill:#e3f2fd
    style SLLPA fill:#fff3e0
    style Result fill:#e8f5e9
```

### Leiden ç®—æ³•

**ç‰¹ç‚¹**ï¼šå±‚æ¬¡åŒ–ç¤¾åŒºæ£€æµ‹ï¼Œè´¨é‡é«˜

```cypher
CALL gds.graph.project(
    'entity-graph',
    '__Entity__',
    {
        RELATES_TO: {
            orientation: 'UNDIRECTED'
        }
    }
)

CALL gds.leiden.write('entity-graph', {
    writeProperty: 'community',
    includeIntermediateCommunities: true,
    maxLevels: 3
})
YIELD communityCount, nodePropertiesWritten
```

### SLLPA ç®—æ³•

**ç‰¹ç‚¹**ï¼šSpeaker-Listener æ ‡ç­¾ä¼ æ’­ï¼Œé€Ÿåº¦å¿«

```cypher
CALL gds.alpha.sllpa.write('entity-graph', {
    writeProperty: 'community',
    maxIterations: 10
})
```

### ç¤¾åŒºæ‘˜è¦ç”Ÿæˆ

```python
class CommunityDetector:
    def generate_summaries(self):
        """ä¸ºæ¯ä¸ªç¤¾åŒºç”Ÿæˆæ‘˜è¦"""
        communities = self._get_communities()

        for community in communities:
            # è·å–ç¤¾åŒºå†…çš„å®ä½“
            entities = self._get_community_entities(community['id'])

            # ä½¿ç”¨ LLM ç”Ÿæˆæ‘˜è¦
            summary = self._generate_summary_with_llm(entities)

            # å†™å…¥ç¤¾åŒºèŠ‚ç‚¹
            self.graph.query("""
                MATCH (c:`__Community__` {id: $community_id})
                SET c.summary = $summary,
                    c.full_content = $full_content
            """, params={
                'community_id': community['id'],
                'summary': summary['summary'],
                'full_content': summary['full_content']
            })

    def _generate_summary_with_llm(self, entities: List[Dict]) -> Dict:
        """ä½¿ç”¨ LLM ç”Ÿæˆç¤¾åŒºæ‘˜è¦"""
        prompt = f"""
        è¯·ä¸ºä»¥ä¸‹å®ä½“ç»„ç”Ÿæˆä¸€ä¸ªç®€æ´çš„æ‘˜è¦ï¼ˆ100å­—ä»¥å†…ï¼‰ï¼š

        {self._format_entities(entities)}

        æ‘˜è¦åº”åŒ…å«ï¼š
        1. è¿™äº›å®ä½“çš„å…±åŒä¸»é¢˜
        2. ä¸»è¦å®ä½“åŠå…¶å…³ç³»
        3. æ ¸å¿ƒçŸ¥è¯†ç‚¹
        """

        response = self.llm.invoke(prompt)

        return {
            'summary': response.content,
            'full_content': self._format_entities(entities)
        }
```

**é…ç½®å‚æ•°**ï¼š
```env
GRAPH_COMMUNITY_ALGORITHM=leiden    # leiden / sllpa
GDS_MEMORY_LIMIT=6                  # GDS å†…å­˜é™åˆ¶ï¼ˆGBï¼‰
GDS_CONCURRENCY=4                   # GDS å¹¶å‘åº¦
```

---

## å‘é‡ç´¢å¼•æ„å»º

### ä¸¤ç§ç´¢å¼•

```mermaid
graph TB
    subgraph å®ä½“ç´¢å¼•[å®ä½“ç´¢å¼• Entity Index]
        E1[å®ä½“1: å›½å®¶å¥–å­¦é‡‘<br/>embedding: [0.1, 0.2, ...]]
        E2[å®ä½“2: å­¦ç”Ÿ<br/>embedding: [0.3, 0.4, ...]]
    end

    subgraph Chunkç´¢å¼•[Chunkç´¢å¼• Chunk Index]
        C1[Chunk 1: text + embedding]
        C2[Chunk 2: text + embedding]
    end

    Query[æŸ¥è¯¢: å¥–å­¦é‡‘ç”³è¯·æ¡ä»¶] --> VecEmbed[å‘é‡åŒ–]
    VecEmbed --> EntitySearch[å®ä½“ç´¢å¼•æœç´¢]
    VecEmbed --> ChunkSearch[Chunkç´¢å¼•æœç´¢]

    EntitySearch --> E1
    ChunkSearch --> C1

    style å®ä½“ç´¢å¼• fill:#e3f2fd
    style Chunkç´¢å¼• fill:#fff3e0
```

### å®ä½“ç´¢å¼•

**åˆ›å»ºç´¢å¼•**ï¼š

```cypher
CREATE VECTOR INDEX entity_index IF NOT EXISTS
FOR (e:`__Entity__`)
ON e.embedding
OPTIONS {
    indexConfig: {
        `vector.dimensions`: 1536,
        `vector.similarity_function`: 'cosine'
    }
}
```

**ä½¿ç”¨ç´¢å¼•**ï¼š

```python
def entity_search(query: str, top_k: int = 5) -> List[Dict]:
    """å®ä½“å‘é‡æœç´¢"""
    # å‘é‡åŒ–æŸ¥è¯¢
    query_vec = embeddings.embed_query(query)

    # å‘é‡æœç´¢
    cypher = """
    CALL db.index.vector.queryNodes(
        'entity_index',
        $top_k,
        $query_vec
    )
    YIELD node, score
    RETURN node.id AS entity_id,
           node.description AS description,
           score
    """

    results = graph.query(cypher, params={
        'top_k': top_k,
        'query_vec': query_vec
    })

    return results
```

### Chunk ç´¢å¼•

**åˆ›å»ºç´¢å¼•**ï¼š

```cypher
CREATE VECTOR INDEX chunk_index IF NOT EXISTS
FOR (c:`__Chunk__`)
ON c.embedding
OPTIONS {
    indexConfig: {
        `vector.dimensions`: 1536,
        `vector.similarity_function`: 'cosine'
    }
}
```

**ä¾èµ–å…³ç³»**ï¼š

```mermaid
graph LR
    E[å®ä½“èŠ‚ç‚¹<br/>å¿…é¡»å·²å­˜åœ¨] --> EI[å®ä½“ç´¢å¼•æ„å»º]
    EI --> CI[Chunkç´¢å¼•æ„å»º]

    style E fill:#ffebee
    style EI fill:#fff3e0
    style CI fill:#e8f5e9
```

**é‡è¦æç¤º**ï¼š
- Chunk ç´¢å¼•æ„å»ºä¾èµ–å®ä½“ç´¢å¼•
- å¿…é¡»å…ˆå®Œæˆå®ä½“ç´¢å¼•ï¼Œå†æ„å»º Chunk ç´¢å¼•
- å¦åˆ™ä¼šå‡ºç°å…³ç³»ç¼ºå¤±é”™è¯¯

---

## å¢é‡ vs å…¨é‡æ„å»º

### å¯¹æ¯”è¡¨

| ç‰¹æ€§ | å…¨é‡æ„å»º | å¢é‡æ„å»º |
|------|----------|----------|
| **æ‰§è¡Œæ–¹å¼** | æ¸…ç©ºæ•°æ®åº“ â†’ é‡æ–°æ„å»º | æ£€æµ‹å˜æ›´ â†’ ä»…å¤„ç†å˜æ›´ |
| **è€—æ—¶** | é•¿ï¼ˆå°æ—¶çº§ï¼‰ | çŸ­ï¼ˆåˆ†é’Ÿçº§ï¼‰ |
| **é€‚ç”¨åœºæ™¯** | åˆæ¬¡æ„å»ºã€å¤§è§„æ¨¡å˜æ›´ | æ—¥å¸¸æ›´æ–°ã€æ–°å¢æ–‡æ¡£ |
| **æ–‡ä»¶è¿½è¸ª** | æ—  | file_registry.json |
| **å†²çªè§£å†³** | æ— å†²çª | éœ€è¦ç­–ç•¥ |
| **èµ„æºæ¶ˆè€—** | é«˜ | ä½ |

### å…¨é‡æ„å»º

**æ‰§è¡Œå‘½ä»¤**ï¼š
```bash
bash scripts/py.sh infrastructure.integrations.build.main
```

**æµç¨‹**ï¼š
```mermaid
sequenceDiagram
    participant U as ç”¨æˆ·
    participant KB as KnowledgeGraphBuilder
    participant DB as Neo4j

    U->>KB: å¯åŠ¨å…¨é‡æ„å»º
    KB->>DB: æ¸…ç©ºæ‰€æœ‰ç´¢å¼•
    KB->>DB: æ¸…ç©ºæ‰€æœ‰æ•°æ®
    KB->>KB: å¤„ç†æ‰€æœ‰æ–‡æ¡£
    KB->>DB: å†™å…¥å®Œæ•´å›¾è°±
    KB->>DB: æ„å»ºç´¢å¼•
    KB->>DB: ç¤¾åŒºæ£€æµ‹
    DB-->>U: æ„å»ºå®Œæˆ
```

### å¢é‡æ„å»º

**æ‰§è¡Œå‘½ä»¤**ï¼š
```bash
# å•æ¬¡å¢é‡æ„å»º
bash scripts/py.sh infrastructure.integrations.build.incremental_update --once

# å®ˆæŠ¤è¿›ç¨‹æ¨¡å¼ï¼ˆå®šæœŸæ£€æŸ¥ï¼‰
bash scripts/py.sh infrastructure.integrations.build.incremental_update --daemon
```

**æ–‡ä»¶è¿½è¸ª**ï¼š

```json
// file_registry.json
{
    "files": {
        "/files/doc1.pdf": {
            "hash": "abc123",
            "modified_time": "2026-01-04T10:00:00",
            "status": "processed"
        },
        "/files/doc2.txt": {
            "hash": "def456",
            "modified_time": "2026-01-04T11:00:00",
            "status": "processed"
        }
    },
    "last_update": "2026-01-04T12:00:00"
}
```

**å˜æ›´æ£€æµ‹**ï¼š

```python
class IncrementalBuilder:
    def detect_changes(self) -> Dict[str, List[str]]:
        """æ£€æµ‹æ–‡ä»¶å˜æ›´"""
        changes = {
            'added': [],      # æ–°å¢æ–‡ä»¶
            'modified': [],   # ä¿®æ”¹æ–‡ä»¶
            'deleted': []     # åˆ é™¤æ–‡ä»¶
        }

        # 1. æ£€æµ‹æ–°å¢å’Œä¿®æ”¹
        for file_path in self._list_files():
            current_hash = self._compute_hash(file_path)
            registry_info = self.registry.get(file_path)

            if not registry_info:
                changes['added'].append(file_path)
            elif registry_info['hash'] != current_hash:
                changes['modified'].append(file_path)

        # 2. æ£€æµ‹åˆ é™¤
        for file_path in self.registry.keys():
            if not os.path.exists(file_path):
                changes['deleted'].append(file_path)

        return changes
```

**å†²çªè§£å†³**ï¼š

```python
def resolve_conflict(self, entity1: Dict, entity2: Dict) -> Dict:
    """è§£å†³å®ä½“å†²çª"""
    if self.strategy == "manual_first":
        # ä¼˜å…ˆä¿ç•™äººå·¥å®¡æ ¸çš„
        return entity1 if entity1.get('manual_verified') else entity2

    elif self.strategy == "auto_first":
        # ä¼˜å…ˆä¿ç•™è‡ªåŠ¨æå–çš„æœ€æ–°ç‰ˆæœ¬
        return entity2

    elif self.strategy == "merge":
        # åˆå¹¶å±æ€§
        return {
            'id': entity1['id'],
            'description': self._merge_descriptions([
                entity1['description'],
                entity2['description']
            ]),
            'type': entity1['type']
        }
```

---

## é…ç½®å‚æ•°è¯¦è§£

### settings.py å‚æ•°

```python
# backend/config/rag.pyï¼ˆé¢†åŸŸ/RAG è¯­ä¹‰ï¼›æ¨èç”¨ .env è¦†ç›–ï¼‰

# å¯¹åº” .envï¼š
# GRAPH_THEME='å­¦ç”Ÿäº‹åŠ¡ç®¡ç†'
# GRAPH_ENTITY_TYPES='å­¦ç”Ÿç±»å‹,å¥–å­¦é‡‘ç±»å‹,å¤„åˆ†ç±»å‹,éƒ¨é—¨,å­¦ç”ŸèŒè´£,ç®¡ç†è§„å®š'
# GRAPH_RELATIONSHIP_TYPES='ç”³è¯·,è¯„é€‰,è¿çºª,èµ„åŠ©,ç”³è¯‰,ç®¡ç†,æƒåˆ©ä¹‰åŠ¡,äº’æ–¥'
# FRONTEND_EXAMPLES='...'

theme = "å­¦ç”Ÿäº‹åŠ¡ç®¡ç†"
entity_types = ["å­¦ç”Ÿç±»å‹", "å¥–å­¦é‡‘ç±»å‹", "å¤„åˆ†ç±»å‹", "éƒ¨é—¨", "å­¦ç”ŸèŒè´£", "ç®¡ç†è§„å®š"]
relationship_types = ["ç”³è¯·", "è¯„é€‰", "è¿çºª", "èµ„åŠ©", "ç”³è¯‰", "ç®¡ç†", "æƒåˆ©ä¹‰åŠ¡", "äº’æ–¥"]

# ç¤ºä¾‹ï¼ˆå°‘æ ·æœ¬å­¦ä¹  / å‰ç«¯ç¤ºä¾‹é—®é¢˜ï¼‰
examples = """
ç¤ºä¾‹1:
å­¦ç”Ÿ : ç”³è¯· : å›½å®¶å¥–å­¦é‡‘
å›½å®¶å¥–å­¦é‡‘ : è¯„é€‰ : è¯„å®¡å§”å‘˜ä¼š

ç¤ºä¾‹2:
å­¦ç”Ÿ : è¿å : å­¦æ ¡è§„å®š
å­¦æ ¡è§„å®š : å¤„åˆ† : è­¦å‘Š
"""
```

### .env å‚æ•°

```bash
# ========== LLM é…ç½® ==========
OPENAI_API_KEY=sk-xxx
OPENAI_BASE_URL=http://localhost:13000/v1
OPENAI_LLM_MODEL=gpt-4o
OPENAI_EMBEDDINGS_MODEL=text-embedding-3-large
OPENAI_REQUEST_TIMEOUT_SECONDS=120

# ========== Neo4j é…ç½® ==========
NEO4J_URI=neo4j://localhost:7687
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=12345678

# ========== æ–‡æ¡£å¤„ç† ==========
FILES_DIR=./files
CHUNK_SIZE=512
OVERLAP=50

# ========== å¹¶è¡Œå¤„ç† ==========
MAX_WORKERS=4              # çº¿ç¨‹æ± å¤§å°
BATCH_SIZE=100             # æ•°æ®åº“æ‰¹å¤„ç†
LLM_BATCH_SIZE=5           # LLM æ‰¹å¤„ç†
ENTITY_BATCH_SIZE=50       # å®ä½“æ‰¹å¤„ç†
CHUNK_BATCH_SIZE=100       # Chunk æ‰¹å¤„ç†
EMBEDDING_BATCH_SIZE=64    # å‘é‡ç”Ÿæˆæ‰¹å¤„ç†

# ========== å®ä½“æ¶ˆæ­§ ==========
DISAMBIG_STRING_THRESHOLD=0.7
DISAMBIG_VECTOR_THRESHOLD=0.8
DISAMBIG_NIL_THRESHOLD=0.75
DISAMBIG_TOP_K=10

# ========== ç¤¾åŒºæ£€æµ‹ ==========
GRAPH_COMMUNITY_ALGORITHM=leiden  # leiden / sllpa
GDS_MEMORY_LIMIT=6                # GB
GDS_CONCURRENCY=4

# ========== å†²çªè§£å†³ ==========
GRAPH_CONFLICT_STRATEGY=merge     # manual_first / auto_first / merge

# ========== æ„å»ºæ¨¡å¼ï¼ˆä»¥å½“å‰å®ç°ä¸ºå‡†ï¼‰ ==========
# document: æ–‡æ¡£æ¨¡å¼ï¼ˆå¦‚â€œå­¦ç”Ÿç®¡ç†â€ï¼‰
# structured: ç»“æ„åŒ–æ¨¡å¼ï¼ˆå¦‚â€œç”µå½±æ¨èâ€ï¼‰
GRAPH_BUILD_MODE=document         # document / structured

# ========== é˜¶æ®µå¼€å…³ï¼ˆç»Ÿä¸€å…¥å£ï¼‰ ==========
# BUILD_RUN_GRAPH: æ˜¯å¦æ‰§è¡Œ Phase 2ï¼ˆå›¾è°±æ„å»ºï¼‰
# BUILD_RUN_INDEX_AND_COMMUNITY: æ˜¯å¦æ‰§è¡Œ Phase 3ï¼ˆå®ä½“ç´¢å¼•ä¸ç¤¾åŒºæ£€æµ‹ï¼‰
# BUILD_RUN_CHUNK_INDEX: æ˜¯å¦æ‰§è¡Œ Chunk å‘é‡ç´¢å¼•
# BUILD_DROP_ALL_INDEXES: æ˜¯å¦æ¸…ç†å…¨åº“ç´¢å¼•ï¼ˆåŒåº“å…±å­˜å»ºè®® falseï¼‰
# BUILD_RUN_GRAPH=true
# BUILD_RUN_INDEX_AND_COMMUNITY=true
# BUILD_RUN_CHUNK_INDEX=true
# BUILD_DROP_ALL_INDEXES=false
```

---

## æ€§èƒ½ä¼˜åŒ–

### ä¼˜åŒ–ç­–ç•¥æ€»è§ˆ

```mermaid
graph TB
    subgraph æå–ä¼˜åŒ–[æå–é˜¶æ®µä¼˜åŒ–]
        O1[ç¼“å­˜ LLM ç»“æœ]
        O2[å¹¶è¡Œå¤„ç†]
        O3[æ‰¹é‡è°ƒç”¨]
    end

    subgraph å†™å…¥ä¼˜åŒ–[å†™å…¥é˜¶æ®µä¼˜åŒ–]
        O4[æ‰¹é‡å†™å…¥]
        O5[äº‹åŠ¡åˆå¹¶]
        O6[ç´¢å¼•ä¼˜åŒ–]
    end

    subgraph æ¶ˆæ­§ä¼˜åŒ–[æ¶ˆæ­§é˜¶æ®µä¼˜åŒ–]
        O7[åˆ†æ‰¹å¤„ç†]
        O8[å‘é‡ç¼“å­˜]
        O9[æ—©åœç­–ç•¥]
    end

    style æå–ä¼˜åŒ– fill:#e3f2fd
    style å†™å…¥ä¼˜åŒ– fill:#fff3e0
    style æ¶ˆæ­§ä¼˜åŒ– fill:#e8f5e9
```

### å¹¶è¡Œå¤„ç†é…ç½®

```python
# æ¨èé…ç½®ï¼ˆ4æ ¸8çº¿ç¨‹æœºå™¨ï¼‰
MAX_WORKERS=4              # CPU æ ¸å¿ƒæ•°
LLM_BATCH_SIZE=5           # æ ¹æ® LLM é™æµè°ƒæ•´
BATCH_SIZE=100             # Neo4j å†™å…¥æ‰¹å¤§å°

# é«˜æ€§èƒ½é…ç½®ï¼ˆ16æ ¸32çº¿ç¨‹æœºå™¨ï¼‰
MAX_WORKERS=16
LLM_BATCH_SIZE=10
BATCH_SIZE=500
```

### æ„å»ºç¼“å­˜

v3 strict ä¸æä¾›æ„å»ºé˜¶æ®µçš„æœ¬åœ°è½ç›˜ç¼“å­˜ï¼›å¦‚éœ€è¿›ä¸€æ­¥ä¼˜åŒ–æ„å»ºè€—æ—¶ï¼Œå»ºè®®ä»æ‰¹å¤„ç†ã€å¹¶å‘åº¦ä¸ LLM è°ƒç”¨ç­–ç•¥å…¥æ‰‹ã€‚

### å†…å­˜ä¼˜åŒ–

```python
# åˆ†æ‰¹åŠ è½½å¤§æ–‡ä»¶
def process_large_file(file_path: str):
    """åˆ†æ‰¹å¤„ç†å¤§æ–‡ä»¶"""
    BATCH_SIZE = 10000  # æ¯æ‰¹å¤„ç† 10000 è¡Œ

    with open(file_path, 'r') as f:
        batch = []
        for line in f:
            batch.append(line)

            if len(batch) >= BATCH_SIZE:
                process_batch(batch)
                batch = []

        if batch:
            process_batch(batch)
```

### æ•°æ®åº“ä¼˜åŒ–

```cypher
-- åˆ›å»ºçº¦æŸï¼ˆåŠ é€ŸæŸ¥è¯¢ï¼‰
CREATE CONSTRAINT entity_id IF NOT EXISTS
FOR (e:`__Entity__`) REQUIRE e.id IS UNIQUE;

CREATE CONSTRAINT chunk_id IF NOT EXISTS
FOR (c:`__Chunk__`) REQUIRE c.id IS UNIQUE;

-- åˆ›å»ºç´¢å¼•
CREATE INDEX entity_type IF NOT EXISTS
FOR (e:`__Entity__`) ON (e.type);

CREATE INDEX entity_canonical IF NOT EXISTS
FOR (e:`__Entity__`) ON (e.canonical_id);
```

### æ€§èƒ½ç›‘æ§

```python
class PerformanceMonitor:
    def __init__(self):
        self.stats = {
            "æ–‡æ¡£å¤„ç†": 0,
            "å®ä½“æå–": 0,
            "å†™å…¥æ•°æ®åº“": 0,
            "å®ä½“æ¶ˆæ­§": 0,
            "ç¤¾åŒºæ£€æµ‹": 0,
            "ç´¢å¼•æ„å»º": 0
        }

    def record(self, stage: str, duration: float):
        """è®°å½•é˜¶æ®µè€—æ—¶"""
        self.stats[stage] = duration

    def report(self):
        """è¾“å‡ºæ€§èƒ½æŠ¥å‘Š"""
        total = sum(self.stats.values())

        print(f"\næ€§èƒ½æŠ¥å‘Šï¼š")
        print(f"æ€»è€—æ—¶: {total:.2f}ç§’")
        print(f"\nå„é˜¶æ®µè€—æ—¶ï¼š")

        for stage, duration in sorted(self.stats.items(), key=lambda x: x[1], reverse=True):
            percentage = duration / total * 100
            print(f"  {stage}: {duration:.2f}ç§’ ({percentage:.1f}%)")
```

---

## ç›¸å…³æ–‡æ¡£

- [Agentç³»ç»Ÿ](./Agentç³»ç»Ÿ.md) - äº†è§£å¦‚ä½•ä½¿ç”¨æ„å»ºçš„çŸ¥è¯†å›¾è°±
- [æœç´¢å¼•æ“](./æœç´¢å¼•æ“.md) - äº†è§£çŸ¥è¯†å›¾è°±çš„æ£€ç´¢ç­–ç•¥
- [ç³»ç»Ÿæ¶æ„æ€»è§ˆ](../01-æ•´ä½“æ¶æ„/ç³»ç»Ÿæ¶æ„æ€»è§ˆ.md) - äº†è§£æ•´ä½“æ¶æ„
- [Neo4j å®˜æ–¹æ–‡æ¡£](https://neo4j.com/docs/) - Neo4j å›¾æ•°æ®åº“æ–‡æ¡£
- [LangChain å®˜æ–¹æ–‡æ¡£](https://python.langchain.com/) - LangChain æ¡†æ¶æ–‡æ¡£

---

## æ›´æ–°æ—¥å¿—

| ç‰ˆæœ¬ | æ—¥æœŸ | æ›´æ–°å†…å®¹ | ä½œè€… |
|------|------|----------|------|
| 1.0 | 2026-01-04 | åˆå§‹ç‰ˆæœ¬ï¼Œå®Œæ•´è¦†ç›–å›¾è°±æ„å»ºæµç¨‹ | Claude |
| - | - | - | - |
