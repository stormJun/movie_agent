# Chain of Explorationï¼ˆæ¢ç´¢é“¾ï¼‰

> **ç›®æ ‡è¯»è€…**ï¼šæ¶æ„å¸ˆã€ç ”ç©¶è€…
> **é˜…è¯»æ—¶é—´**ï¼š35 åˆ†é’Ÿ
> **å‰ç½®çŸ¥è¯†**ï¼šäº†è§£ DeepSearchã€çŸ¥è¯†å›¾è°±
> **éš¾åº¦ç­‰çº§**ï¼šâ­â­â­â­

## ğŸ“‹ æœ¬æ–‡å¤§çº²

- [1. ä»€ä¹ˆæ˜¯ Chain of Exploration](#1-ä»€ä¹ˆæ˜¯-chain-of-exploration)
- [2. æ¢ç´¢æœºåˆ¶](#2-æ¢ç´¢æœºåˆ¶)
- [3. æ¢ç´¢ç­–ç•¥](#3-æ¢ç´¢ç­–ç•¥)
- [4. è¯æ®è¿½è¸ª](#4-è¯æ®è¿½è¸ª)
- [5. ä¸ DeepResearchAgent çš„é›†æˆ](#5-ä¸-deepresearchagent-çš„é›†æˆ)
- [6. ä»£ç å®ç°](#6-ä»£ç å®ç°)
- [7. å®é™…æ¡ˆä¾‹](#7-å®é™…æ¡ˆä¾‹)
- [8. æ€§èƒ½ä¼˜åŒ–](#8-æ€§èƒ½ä¼˜åŒ–)

---

## 1. ä»€ä¹ˆæ˜¯ Chain of Exploration

### 1.1 å®šä¹‰

**Chain of Exploration**ï¼ˆæ¢ç´¢é“¾ï¼‰æ˜¯ä¸€ç§åœ¨**çŸ¥è¯†å›¾è°±ä¸Šè¿›è¡Œæ·±åº¦æ¢ç´¢çš„æœºåˆ¶**ï¼Œé€šè¿‡è¿­ä»£å¼çš„é‚»å±…æ‰©å±•å’Œè·¯å¾„è¯„åˆ†ï¼Œä»èµ·å§‹å®ä½“å‡ºå‘ï¼Œé€æ­¥å‘ç°ç›¸å…³å®ä½“å’Œå…³ç³»ï¼Œæ„å»ºå®Œæ•´çš„è¯æ®é“¾ã€‚

### 1.2 ä¸ Chain of Thought çš„åŒºåˆ«

**Chain of Thoughtï¼ˆæ€ç»´é“¾ï¼‰**ï¼š
```
é—®é¢˜ â†’ æ€è€ƒæ­¥éª¤1 â†’ æ€è€ƒæ­¥éª¤2 â†’ ... â†’ ç­”æ¡ˆ
ï¼ˆåœ¨æ–‡æœ¬ç©ºé—´ä¸­æ¨ç†ï¼‰
```

**Chain of Explorationï¼ˆæ¢ç´¢é“¾ï¼‰**ï¼š
```
é—®é¢˜ â†’ å‘é‡æ£€ç´¢èµ·å§‹èŠ‚ç‚¹ â†’ æ¢ç´¢1è·³é‚»å±… â†’ è¯„ä¼° â†’ æ¢ç´¢2è·³é‚»å±… â†’ ... â†’ ç­”æ¡ˆ
ï¼ˆåœ¨å›¾è°±ç©ºé—´ä¸­æ¢ç´¢ï¼‰
```

**æ ¸å¿ƒå·®å¼‚**ï¼š

| ç»´åº¦ | Chain of Thought | Chain of Exploration |
|------|------------------|----------------------|
| **æ¢ç´¢ç©ºé—´** | æ–‡æœ¬/æ€ç»´ç©ºé—´ | çŸ¥è¯†å›¾è°± |
| **å¯¼èˆªæ–¹å¼** | LLM æ¨ç† | å›¾éå† + LLM è¯„ä¼° |
| **ç»“æ„åŒ–ç¨‹åº¦** | ä½ï¼ˆè‡ªç”±æ–‡æœ¬ï¼‰ | é«˜ï¼ˆå®ä½“-å…³ç³»ï¼‰ |
| **å¯æº¯æºæ€§** | ä¸­ç­‰ | å¼ºï¼ˆæ˜ç¡®çš„å›¾è·¯å¾„ï¼‰ |
| **æ¨ç†ç±»å‹** | é€»è¾‘æ¨ç† | å…³ç³»æ¨ç† + é€»è¾‘æ¨ç† |

### 1.3 åœ¨çŸ¥è¯†å›¾è°±ä¸Šçš„æ¢ç´¢

**çŸ¥è¯†å›¾è°±ç»“æ„**ï¼š
```
å®ä½“ â”€å…³ç³»â†’ å®ä½“ â”€å…³ç³»â†’ å®ä½“
  â†“              â†“
å±æ€§            å±æ€§
```

**æ¢ç´¢è¿‡ç¨‹**ï¼š
```mermaid
graph TD
    A[èµ·å§‹å®ä½“: å›½å®¶å¥–å­¦é‡‘] --> B[1è·³é‚»å±…: å­¦ç”Ÿã€è¯„å®šæ¡ä»¶ã€é‡‘é¢]
    B --> C[2è·³é‚»å±…: æˆç»©è¦æ±‚ã€å¾·è‚²åˆ†æ•°ã€ç”³è¯·æµç¨‹]
    C --> D[3è·³é‚»å±…: å…·ä½“é‡åŒ–æ ‡å‡†ã€äº’æ–¥å…³ç³»]
    D --> E[æ„å»ºè¯æ®é“¾]
    E --> F[ç”Ÿæˆç­”æ¡ˆ]
```

**ä¼˜åŠ¿**ï¼š
- åˆ©ç”¨å›¾ç»“æ„çš„**å…³ç³»è¯­ä¹‰**
- é€šè¿‡**é‚»å±…æ‰©å±•**å‘ç°éšå«ä¿¡æ¯
- æä¾›**å¯éªŒè¯çš„æ¢ç´¢è·¯å¾„**

---

## 2. æ¢ç´¢æœºåˆ¶

### 2.1 èµ·å§‹èŠ‚ç‚¹é€‰æ‹©ï¼ˆå‘é‡æ£€ç´¢ï¼‰

**ç›®æ ‡**ï¼šä»ç”¨æˆ·é—®é¢˜å‡ºå‘ï¼Œæ‰¾åˆ°æœ€ç›¸å…³çš„å®ä½“ä½œä¸ºæ¢ç´¢èµ·ç‚¹ã€‚

**æ–¹æ³•**ï¼šå‘é‡ç›¸ä¼¼åº¦æ£€ç´¢

```python
# æ ¸å¿ƒä»£ç ï¼šbackend/graphrag_agent/search/local_search.py

def search(self, query: str) -> str:
    # åˆå§‹åŒ–å‘é‡å­˜å‚¨
    vector_store = from_existing_index(
        self.embeddings,
        index_name=self.index_name,  # "entity_index"
        retrieval_query=self.retrieval_query
    )

    # æ‰§è¡Œç›¸ä¼¼åº¦æœç´¢ï¼Œæ‰¾åˆ° top_k ä¸ªæœ€ç›¸å…³å®ä½“
    docs = vector_store.similarity_search(
        query,
        k=self.top_entities,  # é»˜è®¤ 5 ä¸ªå®ä½“
        params={
            "topChunks": self.top_chunks,
            "topCommunities": self.top_communities,
            # ...
        }
    )
```

**ç¤ºä¾‹**ï¼š

```
é—®é¢˜ï¼š"æ—·è¯¾å¤šå°‘å­¦æ—¶ä¼šè¢«é€€å­¦ï¼Ÿ"

å‘é‡æ£€ç´¢ç»“æœï¼ˆtop 3 å®ä½“ï¼‰ï¼š
1. å®ä½“: "é€€å­¦å¤„ç†" (ç›¸ä¼¼åº¦: 0.92)
2. å®ä½“: "æ—·è¯¾" (ç›¸ä¼¼åº¦: 0.89)
3. å®ä½“: "å­¦ç”Ÿçºªå¾‹å¤„åˆ†" (ç›¸ä¼¼åº¦: 0.85)

é€‰æ‹©èµ·å§‹èŠ‚ç‚¹ï¼š["é€€å­¦å¤„ç†", "æ—·è¯¾", "å­¦ç”Ÿçºªå¾‹å¤„åˆ†"]
```

### 2.2 é‚»å±…æ‰©å±•ï¼ˆ1è·³ã€2è·³ï¼‰

**1 è·³é‚»å±…**ï¼šç›´æ¥è¿æ¥çš„å®ä½“

**Cypher æŸ¥è¯¢**ï¼š
```cypher
// è·å–1è·³é‚»å±…
MATCH (start:__Entity__)-[r]-(neighbor:__Entity__)
WHERE start.id IN $start_entities
RETURN neighbor, r
ORDER BY r.weight DESC
LIMIT 20
```

**2 è·³é‚»å±…**ï¼šé€šè¿‡ä¸­é—´èŠ‚ç‚¹è¿æ¥çš„å®ä½“

```cypher
// è·å–2è·³é‚»å±…
MATCH (start:__Entity__)-[r1]-(mid:__Entity__)-[r2]-(neighbor:__Entity__)
WHERE start.id IN $start_entities
  AND neighbor <> start  // æ’é™¤å›åˆ°èµ·ç‚¹
RETURN neighbor, r1, mid, r2
ORDER BY r1.weight * r2.weight DESC
LIMIT 30
```

**ç¤ºä¾‹**ï¼š

```
èµ·å§‹å®ä½“ï¼š"æ—·è¯¾"

1è·³é‚»å±…ï¼š
  "æ—·è¯¾" -[å¯¼è‡´]-> "é€€å­¦å¤„ç†"
  "æ—·è¯¾" -[ç´¯è®¡åˆ°]-> "50å­¦æ—¶"
  "æ—·è¯¾" -[å±äº]-> "è¿çºªè¡Œä¸º"

2è·³é‚»å±…ï¼š
  "æ—·è¯¾" -[å¯¼è‡´]-> "é€€å­¦å¤„ç†" -[ä¾æ®]-> "å­¦ç”Ÿçºªå¾‹å¤„åˆ†ç®¡ç†è§„å®š"
  "æ—·è¯¾" -[ç´¯è®¡åˆ°]-> "50å­¦æ—¶" -[å®šä¹‰åœ¨]-> "å­¦ç”Ÿæ‰‹å†Œç¬¬15æ¡"
```

### 2.3 è·¯å¾„è¯„åˆ†å’Œå‰ªæ

**è·¯å¾„è¯„åˆ†ç®—æ³•**ï¼š

```python
def score_path(path, query_embedding):
    """
    ä¸ºæ¢ç´¢è·¯å¾„æ‰“åˆ†

    å‚æ•°:
        path: è·¯å¾„å¯¹è±¡ [å®ä½“1, å…³ç³»1, å®ä½“2, å…³ç³»2, ...]
        query_embedding: é—®é¢˜çš„å‘é‡è¡¨ç¤º

    è¿”å›:
        float: è·¯å¾„å¾—åˆ† (0-1)
    """
    score = 0.0

    # 1. è¯­ä¹‰ç›¸å…³æ€§ï¼ˆ30%æƒé‡ï¼‰
    entities = [p for i, p in enumerate(path) if i % 2 == 0]
    entity_embeddings = get_embeddings(entities)
    semantic_score = cosine_similarity(query_embedding, entity_embeddings).mean()
    score += 0.3 * semantic_score

    # 2. å…³ç³»æƒé‡ï¼ˆ40%æƒé‡ï¼‰
    relations = [p for i, p in enumerate(path) if i % 2 == 1]
    relation_weights = [r.weight for r in relations]
    weight_score = sum(relation_weights) / len(relation_weights) if relation_weights else 0
    score += 0.4 * weight_score

    # 3. è·¯å¾„é•¿åº¦æƒ©ç½šï¼ˆ20%æƒé‡ï¼‰
    path_length = len(entities) - 1
    length_penalty = 1.0 / (1.0 + 0.2 * path_length)  # è¶Šé•¿è¶Šä½
    score += 0.2 * length_penalty

    # 4. æ–°é¢–æ€§å¥–åŠ±ï¼ˆ10%æƒé‡ï¼‰
    # ä¼˜å…ˆæ¢ç´¢æœªè®¿é—®è¿‡çš„å®ä½“
    novelty_score = len([e for e in entities if e not in visited]) / len(entities)
    score += 0.1 * novelty_score

    return score
```

**å‰ªæç­–ç•¥**ï¼š

```python
def prune_paths(paths, top_k=10, threshold=0.6):
    """
    å‰ªæï¼šä¿ç•™é«˜è´¨é‡è·¯å¾„

    å‚æ•°:
        paths: æ‰€æœ‰å€™é€‰è·¯å¾„
        top_k: ä¿ç•™å‰kæ¡è·¯å¾„
        threshold: æœ€ä½åˆ†æ•°é˜ˆå€¼

    è¿”å›:
        List: å‰ªæåçš„è·¯å¾„åˆ—è¡¨
    """
    # 1. æŒ‰å¾—åˆ†æ’åº
    scored_paths = [(score_path(p, query_emb), p) for p in paths]
    scored_paths.sort(reverse=True, key=lambda x: x[0])

    # 2. è¿‡æ»¤ä½åˆ†è·¯å¾„
    filtered = [p for score, p in scored_paths if score >= threshold]

    # 3. å– top_k
    return filtered[:top_k]
```

### 2.4 ç»ˆæ­¢æ¡ä»¶

**æ¢ç´¢ä½•æ—¶åœæ­¢ï¼Ÿ**

```python
def should_stop_exploration(iteration, evidence, confidence):
    """
    åˆ¤æ–­æ˜¯å¦åœæ­¢æ¢ç´¢

    å‚æ•°:
        iteration: å½“å‰è¿­ä»£æ¬¡æ•°
        evidence: å·²æ”¶é›†çš„è¯æ®
        confidence: å½“å‰ç½®ä¿¡åº¦

    è¿”å›:
        bool: æ˜¯å¦åœæ­¢
    """
    # æ¡ä»¶1: è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°
    if iteration >= MAX_ITERATIONS:  # é»˜è®¤ 5
        return True

    # æ¡ä»¶2: ç½®ä¿¡åº¦è¶³å¤Ÿé«˜
    if confidence > CONFIDENCE_THRESHOLD:  # é»˜è®¤ 0.9
        return True

    # æ¡ä»¶3: æ— æ–°è¯æ®å‘ç°ï¼ˆè¿ç»­2è½®ï¼‰
    if len(evidence) > 0:
        recent_evidence = evidence[-2:]
        if all(len(e) == 0 for e in recent_evidence):
            return True

    # æ¡ä»¶4: LLM åˆ¤æ–­å·²æœ‰è¶³å¤Ÿä¿¡æ¯
    if llm_judges_sufficient(evidence, query):
        return True

    return False
```

---

## 3. æ¢ç´¢ç­–ç•¥

### 3.1 æ·±åº¦ä¼˜å…ˆ vs å¹¿åº¦ä¼˜å…ˆ

**æ·±åº¦ä¼˜å…ˆæœç´¢ï¼ˆDFSï¼‰**ï¼š

```
èµ·å§‹èŠ‚ç‚¹
  â””â”€ é‚»å±…1
      â””â”€ é‚»å±…1.1
          â””â”€ é‚»å±…1.1.1  â† æ·±å…¥æ¢ç´¢
      â””â”€ é‚»å±…1.2
  â””â”€ é‚»å±…2
```

**ä¼˜åŠ¿**ï¼š
- å¿«é€Ÿåˆ°è¾¾è¿œç«¯èŠ‚ç‚¹
- é€‚åˆæŸ¥æ‰¾ç‰¹å®šè·¯å¾„

**åŠ£åŠ¿**ï¼š
- å¯èƒ½é”™è¿‡è¿‘é‚»çš„é‡è¦ä¿¡æ¯

**å¹¿åº¦ä¼˜å…ˆæœç´¢ï¼ˆBFSï¼‰**ï¼š

```
èµ·å§‹èŠ‚ç‚¹
  â”œâ”€ é‚»å±…1
  â”œâ”€ é‚»å±…2
  â””â”€ é‚»å±…3     â† å…ˆæ¢ç´¢æ‰€æœ‰1è·³é‚»å±…
      â”œâ”€ é‚»å±…3.1
      â””â”€ é‚»å±…3.2  â† å†æ¢ç´¢2è·³é‚»å±…
```

**ä¼˜åŠ¿**ï¼š
- å…¨é¢è¦†ç›–è¿‘é‚»
- ä¸æ˜“é—æ¼é‡è¦ä¿¡æ¯

**åŠ£åŠ¿**ï¼š
- è®¡ç®—é‡å¤§
- å¯èƒ½é™·å…¥å±€éƒ¨

**æœ¬é¡¹ç›®çš„æ··åˆç­–ç•¥**ï¼š

```python
def hybrid_exploration(start_entities, max_depth=2):
    """
    æ··åˆæ¢ç´¢ç­–ç•¥ï¼šBFS + ç›¸å…³æ€§å‰ªæ

    1. ç¬¬1è·³ï¼šBFS æ¢ç´¢æ‰€æœ‰ç›´æ¥é‚»å±…
    2. è¯„åˆ†å‰ªæï¼šä¿ç•™ top_k ä¸ªæœ€ç›¸å…³é‚»å±…
    3. ç¬¬2è·³ï¼šä»å‰ªæåçš„é‚»å±…ç»§ç»­ BFS
    """
    visited = set()
    evidence = []

    # ç¬¬1è·³ï¼šå¹¿åº¦ä¼˜å…ˆ
    neighbors_1hop = get_all_neighbors(start_entities)

    # è¯„åˆ†å’Œå‰ªæ
    scored_neighbors = score_entities(neighbors_1hop, query)
    top_neighbors = prune_entities(scored_neighbors, top_k=10)

    # æ”¶é›†è¯æ®
    evidence.extend(extract_evidence(top_neighbors))
    visited.update(top_neighbors)

    # ç¬¬2è·³ï¼šä»é«˜åˆ†é‚»å±…ç»§ç»­æ¢ç´¢
    for neighbor in top_neighbors:
        neighbors_2hop = get_all_neighbors([neighbor])
        new_neighbors = [n for n in neighbors_2hop if n not in visited]

        # å†æ¬¡è¯„åˆ†å’Œå‰ªæ
        scored_2hop = score_entities(new_neighbors, query)
        top_2hop = prune_entities(scored_2hop, top_k=5)

        evidence.extend(extract_evidence(top_2hop))
        visited.update(top_2hop)

    return evidence, visited
```

### 3.2 ç›¸å…³æ€§å¼•å¯¼çš„æ¢ç´¢

**æ ¸å¿ƒæ€æƒ³**ï¼šåˆ©ç”¨ LLM è¯„ä¼°æ¯ä¸ªé‚»å±…èŠ‚ç‚¹ä¸é—®é¢˜çš„ç›¸å…³æ€§ï¼Œä¼˜å…ˆæ¢ç´¢é«˜ç›¸å…³èŠ‚ç‚¹ã€‚

**å®ç°ä»£ç **ï¼ˆç®€åŒ–ç‰ˆï¼‰ï¼š

```python
def relevance_guided_exploration(start_entities, query):
    """
    ç›¸å…³æ€§å¼•å¯¼çš„æ¢ç´¢

    å‚æ•°:
        start_entities: èµ·å§‹å®ä½“åˆ—è¡¨
        query: ç”¨æˆ·é—®é¢˜

    è¿”å›:
        æ¢ç´¢ç»“æœ
    """
    current_level = start_entities
    explored_paths = []

    for depth in range(MAX_DEPTH):
        next_level = []

        for entity in current_level:
            # è·å–é‚»å±…
            neighbors = get_neighbors(entity)

            # ä¸ºæ¯ä¸ªé‚»å±…è¯„ä¼°ç›¸å…³æ€§
            for neighbor in neighbors:
                relevance = evaluate_relevance(neighbor, query, entity)

                if relevance > RELEVANCE_THRESHOLD:  # 0.7
                    next_level.append(neighbor)
                    explored_paths.append({
                        "from": entity,
                        "to": neighbor,
                        "relevance": relevance,
                        "depth": depth
                    })

        # å‰ªæï¼šåªä¿ç•™ top_k ä¸ªæœ€ç›¸å…³çš„é‚»å±…
        next_level = sorted(next_level,
                           key=lambda x: x["relevance"],
                           reverse=True)[:TOP_K]

        current_level = next_level

        if not current_level:
            break

    return explored_paths
```

**ç›¸å…³æ€§è¯„ä¼°**ï¼š

```python
def evaluate_relevance(neighbor, query, source_entity):
    """
    è¯„ä¼°é‚»å±…å®ä½“çš„ç›¸å…³æ€§

    ä½¿ç”¨ LLM æˆ–å‘é‡ç›¸ä¼¼åº¦
    """
    # æ–¹æ³•1: å‘é‡ç›¸ä¼¼åº¦
    query_emb = get_embedding(query)
    neighbor_emb = get_embedding(neighbor.description)
    semantic_sim = cosine_similarity(query_emb, neighbor_emb)

    # æ–¹æ³•2: å…³ç³»æƒé‡
    relation = get_relation(source_entity, neighbor)
    relation_weight = relation.weight / MAX_WEIGHT  # å½’ä¸€åŒ–

    # æ–¹æ³•3: LLM åˆ¤æ–­
    prompt = f"""
    é—®é¢˜ï¼š{query}
    å½“å‰å·²æ¢ç´¢åˆ°å®ä½“ï¼š{source_entity.name}
    å¾…è¯„ä¼°é‚»å±…å®ä½“ï¼š{neighbor.name}
    å…³ç³»ï¼š{relation.type}

    è¯·è¯„ä¼°è¯¥é‚»å±…å®ä½“å¯¹å›ç­”é—®é¢˜çš„ç›¸å…³æ€§ï¼ˆ0-1åˆ†ï¼‰ã€‚
    """
    llm_score = llm.invoke(prompt).extract_score()

    # ç»¼åˆå¾—åˆ†
    final_score = 0.4 * semantic_sim + 0.3 * relation_weight + 0.3 * llm_score
    return final_score
```

### 3.3 åŠ¨æ€è·¯å¾„è°ƒæ•´

**åœºæ™¯**ï¼šæ¢ç´¢è¿‡ç¨‹ä¸­å‘ç°æŸæ¡è·¯å¾„ä¸ç›¸å…³ï¼Œéœ€è¦å›æº¯å¹¶é€‰æ‹©æ–°è·¯å¾„ã€‚

**å›æº¯æœºåˆ¶**ï¼š

```python
class ExplorationTracker:
    """æ¢ç´¢è·¯å¾„è¿½è¸ªå™¨"""

    def __init__(self):
        self.path_stack = []  # è·¯å¾„æ ˆ
        self.visited = set()  # å·²è®¿é—®èŠ‚ç‚¹
        self.dead_ends = set()  # æ­»èƒ¡åŒ

    def explore(self, node):
        """æ¢ç´¢èŠ‚ç‚¹"""
        self.path_stack.append(node)
        self.visited.add(node)

    def backtrack(self):
        """å›æº¯åˆ°ä¸Šä¸€ä¸ªèŠ‚ç‚¹"""
        if self.path_stack:
            dead_node = self.path_stack.pop()
            self.dead_ends.add(dead_node)
            return self.path_stack[-1] if self.path_stack else None
        return None

    def should_backtrack(self, current_node, neighbors):
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥å›æº¯"""
        # æ¡ä»¶1: æ‰€æœ‰é‚»å±…éƒ½å·²è®¿é—®
        unvisited_neighbors = [n for n in neighbors if n not in self.visited]
        if not unvisited_neighbors:
            return True

        # æ¡ä»¶2: æ‰€æœ‰é‚»å±…éƒ½æ˜¯æ­»èƒ¡åŒ
        if all(n in self.dead_ends for n in neighbors):
            return True

        # æ¡ä»¶3: å½“å‰èŠ‚ç‚¹ç›¸å…³æ€§å¤ªä½
        if evaluate_relevance(current_node, query) < 0.3:
            return True

        return False
```

**åŠ¨æ€è°ƒæ•´ç¤ºä¾‹**ï¼š

```
é—®é¢˜ï¼š"å›½å®¶å¥–å­¦é‡‘å’ŒåŠ±å¿—å¥–å­¦é‡‘å¯ä»¥åŒæ—¶ç”³è¯·å—ï¼Ÿ"

åˆå§‹è·¯å¾„ï¼š
  å›½å®¶å¥–å­¦é‡‘ â†’ è¯„å®šæ ‡å‡† â†’ æˆç»©è¦æ±‚ (ç›¸å…³æ€§: 0.5)
  ï¼ˆä½ç›¸å…³æ€§ï¼Œä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼‰

è°ƒæ•´ç­–ç•¥ï¼š
  å›æº¯åˆ° "å›½å®¶å¥–å­¦é‡‘"
  é€‰æ‹©æ–°è·¯å¾„ï¼š
    å›½å®¶å¥–å­¦é‡‘ â†’ äº’æ–¥å…³ç³» â†’ åŠ±å¿—å¥–å­¦é‡‘ (ç›¸å…³æ€§: 0.95)
    ï¼ˆé«˜ç›¸å…³æ€§ï¼Œå‘ç°å…³é”®ä¿¡æ¯ï¼šä¸¤è€…äº’æ–¥ï¼‰
```

---

## 4. è¯æ®è¿½è¸ª

### 4.1 æ¢ç´¢è·¯å¾„è®°å½•

**æ•°æ®ç»“æ„**ï¼š

```python
@dataclass
class ExplorationStep:
    """æ¢ç´¢æ­¥éª¤"""
    step_number: int
    node_id: str
    node_type: str  # entity / chunk / community
    node_description: str
    from_node: Optional[str]
    relation: Optional[str]
    reasoning: str  # LLM ç”Ÿæˆçš„æ¨ç†è¿‡ç¨‹
    evidence: List[str]  # è¯¥æ­¥éª¤æ”¶é›†çš„è¯æ®
    score: float  # ç›¸å…³æ€§å¾—åˆ†
    timestamp: datetime

class ExplorationPath:
    """å®Œæ•´æ¢ç´¢è·¯å¾„"""

    def __init__(self, query: str):
        self.query = query
        self.steps: List[ExplorationStep] = []
        self.total_evidence = []

    def add_step(self, step: ExplorationStep):
        """æ·»åŠ æ¢ç´¢æ­¥éª¤"""
        self.steps.append(step)
        self.total_evidence.extend(step.evidence)

    def get_path_summary(self) -> str:
        """è·å–è·¯å¾„æ‘˜è¦"""
        summary = f"é—®é¢˜ï¼š{self.query}\n\næ¢ç´¢è·¯å¾„ï¼š\n"
        for step in self.steps:
            summary += f"{step.step_number}. {step.node_id}"
            if step.relation:
                summary += f" --[{step.relation}]--> "
            summary += f"\n   æ¨ç†ï¼š{step.reasoning}\n"
            summary += f"   è¯æ®æ•°é‡ï¼š{len(step.evidence)}\n\n"
        return summary
```

**è®°å½•ç¤ºä¾‹**ï¼š

```python
# æ¢ç´¢è¿‡ç¨‹è®°å½•
exploration = ExplorationPath(query="æ—·è¯¾å¤šå°‘å­¦æ—¶ä¼šè¢«é€€å­¦ï¼Ÿ")

# æ­¥éª¤1: èµ·å§‹å®ä½“
exploration.add_step(ExplorationStep(
    step_number=1,
    node_id="æ—·è¯¾",
    node_type="entity",
    node_description="å­¦ç”Ÿæœªç»æ‰¹å‡†ç¼ºå¸­æ•™å­¦æ´»åŠ¨",
    from_node=None,
    relation=None,
    reasoning="é€šè¿‡å‘é‡æ£€ç´¢ï¼Œ'æ—·è¯¾'æ˜¯ä¸é—®é¢˜æœ€ç›¸å…³çš„èµ·å§‹å®ä½“",
    evidence=[],
    score=0.92
))

# æ­¥éª¤2: æ¢ç´¢1è·³é‚»å±…
exploration.add_step(ExplorationStep(
    step_number=2,
    node_id="é€€å­¦å¤„ç†",
    node_type="entity",
    node_description="å­¦æ ¡å¯¹ä¸¥é‡è¿çºªå­¦ç”Ÿçš„å¤„åˆ†æ–¹å¼",
    from_node="æ—·è¯¾",
    relation="å¯¼è‡´",
    reasoning="æ—·è¯¾ç´¯è®¡åˆ°ä¸€å®šç¨‹åº¦ä¼šå¯¼è‡´é€€å­¦å¤„ç†ï¼Œè¿™æ˜¯å…³é”®ä¿¡æ¯",
    evidence=["å­¦ç”Ÿçºªå¾‹å¤„åˆ†ç®¡ç†è§„å®šç¬¬15æ¡"],
    score=0.95
))

# æ­¥éª¤3: æ¢ç´¢2è·³é‚»å±…
exploration.add_step(ExplorationStep(
    step_number=3,
    node_id="50å­¦æ—¶",
    node_type="entity",
    node_description="æ—·è¯¾é€€å­¦çš„é‡åŒ–æ ‡å‡†",
    from_node="é€€å­¦å¤„ç†",
    relation="é‡åŒ–æ ‡å‡†",
    reasoning="æ‰¾åˆ°å…·ä½“çš„å­¦æ—¶æ•°é‡ï¼Œå¯ä»¥ç²¾ç¡®å›ç­”é—®é¢˜",
    evidence=["åœ¨ä¸€å­¦æœŸå†…æ—·è¯¾ç´¯è®¡è¾¾åˆ°50å­¦æ—¶ï¼Œç»™äºˆé€€å­¦å¤„ç†"],
    score=0.98
))
```

### 4.2 è¯æ®é“¾æ„å»º

**è¯æ®é“¾**ï¼šä»é—®é¢˜åˆ°ç­”æ¡ˆçš„å®Œæ•´æ¨ç†è·¯å¾„ã€‚

```python
class EvidenceChain:
    """è¯æ®é“¾"""

    def __init__(self):
        self.nodes = []  # è¯æ®èŠ‚ç‚¹
        self.links = []  # èŠ‚ç‚¹é—´çš„é€»è¾‘å…³ç³»

    def add_evidence(self, evidence: Dict):
        """
        æ·»åŠ è¯æ®èŠ‚ç‚¹

        evidence = {
            "content": "...",  # è¯æ®å†…å®¹
            "source": "...",  # æ¥æºæ–‡æ¡£
            "entity": "...",  # å…³è”å®ä½“
            "confidence": 0.95  # ç½®ä¿¡åº¦
        }
        """
        self.nodes.append(evidence)

    def link_evidences(self, from_idx: int, to_idx: int, relation: str):
        """
        å…³è”ä¸¤ä¸ªè¯æ®

        å‚æ•°:
            from_idx: æºè¯æ®ç´¢å¼•
            to_idx: ç›®æ ‡è¯æ®ç´¢å¼•
            relation: é€»è¾‘å…³ç³»ï¼ˆsupports / contradicts / extendsï¼‰
        """
        self.links.append({
            "from": from_idx,
            "to": to_idx,
            "relation": relation
        })

    def visualize(self) -> str:
        """å¯è§†åŒ–è¯æ®é“¾"""
        output = "è¯æ®é“¾ï¼š\n\n"

        for i, evidence in enumerate(self.nodes):
            output += f"[è¯æ®{i+1}] {evidence['content']}\n"
            output += f"  æ¥æºï¼š{evidence['source']}\n"
            output += f"  ç½®ä¿¡åº¦ï¼š{evidence['confidence']}\n"

            # æ˜¾ç¤ºä¸å…¶ä»–è¯æ®çš„å…³ç³»
            related = [l for l in self.links if l["from"] == i]
            if related:
                output += "  æ”¯æŒï¼š\n"
                for link in related:
                    next_evidence = self.nodes[link["to"]]
                    output += f"    â†’ [{link['relation']}] {next_evidence['content'][:50]}...\n"

            output += "\n"

        return output
```

**ç¤ºä¾‹è¯æ®é“¾**ï¼š

```
é—®é¢˜ï¼š"ä¼˜ç§€å­¦ç”Ÿå¯ä»¥ç”³è¯·å›½å®¶å¥–å­¦é‡‘å—ï¼Ÿ"

è¯æ®é“¾ï¼š

[è¯æ®1] ä¼˜ç§€å­¦ç”Ÿæ˜¯æŒ‡å¾·æ™ºä½“ç¾å…¨é¢å‘å±•çš„å­¦ç”Ÿ
  æ¥æºï¼šå­¦ç”Ÿç®¡ç†è§„å®šç¬¬3ç« 
  ç½®ä¿¡åº¦ï¼š0.90
  æ”¯æŒï¼š
    â†’ [extends] ä¼˜ç§€å­¦ç”Ÿè¯„å®šéœ€è¦æ»¡è¶³æˆç»©å’Œå¾·è‚²åŒé‡æ ‡å‡†

[è¯æ®2] ä¼˜ç§€å­¦ç”Ÿè¯„å®šéœ€è¦æ»¡è¶³æˆç»©å’Œå¾·è‚²åŒé‡æ ‡å‡†
  æ¥æºï¼šä¼˜ç§€å­¦ç”Ÿè¯„å®šåŠæ³•ç¬¬5æ¡
  ç½®ä¿¡åº¦ï¼š0.92
  æ”¯æŒï¼š
    â†’ [relates_to] å›½å®¶å¥–å­¦é‡‘è¯„å®šä¹Ÿéœ€è¦æˆç»©å’Œå¾·è‚²è¦æ±‚

[è¯æ®3] å›½å®¶å¥–å­¦é‡‘è¯„å®šä¹Ÿéœ€è¦æˆç»©å’Œå¾·è‚²è¦æ±‚
  æ¥æºï¼šå›½å®¶å¥–å­¦é‡‘ç®¡ç†åŠæ³•ç¬¬4æ¡
  ç½®ä¿¡åº¦ï¼š0.95
  æ”¯æŒï¼š
    â†’ [concludes] ä¼˜ç§€å­¦ç”Ÿç¬¦åˆå›½å®¶å¥–å­¦é‡‘çš„éƒ¨åˆ†æ¡ä»¶ï¼Œä½†ä¸æ˜¯å……åˆ†æ¡ä»¶

[è¯æ®4] å›½å®¶å¥–å­¦é‡‘è¯„å®šä¸­ï¼Œä¼˜ç§€å­¦ç”Ÿè£èª‰å¯ä½œä¸ºåŠ åˆ†é¡¹
  æ¥æºï¼šå›½å®¶å¥–å­¦é‡‘è¯„å®šç»†åˆ™ç¬¬7æ¡
  ç½®ä¿¡åº¦ï¼š0.88

ç»¼åˆç»“è®ºï¼šä¼˜ç§€å­¦ç”Ÿå¯ä»¥ç”³è¯·å›½å®¶å¥–å­¦é‡‘ï¼Œä¸”è£èª‰ç§°å·å¯ä»¥ä½œä¸ºåŠ åˆ†é¡¹ï¼Œä½†ä»éœ€æ»¡è¶³å…¶ä»–æ¡ä»¶ï¼ˆæˆç»©æ’åã€æ— è¿çºªç­‰ï¼‰ã€‚
```

### 4.3 ç½®ä¿¡åº¦è¯„ä¼°

**ç½®ä¿¡åº¦æ¥æº**ï¼š

```python
def calculate_confidence(evidence_chain):
    """
    è®¡ç®—è¯æ®é“¾çš„æ€»ä½“ç½®ä¿¡åº¦

    å‚æ•°:
        evidence_chain: è¯æ®é“¾å¯¹è±¡

    è¿”å›:
        float: æ€»ä½“ç½®ä¿¡åº¦ (0-1)
    """
    # å› ç´ 1: è¯æ®æ•°é‡ï¼ˆè¶Šå¤šè¶Šå¥½ï¼Œä½†è¾¹é™…é€’å‡ï¼‰
    num_evidence = len(evidence_chain.nodes)
    quantity_score = min(1.0, num_evidence / 5)  # 5ä¸ªè¯æ®ä¸ºæ»¡åˆ†

    # å› ç´ 2: å•ä¸ªè¯æ®çš„å¹³å‡ç½®ä¿¡åº¦
    avg_confidence = sum(e["confidence"] for e in evidence_chain.nodes) / num_evidence

    # å› ç´ 3: è¯æ®æ¥æºçš„æƒå¨æ€§
    source_weights = {
        "å®˜æ–¹æ–‡ä»¶": 1.0,
        "çŸ¥è¯†å›¾è°±": 0.9,
        "ç¤¾åŒºæ‘˜è¦": 0.7,
        "æ–‡æœ¬ç‰‡æ®µ": 0.6,
    }
    avg_source_weight = sum(
        source_weights.get(e["source"], 0.5) for e in evidence_chain.nodes
    ) / num_evidence

    # å› ç´ 4: è¯æ®é—´çš„ä¸€è‡´æ€§ï¼ˆæ— çŸ›ç›¾åˆ™åŠ åˆ†ï¼‰
    contradictions = sum(1 for link in evidence_chain.links if link["relation"] == "contradicts")
    consistency_penalty = 0.9 ** contradictions  # æ¯ä¸ªçŸ›ç›¾é™ä½10%

    # ç»¼åˆè®¡ç®—
    confidence = (
        0.2 * quantity_score +
        0.4 * avg_confidence +
        0.3 * avg_source_weight +
        0.1
    ) * consistency_penalty

    return min(1.0, confidence)
```

**ç½®ä¿¡åº¦åˆ†çº§**ï¼š

| ç½®ä¿¡åº¦èŒƒå›´ | ç­‰çº§ | è¯´æ˜ |
|-----------|------|------|
| 0.9 - 1.0 | æé«˜ | æœ‰å……åˆ†çš„æƒå¨è¯æ®æ”¯æŒ |
| 0.7 - 0.9 | é«˜ | æœ‰å¤šä¸ªè¯æ®æ”¯æŒï¼Œä½†å¯èƒ½ç¼ºå°‘éƒ¨åˆ†ç»†èŠ‚ |
| 0.5 - 0.7 | ä¸­ | æœ‰ä¸€å®šè¯æ®ï¼Œä½†å¯èƒ½å­˜åœ¨çŸ›ç›¾æˆ–ä¿¡æ¯ä¸å…¨ |
| 0.3 - 0.5 | ä½ | è¯æ®ä¸è¶³æˆ–è´¨é‡è¾ƒä½ |
| 0.0 - 0.3 | æä½ | å‡ ä¹æ²¡æœ‰å¯é è¯æ® |

---

## 5. ä¸ DeepResearchAgent çš„é›†æˆ

### 5.1 Think-Search-Explore å¾ªç¯

**DeepResearchAgent çš„å·¥ä½œæµç¨‹**ï¼š

```python
# backend/graphrag_agent/agents/deep_research_agent.py

def ask_with_thinking(self, query: str):
    """
    å®Œæ•´çš„æ€è€ƒ-æœç´¢-æ¢ç´¢å¾ªç¯
    """
    # åˆå§‹åŒ–
    exploration_path = ExplorationPath(query)
    evidence_chain = EvidenceChain()
    confidence = 0.0

    for iteration in range(MAX_ITERATIONS):
        # ========== é˜¶æ®µ1: Thinkï¼ˆæ€è€ƒï¼‰ ==========
        # ç”Ÿæˆå­æŸ¥è¯¢
        sub_queries = self.query_generator.generate_sub_queries(query)

        # ========== é˜¶æ®µ2: Searchï¼ˆæœç´¢ï¼‰ ==========
        for sub_q in sub_queries:
            # æ‰§è¡ŒåŒè·¯å¾„æœç´¢
            search_result = self.dual_searcher.search(sub_q)

            # ========== é˜¶æ®µ3: Exploreï¼ˆæ¢ç´¢ï¼‰ ==========
            # ä»æœç´¢ç»“æœä¸­æå–èµ·å§‹å®ä½“
            start_entities = extract_entities(search_result)

            # åœ¨çŸ¥è¯†å›¾è°±ä¸Šè¿›è¡Œ Chain of Exploration
            explored = self.explore_on_graph(
                start_entities=start_entities,
                query=sub_q,
                max_depth=2
            )

            # è®°å½•æ¢ç´¢è·¯å¾„
            exploration_path.steps.extend(explored["steps"])

            # æ„å»ºè¯æ®é“¾
            for evidence in explored["evidence"]:
                evidence_chain.add_evidence(evidence)

            # æ›´æ–°ç½®ä¿¡åº¦
            confidence = calculate_confidence(evidence_chain)

        # ========== é˜¶æ®µ4: Evaluateï¼ˆè¯„ä¼°ï¼‰ ==========
        if confidence > CONFIDENCE_THRESHOLD:
            break

        # ç”Ÿæˆè·Ÿè¿›æŸ¥è¯¢
        followup_queries = self.query_generator.generate_followup_queries(
            query, evidence_chain.get_summary()
        )

        if not followup_queries:
            break

    # ========== é˜¶æ®µ5: Synthesizeï¼ˆç»¼åˆï¼‰ ==========
    final_answer = self.synthesize_answer(
        query=query,
        exploration_path=exploration_path,
        evidence_chain=evidence_chain
    )

    return {
        "answer": final_answer,
        "thinking_process": exploration_path.get_path_summary(),
        "evidence": evidence_chain.visualize(),
        "confidence": confidence
    }
```

### 5.2 å¤šè½®æ¢ç´¢è¿­ä»£

**è¿­ä»£è¿‡ç¨‹ç¤ºä¾‹**ï¼š

```
é—®é¢˜ï¼š"å­¦ç”Ÿå› ç—…ä¸èƒ½å‚åŠ è€ƒè¯•æ€ä¹ˆåŠï¼Ÿ"

========== ç¬¬1è½®è¿­ä»£ ==========
ã€æ€è€ƒã€‘éœ€è¦äº†è§£è¯·å‡åˆ¶åº¦å’Œç¼“è€ƒæ”¿ç­–

ã€æœç´¢1ã€‘"å­¦ç”Ÿè¯·å‡åˆ¶åº¦"
  â””â”€ æ¢ç´¢ï¼š
      è¯·å‡ â†’ ç—…å‡ â†’ éœ€è¦åŒ»é™¢è¯æ˜
      è¯æ®ï¼šå­¦ç”Ÿè¯·å‡ç®¡ç†åŠæ³•ç¬¬3æ¡

ã€æœç´¢2ã€‘"ç¼“è€ƒæ”¿ç­–"
  â””â”€ æ¢ç´¢ï¼š
      ç¼“è€ƒ â†’ ç”³è¯·æ¡ä»¶ â†’ å› ç—…å¯ä»¥ç”³è¯·
      è¯æ®ï¼šè€ƒè¯•ç®¡ç†è§„å®šç¬¬12æ¡

ã€è¯„ä¼°ã€‘ç½®ä¿¡åº¦ï¼š0.65ï¼ˆä¸­ç­‰ï¼‰
ã€åˆ¤æ–­ã€‘ä¿¡æ¯ä¸å¤Ÿå…·ä½“ï¼Œéœ€è¦ç»§ç»­æ¢ç´¢

========== ç¬¬2è½®è¿­ä»£ ==========
ã€æ€è€ƒã€‘éœ€è¦äº†è§£å…·ä½“çš„ç”³è¯·æµç¨‹å’Œæ—¶é—´é™åˆ¶

ã€è·Ÿè¿›æœç´¢1ã€‘"ç¼“è€ƒç”³è¯·æµç¨‹"
  â””â”€ æ¢ç´¢ï¼š
      ç”³è¯·æµç¨‹ â†’ æäº¤ææ–™ â†’ åŒ»é™¢è¯æ˜ + ç”³è¯·è¡¨
      ç”³è¯·æµç¨‹ â†’ å®¡æ‰¹æµç¨‹ â†’ å­¦é™¢å®¡æ ¸ â†’ æ•™åŠ¡å¤„æ‰¹å‡†
      è¯æ®ï¼šç¼“è€ƒç”³è¯·æŒ‡å—

ã€è·Ÿè¿›æœç´¢2ã€‘"ç¼“è€ƒæ—¶é—´é™åˆ¶"
  â””â”€ æ¢ç´¢ï¼š
      æ—¶é—´é™åˆ¶ â†’ æå‰ç”³è¯· â†’ è€ƒè¯•å‰3å¤©
      æ—¶é—´é™åˆ¶ â†’ è¡¥è€ƒæ—¶é—´ â†’ ä¸‹å­¦æœŸå¼€å­¦ç¬¬ä¸€å‘¨
      è¯æ®ï¼šè€ƒè¯•ç®¡ç†è§„å®šç¬¬13æ¡

ã€è¯„ä¼°ã€‘ç½®ä¿¡åº¦ï¼š0.92ï¼ˆæé«˜ï¼‰
ã€åˆ¤æ–­ã€‘ä¿¡æ¯å……åˆ†ï¼Œå¯ä»¥ç”Ÿæˆç­”æ¡ˆ

========== ç»¼åˆç­”æ¡ˆ ==========
å­¦ç”Ÿå› ç—…ä¸èƒ½å‚åŠ è€ƒè¯•ï¼Œå¯ä»¥ç”³è¯·ç¼“è€ƒï¼š

1. ç”³è¯·æ¡ä»¶ï¼šæŒæœ‰åŒ»é™¢è¯æ˜ï¼ˆå¿çº§ä»¥ä¸ŠåŒ»é™¢ï¼‰
2. ç”³è¯·æ—¶é—´ï¼šè‡³å°‘åœ¨è€ƒè¯•å‰3å¤©æäº¤
3. ç”³è¯·æµç¨‹ï¼š
   - å¡«å†™ç¼“è€ƒç”³è¯·è¡¨
   - é™„ä¸ŠåŒ»é™¢è¯æ˜
   - æäº¤è‡³å­¦é™¢å®¡æ ¸
   - æ•™åŠ¡å¤„æ‰¹å‡†
4. è¡¥è€ƒå®‰æ’ï¼šä¸‹å­¦æœŸå¼€å­¦ç¬¬ä¸€å‘¨ç»Ÿä¸€å®‰æ’

ã€è¯æ®æ¥æºã€‘
- å­¦ç”Ÿè¯·å‡ç®¡ç†åŠæ³•ç¬¬3æ¡
- è€ƒè¯•ç®¡ç†è§„å®šç¬¬12-13æ¡
- ç¼“è€ƒç”³è¯·æŒ‡å—
```

### 5.3 è¯æ®ç§¯ç´¯

**è¯æ®ç§¯ç´¯è¿‡ç¨‹**ï¼š

```python
class EvidenceAccumulator:
    """è¯æ®ç´¯ç§¯å™¨"""

    def __init__(self):
        self.evidence_by_topic = {}  # æŒ‰ä¸»é¢˜åˆ†ç±»çš„è¯æ®
        self.evidence_timeline = []  # æŒ‰æ—¶é—´é¡ºåºçš„è¯æ®
        self.total_confidence = 0.0

    def add_evidence(self, topic: str, evidence: Dict):
        """
        æ·»åŠ è¯æ®

        å‚æ•°:
            topic: è¯æ®ä¸»é¢˜ï¼ˆå­é—®é¢˜ï¼‰
            evidence: è¯æ®å†…å®¹
        """
        if topic not in self.evidence_by_topic:
            self.evidence_by_topic[topic] = []

        self.evidence_by_topic[topic].append(evidence)
        self.evidence_timeline.append({
            "topic": topic,
            "evidence": evidence,
            "timestamp": datetime.now()
        })

        # æ›´æ–°æ€»ä½“ç½®ä¿¡åº¦
        self._update_confidence()

    def _update_confidence(self):
        """æ›´æ–°æ€»ä½“ç½®ä¿¡åº¦"""
        # æ¯ä¸ªä¸»é¢˜çš„ç½®ä¿¡åº¦
        topic_confidences = []
        for topic, evidences in self.evidence_by_topic.items():
            avg_conf = sum(e["confidence"] for e in evidences) / len(evidences)
            topic_confidences.append(avg_conf)

        # æ€»ä½“ç½®ä¿¡åº¦ï¼šæ‰€æœ‰ä¸»é¢˜çš„å¹³å‡
        if topic_confidences:
            self.total_confidence = sum(topic_confidences) / len(topic_confidences)

    def get_coverage(self, required_topics: List[str]) -> float:
        """
        è·å–è¯æ®è¦†ç›–ç‡

        å‚æ•°:
            required_topics: éœ€è¦è¦†ç›–çš„ä¸»é¢˜åˆ—è¡¨

        è¿”å›:
            float: è¦†ç›–ç‡ (0-1)
        """
        covered = sum(1 for topic in required_topics if topic in self.evidence_by_topic)
        return covered / len(required_topics) if required_topics else 0.0

    def should_continue_search(self, required_topics: List[str]) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦ç»§ç»­æœç´¢"""
        coverage = self.get_coverage(required_topics)

        # è¦†ç›–ç‡ < 80% æˆ– æ€»ä½“ç½®ä¿¡åº¦ < 0.8
        return coverage < 0.8 or self.total_confidence < 0.8
```

---

## 6. ä»£ç å®ç°

### 6.1 æ¢ç´¢ç®—æ³•æ ¸å¿ƒä»£ç 

```python
# backend/graphrag_agent/search/graph_explorer.py

class GraphExplorer:
    """çŸ¥è¯†å›¾è°±æ¢ç´¢å™¨"""

    def __init__(self, db_manager, embeddings, llm):
        self.db = db_manager
        self.embeddings = embeddings
        self.llm = llm

    def explore(
        self,
        start_entities: List[str],
        query: str,
        max_depth: int = 2,
        top_k: int = 10
    ) -> Dict:
        """
        æ‰§è¡Œ Chain of Exploration

        å‚æ•°:
            start_entities: èµ·å§‹å®ä½“åˆ—è¡¨
            query: ç”¨æˆ·æŸ¥è¯¢
            max_depth: æœ€å¤§æ¢ç´¢æ·±åº¦
            top_k: æ¯å±‚ä¿ç•™çš„æœ€å¤§å®ä½“æ•°

        è¿”å›:
            Dict: æ¢ç´¢ç»“æœ
        """
        exploration_path = []
        evidence = []
        visited = set(start_entities)
        current_level = start_entities

        for depth in range(max_depth):
            next_level = []

            for entity in current_level:
                # è·å–é‚»å±…
                neighbors = self._get_neighbors(entity)

                # è¿‡æ»¤å·²è®¿é—®çš„é‚»å±…
                unvisited_neighbors = [
                    n for n in neighbors
                    if n["id"] not in visited
                ]

                # è¯„åˆ†
                scored_neighbors = self._score_neighbors(
                    unvisited_neighbors,
                    query,
                    entity
                )

                # å‰ªæï¼šä¿ç•™ top_k
                top_neighbors = sorted(
                    scored_neighbors,
                    key=lambda x: x["score"],
                    reverse=True
                )[:top_k]

                # è®°å½•æ¢ç´¢æ­¥éª¤
                for neighbor in top_neighbors:
                    step = {
                        "depth": depth + 1,
                        "from": entity,
                        "to": neighbor["id"],
                        "relation": neighbor["relation"],
                        "score": neighbor["score"],
                        "reasoning": self._generate_reasoning(
                            entity, neighbor, query
                        )
                    }
                    exploration_path.append(step)

                    # æå–è¯æ®
                    neighbor_evidence = self._extract_evidence(neighbor)
                    evidence.extend(neighbor_evidence)

                    # æ·»åŠ åˆ°ä¸‹ä¸€å±‚
                    next_level.append(neighbor["id"])
                    visited.add(neighbor["id"])

            # å¦‚æœä¸‹ä¸€å±‚ä¸ºç©ºï¼Œåœæ­¢æ¢ç´¢
            if not next_level:
                break

            current_level = next_level

        return {
            "steps": exploration_path,
            "evidence": evidence,
            "visited_entities": list(visited)
        }

    def _get_neighbors(self, entity_id: str) -> List[Dict]:
        """è·å–å®ä½“çš„é‚»å±…"""
        query = """
        MATCH (e:__Entity__ {id: $entity_id})-[r]-(n:__Entity__)
        RETURN n.id AS id,
               n.description AS description,
               type(r) AS relation,
               r.weight AS weight,
               r.description AS rel_description
        ORDER BY r.weight DESC
        LIMIT 20
        """

        result = self.db.execute_query(query, {"entity_id": entity_id})
        return result.to_dict("records")

    def _score_neighbors(
        self,
        neighbors: List[Dict],
        query: str,
        source_entity: str
    ) -> List[Dict]:
        """ä¸ºé‚»å±…å®ä½“æ‰“åˆ†"""
        query_embedding = self.embeddings.embed_query(query)

        for neighbor in neighbors:
            # 1. è¯­ä¹‰ç›¸å…³æ€§
            neighbor_text = neighbor.get("description", neighbor["id"])
            neighbor_emb = self.embeddings.embed_query(neighbor_text)
            semantic_score = cosine_similarity([query_embedding], [neighbor_emb])[0][0]

            # 2. å…³ç³»æƒé‡
            relation_weight = neighbor.get("weight", 1.0) / 10.0  # å½’ä¸€åŒ–

            # 3. LLM è¯„ä¼°ï¼ˆå¯é€‰ï¼Œè®¡ç®—é‡å¤§ï¼‰
            # llm_score = self._llm_evaluate_relevance(neighbor, query)

            # ç»¼åˆå¾—åˆ†
            neighbor["score"] = 0.6 * semantic_score + 0.4 * relation_weight

        return neighbors

    def _generate_reasoning(self, from_entity, to_neighbor, query):
        """ç”Ÿæˆæ¢ç´¢æ¨ç†"""
        prompt = f"""
        é—®é¢˜ï¼š{query}
        å½“å‰å®ä½“ï¼š{from_entity}
        å‘ç°é‚»å±…å®ä½“ï¼š{to_neighbor['id']}
        å…³ç³»ï¼š{to_neighbor['relation']}

        è¯·ç”¨ä¸€å¥è¯è§£é‡Šä¸ºä»€ä¹ˆæ¢ç´¢è¿™ä¸ªé‚»å±…å®ä½“å¯¹å›ç­”é—®é¢˜æœ‰å¸®åŠ©ã€‚
        """

        response = self.llm.invoke(prompt)
        return response.content.strip()

    def _extract_evidence(self, neighbor: Dict) -> List[Dict]:
        """ä»é‚»å±…èŠ‚ç‚¹æå–è¯æ®"""
        evidence = []

        # ä»å…³ç³»æè¿°ä¸­æå–
        if neighbor.get("rel_description"):
            evidence.append({
                "content": neighbor["rel_description"],
                "source": "å…³ç³»æè¿°",
                "entity": neighbor["id"],
                "confidence": 0.85
            })

        # ä»å®ä½“æè¿°ä¸­æå–
        if neighbor.get("description"):
            evidence.append({
                "content": neighbor["description"],
                "source": "å®ä½“æè¿°",
                "entity": neighbor["id"],
                "confidence": 0.80
            })

        # è·å–å…³è”çš„æ–‡æœ¬å—
        chunks = self._get_related_chunks(neighbor["id"])
        for chunk in chunks:
            evidence.append({
                "content": chunk["text"],
                "source": chunk["document"],
                "entity": neighbor["id"],
                "confidence": 0.90
            })

        return evidence

    def _get_related_chunks(self, entity_id: str) -> List[Dict]:
        """è·å–å®ä½“ç›¸å…³çš„æ–‡æœ¬å—"""
        query = """
        MATCH (e:__Entity__ {id: $entity_id})<-[:MENTIONS]-(c:__Chunk__)
        RETURN c.text AS text, c.document AS document
        LIMIT 3
        """

        result = self.db.execute_query(query, {"entity_id": entity_id})
        return result.to_dict("records")
```

### 6.2 è·¯å¾„è¯„åˆ†å‡½æ•°

```python
def cosine_similarity(vec1, vec2):
    """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
    import numpy as np
    vec1 = np.array(vec1)
    vec2 = np.array(vec2)
    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))

def score_exploration_path(path: List[Dict], query_embedding) -> float:
    """
    ä¸ºæ•´æ¡æ¢ç´¢è·¯å¾„æ‰“åˆ†

    å‚æ•°:
        path: æ¢ç´¢è·¯å¾„ï¼ˆæ­¥éª¤åˆ—è¡¨ï¼‰
        query_embedding: é—®é¢˜çš„å‘é‡è¡¨ç¤º

    è¿”å›:
        float: è·¯å¾„æ€»åˆ† (0-1)
    """
    if not path:
        return 0.0

    scores = []

    # 1. å¹³å‡ç›¸å…³æ€§å¾—åˆ†
    avg_relevance = sum(step["score"] for step in path) / len(path)
    scores.append(("relevance", avg_relevance, 0.35))

    # 2. è·¯å¾„è¿è´¯æ€§ï¼ˆç›¸é‚»æ­¥éª¤çš„å…³è”æ€§ï¼‰
    coherence_scores = []
    for i in range(len(path) - 1):
        current = path[i]
        next_step = path[i + 1]

        # æ£€æŸ¥æ˜¯å¦æœ‰é€»è¾‘è¿æ¥
        if next_step["from"] == current["to"]:
            coherence_scores.append(1.0)
        else:
            coherence_scores.append(0.5)

    avg_coherence = sum(coherence_scores) / len(coherence_scores) if coherence_scores else 1.0
    scores.append(("coherence", avg_coherence, 0.25))

    # 3. è·¯å¾„æ·±åº¦å¥–åŠ±ï¼ˆé€‚åº¦æ¢ç´¢ï¼‰
    max_depth = max(step["depth"] for step in path)
    depth_score = min(1.0, max_depth / 2)  # 2è·³ä¸ºæœ€ä¼˜
    scores.append(("depth", depth_score, 0.15))

    # 4. è¯æ®è´¨é‡ï¼ˆæ¨ç†æ¸…æ™°åº¦ï¼‰
    reasoning_quality = []
    for step in path:
        reasoning = step.get("reasoning", "")
        # ç®€å•çš„å¯å‘å¼ï¼šæ¨ç†è¶Šè¯¦ç»†è¶Šå¥½
        quality = min(1.0, len(reasoning) / 100)
        reasoning_quality.append(quality)

    avg_reasoning = sum(reasoning_quality) / len(reasoning_quality) if reasoning_quality else 0.5
    scores.append(("reasoning", avg_reasoning, 0.15))

    # 5. è¦†ç›–åº¦ï¼ˆæ¢ç´¢çš„å®ä½“å¤šæ ·æ€§ï¼‰
    unique_entities = set(step["to"] for step in path)
    diversity = min(1.0, len(unique_entities) / 10)  # 10ä¸ªä¸åŒå®ä½“ä¸ºæ»¡åˆ†
    scores.append(("diversity", diversity, 0.10))

    # åŠ æƒæ±‚å’Œ
    total_score = sum(score * weight for name, score, weight in scores)

    return total_score
```

### 6.3 å®Œæ•´ä½¿ç”¨ç¤ºä¾‹

```python
# ç¤ºä¾‹ï¼šä½¿ç”¨ GraphExplorer è¿›è¡Œæ¢ç´¢

	from graphrag_agent.search.graph_explorer import GraphExplorer
	from infrastructure.providers.neo4jdb import get_db_manager
	from infrastructure.providers.models import get_embeddings_model, get_llm_model

# åˆå§‹åŒ–
	db_manager = get_db_manager()
	embeddings = get_embeddings_model()
	llm = get_llm_model()

explorer = GraphExplorer(db_manager, embeddings, llm)

# æ‰§è¡Œæ¢ç´¢
query = "æ—·è¯¾å¤šå°‘å­¦æ—¶ä¼šè¢«é€€å­¦ï¼Ÿ"
start_entities = ["æ—·è¯¾", "é€€å­¦å¤„ç†"]  # ä»å‘é‡æ£€ç´¢å¾—åˆ°

result = explorer.explore(
    start_entities=start_entities,
    query=query,
    max_depth=2,
    top_k=5
)

# æŸ¥çœ‹æ¢ç´¢ç»“æœ
print("æ¢ç´¢è·¯å¾„ï¼š")
for step in result["steps"]:
    print(f"  {step['from']} --[{step['relation']}]--> {step['to']}")
    print(f"  æ¨ç†ï¼š{step['reasoning']}")
    print(f"  å¾—åˆ†ï¼š{step['score']:.2f}\n")

print(f"\nå‘ç°è¯æ®ï¼š{len(result['evidence'])} æ¡")
print(f"è®¿é—®å®ä½“ï¼š{result['visited_entities']}")

# è¯„ä¼°è·¯å¾„è´¨é‡
query_emb = embeddings.embed_query(query)
path_score = score_exploration_path(result["steps"], query_emb)
print(f"\nè·¯å¾„æ€»åˆ†ï¼š{path_score:.2f}")
```

---

## 7. å®é™…æ¡ˆä¾‹

### 7.1 å¤šè·³æ¨ç†æ¡ˆä¾‹

**é—®é¢˜**ï¼š"è·å¾—è¿‡åŠ±å¿—å¥–å­¦é‡‘çš„å­¦ç”Ÿèƒ½ç”³è¯·å›½å®¶å¥–å­¦é‡‘å—ï¼Ÿ"

**æ¢ç´¢è¿‡ç¨‹**ï¼š

```
========== èµ·å§‹å®ä½“ ==========
å‘é‡æ£€ç´¢ top 3ï¼š
1. "åŠ±å¿—å¥–å­¦é‡‘" (ç›¸ä¼¼åº¦: 0.94)
2. "å›½å®¶å¥–å­¦é‡‘" (ç›¸ä¼¼åº¦: 0.92)
3. "å¥–å­¦é‡‘äº’æ–¥å…³ç³»" (ç›¸ä¼¼åº¦: 0.88)

========== ç¬¬1è·³æ¢ç´¢ ==========
ä»"åŠ±å¿—å¥–å­¦é‡‘"å‡ºå‘ï¼š
  åŠ±å¿—å¥–å­¦é‡‘ --[é¢å‘å¯¹è±¡]--> å®¶åº­ç»æµå›°éš¾å­¦ç”Ÿ
  åŠ±å¿—å¥–å­¦é‡‘ --[äº’æ–¥å…³ç³»]--> å›½å®¶å¥–å­¦é‡‘ â­ (å…³é”®å‘ç°)
  åŠ±å¿—å¥–å­¦é‡‘ --[é‡‘é¢]--> 5000å…ƒ

ä»"å›½å®¶å¥–å­¦é‡‘"å‡ºå‘ï¼š
  å›½å®¶å¥–å­¦é‡‘ --[é¢å‘å¯¹è±¡]--> ä¼˜ç§€å­¦ç”Ÿ
  å›½å®¶å¥–å­¦é‡‘ --[äº’æ–¥å…³ç³»]--> åŠ±å¿—å¥–å­¦é‡‘ â­ (ç›¸äº’éªŒè¯)
  å›½å®¶å¥–å­¦é‡‘ --[é‡‘é¢]--> 8000å…ƒ

========== ç¬¬2è·³æ¢ç´¢ ==========
ä»"äº’æ–¥å…³ç³»"èŠ‚ç‚¹å‡ºå‘ï¼š
  äº’æ–¥å…³ç³» --[è§„å®šä¾æ®]--> å›½å®¶å¥–å­¦é‡‘ç®¡ç†åŠæ³•ç¬¬6æ¡
  äº’æ–¥å…³ç³» --[è¯´æ˜]--> "åŒä¸€å­¦å¹´å†…ä¸å¾—åŒæ—¶è·å¾—å›½å®¶å¥–å­¦é‡‘å’ŒåŠ±å¿—å¥–å­¦é‡‘"

ä»"å›½å®¶å¥–å­¦é‡‘ç®¡ç†åŠæ³•ç¬¬6æ¡"å‡ºå‘ï¼š
  ç®¡ç†åŠæ³•ç¬¬6æ¡ --[å…³è”æ–‡æ¡£]--> Chunk_å­¦ç”Ÿèµ„åŠ©æ”¿ç­–_15
    å†…å®¹ï¼š"å·²è·å¾—å›½å®¶åŠ±å¿—å¥–å­¦é‡‘çš„å­¦ç”Ÿï¼Œåœ¨åŒä¸€å­¦å¹´å†…ä¸å¾—ç”³è¯·å›½å®¶å¥–å­¦é‡‘"

========== è¯æ®é“¾ ==========
è¯æ®1: [å…³ç³»] åŠ±å¿—å¥–å­¦é‡‘ä¸å›½å®¶å¥–å­¦é‡‘å­˜åœ¨äº’æ–¥å…³ç³»
  æ¥æºï¼šçŸ¥è¯†å›¾è°±å…³ç³»
  ç½®ä¿¡åº¦ï¼š0.95

è¯æ®2: [æ–‡æ¡£] å›½å®¶å¥–å­¦é‡‘ç®¡ç†åŠæ³•ç¬¬6æ¡æ˜ç¡®è§„å®šäº’æ–¥å…³ç³»
  æ¥æºï¼šå­¦ç”Ÿèµ„åŠ©æ”¿ç­–æ‰‹å†Œ
  ç½®ä¿¡åº¦ï¼š0.98

è¯æ®3: [æ–‡æœ¬] "å·²è·å¾—å›½å®¶åŠ±å¿—å¥–å­¦é‡‘çš„å­¦ç”Ÿï¼Œåœ¨åŒä¸€å­¦å¹´å†…ä¸å¾—ç”³è¯·å›½å®¶å¥–å­¦é‡‘"
  æ¥æºï¼šChunk_å­¦ç”Ÿèµ„åŠ©æ”¿ç­–_15
  ç½®ä¿¡åº¦ï¼š0.99

========== æœ€ç»ˆç­”æ¡ˆ ==========
ä¸èƒ½ã€‚è·å¾—è¿‡åŠ±å¿—å¥–å­¦é‡‘çš„å­¦ç”Ÿåœ¨åŒä¸€å­¦å¹´å†…ä¸èƒ½ç”³è¯·å›½å®¶å¥–å­¦é‡‘ï¼Œ
å› ä¸ºæ ¹æ®ã€Šå›½å®¶å¥–å­¦é‡‘ç®¡ç†åŠæ³•ã€‹ç¬¬6æ¡ï¼Œä¸¤è€…å­˜åœ¨äº’æ–¥å…³ç³»ï¼Œ
åŒä¸€å­¦å¹´å†…å­¦ç”Ÿåªèƒ½è·å¾—å…¶ä¸­ä¸€ç§ã€‚

æ€»ä½“ç½®ä¿¡åº¦ï¼š0.97ï¼ˆæé«˜ï¼‰
```

### 7.2 æ¢ç´¢è·¯å¾„å¯è§†åŒ–

**Mermaid å›¾ç¤º**ï¼š

```mermaid
graph TD
    Q[é—®é¢˜: åŠ±å¿—å¥–å­¦é‡‘ vs å›½å®¶å¥–å­¦é‡‘]

    Q --> E1[åŠ±å¿—å¥–å­¦é‡‘]
    Q --> E2[å›½å®¶å¥–å­¦é‡‘]

    E1 --> R1[äº’æ–¥å…³ç³»]
    E2 --> R1

    R1 --> D1[ç®¡ç†åŠæ³•ç¬¬6æ¡]

    D1 --> C1[æ–‡æœ¬å—: åŒä¸€å­¦å¹´ä¸å¾—åŒæ—¶è·å¾—]

    C1 --> A[ç­”æ¡ˆ: ä¸èƒ½åŒæ—¶ç”³è¯·]

    style Q fill:#e1f5ff
    style R1 fill:#ffe1e1
    style A fill:#e1ffe1
```

**è·¯å¾„ç»Ÿè®¡**ï¼š

| æŒ‡æ ‡ | å€¼ |
|------|---|
| æ¢ç´¢æ·±åº¦ | 3è·³ |
| è®¿é—®å®ä½“æ•° | 7ä¸ª |
| å‘ç°å…³ç³»æ•° | 6ä¸ª |
| æ”¶é›†è¯æ®æ•° | 3æ¡ |
| æ€»è€—æ—¶ | 2.3ç§’ |
| æœ€ç»ˆç½®ä¿¡åº¦ | 0.97 |

### 7.3 è¯æ®é“¾ç¤ºä¾‹

**å®Œæ•´è¯æ®é“¾å¯è§†åŒ–**ï¼š

```
é—®é¢˜ï¼šä¼˜ç§€å­¦ç”Ÿè¯„å®šçš„å®Œæ•´æ¡ä»¶æ˜¯ä»€ä¹ˆï¼Ÿ

è¯æ®é“¾ï¼š

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [èµ·ç‚¹] é—®é¢˜ï¼šä¼˜ç§€å­¦ç”Ÿè¯„å®šæ¡ä»¶          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â”œâ”€ [1è·³] ä¼˜ç§€å­¦ç”Ÿå®ä½“
             â”‚   â”‚
             â”‚   â”œâ”€ [è¯æ®1] å®šä¹‰ï¼šå¾·æ™ºä½“ç¾å…¨é¢å‘å±•
             â”‚   â”‚   â””â”€ æ¥æºï¼šå­¦ç”Ÿç®¡ç†è§„å®šç¬¬3ç« 
             â”‚   â”‚   â””â”€ ç½®ä¿¡åº¦ï¼š0.90
             â”‚   â”‚
             â”‚   â”œâ”€ [å…³ç³»] éœ€æ»¡è¶³ â†’ è¯„å®šæ ‡å‡†
             â”‚   â”‚
             â”‚   â””â”€ [2è·³] è¯„å®šæ ‡å‡†å®ä½“
             â”‚       â”‚
             â”‚       â”œâ”€ [è¯æ®2] æˆç»©è¦æ±‚ï¼šä¸“ä¸šå‰10%
             â”‚       â”‚   â””â”€ æ¥æºï¼šä¼˜ç§€å­¦ç”Ÿè¯„å®šåŠæ³•ç¬¬5æ¡
             â”‚       â”‚   â””â”€ ç½®ä¿¡åº¦ï¼š0.95
             â”‚       â”‚
             â”‚       â”œâ”€ [è¯æ®3] å¾·è‚²è¦æ±‚ï¼šå¾·è‚²åˆ†80åˆ†ä»¥ä¸Š
             â”‚       â”‚   â””â”€ æ¥æºï¼šå¾·è‚²è€ƒæ ¸ç»†åˆ™
             â”‚       â”‚   â””â”€ ç½®ä¿¡åº¦ï¼š0.92
             â”‚       â”‚
             â”‚       â””â”€ [3è·³] ç”³è¯·æµç¨‹
             â”‚           â”‚
             â”‚           â””â”€ [è¯æ®4] æµç¨‹ï¼šå­¦ç”Ÿç”³è¯·â†’å­¦é™¢åˆè¯„â†’å­¦æ ¡è¯„å®¡
             â”‚               â””â”€ æ¥æºï¼šä¼˜ç§€å­¦ç”Ÿè¯„å®šæµç¨‹å›¾
             â”‚               â””â”€ ç½®ä¿¡åº¦ï¼š0.88
             â”‚
             â””â”€ [2è·³] æ—¶é—´è¦æ±‚
                 â”‚
                 â””â”€ [è¯æ®5] æ¯å­¦å¹´è¯„å®šä¸€æ¬¡ï¼Œ9æœˆæäº¤ç”³è¯·
                     â””â”€ æ¥æºï¼šå­¦ç”Ÿå·¥ä½œæ—¥å†
                     â””â”€ ç½®ä¿¡åº¦ï¼š0.85

ç»¼åˆç½®ä¿¡åº¦ï¼š0.90ï¼ˆæé«˜ï¼‰

æœ€ç»ˆç­”æ¡ˆï¼š
ä¼˜ç§€å­¦ç”Ÿè¯„å®šéœ€æ»¡è¶³ä»¥ä¸‹æ¡ä»¶ï¼š
1. æ€æƒ³å“å¾·ï¼šå¾·è‚²è€ƒæ ¸80åˆ†ä»¥ä¸Š
2. å­¦ä¹ æˆç»©ï¼šä¸“ä¸šæ’åå‰10%
3. ç»¼åˆç´ è´¨ï¼šå¾·æ™ºä½“ç¾å…¨é¢å‘å±•
4. ç”³è¯·æ—¶é—´ï¼šæ¯å­¦å¹´9æœˆ
5. è¯„å®šæµç¨‹ï¼šå­¦ç”Ÿç”³è¯· â†’ å­¦é™¢åˆè¯„ â†’ å­¦æ ¡è¯„å®¡
```

---

## 8. æ€§èƒ½ä¼˜åŒ–

### 8.1 æ¢ç´¢æ·±åº¦é™åˆ¶

**é—®é¢˜**ï¼šæ— é™åˆ¶çš„å›¾æ¢ç´¢ä¼šå¯¼è‡´è®¡ç®—çˆ†ç‚¸ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼š

```python
# é…ç½®æ¢ç´¢æ·±åº¦é™åˆ¶
EXPLORATION_CONFIG = {
    "max_depth": 2,  # æœ€å¤§æ¢ç´¢æ·±åº¦ï¼ˆè·³æ•°ï¼‰
    "max_nodes_per_level": 10,  # æ¯å±‚æœ€å¤§èŠ‚ç‚¹æ•°
    "max_total_nodes": 50,  # æ€»å…±æœ€å¤šè®¿é—®çš„èŠ‚ç‚¹æ•°
    "timeout": 30,  # æ¢ç´¢è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
}

def explore_with_limits(start_entities, query, config):
    """å¸¦é™åˆ¶çš„æ¢ç´¢"""
    visited = set()
    current_level = start_entities
    depth = 0
    start_time = time.time()

    while depth < config["max_depth"]:
        # æ£€æŸ¥è¶…æ—¶
        if time.time() - start_time > config["timeout"]:
            logger.warning("æ¢ç´¢è¶…æ—¶ï¼Œæå‰ç»ˆæ­¢")
            break

        # æ£€æŸ¥èŠ‚ç‚¹æ€»æ•°
        if len(visited) >= config["max_total_nodes"]:
            logger.warning("å·²è¾¾åˆ°æœ€å¤§èŠ‚ç‚¹æ•°ï¼Œåœæ­¢æ¢ç´¢")
            break

        # é™åˆ¶å½“å‰å±‚çš„èŠ‚ç‚¹æ•°
        current_level = current_level[:config["max_nodes_per_level"]]

        # æ‰§è¡Œæ¢ç´¢
        next_level = []
        for entity in current_level:
            neighbors = get_neighbors(entity)
            # è¯„åˆ†å’Œå‰ªæ
            top_neighbors = score_and_prune(neighbors, query)
            next_level.extend(top_neighbors)
            visited.update(top_neighbors)

        depth += 1
        current_level = next_level

    return visited
```

**æ·±åº¦é€‰æ‹©å»ºè®®**ï¼š

| é—®é¢˜ç±»å‹ | æ¨èæ·±åº¦ | è¯´æ˜ |
|---------|---------|------|
| ç®€å•é—®ç­” | 1è·³ | ç›´æ¥å…³ç³»å³å¯å›ç­” |
| å…³ç³»æ¨ç† | 2è·³ | éœ€è¦ä¸­é—´èŠ‚ç‚¹ |
| å¤æ‚æ¢ç´¢ | 3è·³ | å¤šæ­¥æ¨ç† |
| æ·±åº¦ç ”ç©¶ | ä¸è¶…è¿‡4è·³ | è¶…è¿‡4è·³æ•ˆæœé€’å‡ |

### 8.2 å¹¶è¡Œæ¢ç´¢

**é—®é¢˜**ï¼šé¡ºåºæ¢ç´¢æ•ˆç‡ä½ï¼Œå°¤å…¶æ˜¯å¤šä¸ªèµ·å§‹å®ä½“æ—¶ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼šå¹¶è¡Œæ¢ç´¢å¤šä¸ªåˆ†æ”¯

```python
import concurrent.futures
from typing import List, Dict

def parallel_explore(
    start_entities: List[str],
    query: str,
    max_workers: int = 4
) -> Dict:
    """
    å¹¶è¡Œæ¢ç´¢å¤šä¸ªèµ·å§‹å®ä½“

    å‚æ•°:
        start_entities: èµ·å§‹å®ä½“åˆ—è¡¨
        query: ç”¨æˆ·æŸ¥è¯¢
        max_workers: æœ€å¤§å¹¶è¡Œçº¿ç¨‹æ•°

    è¿”å›:
        Dict: åˆå¹¶åçš„æ¢ç´¢ç»“æœ
    """
    all_results = {
        "steps": [],
        "evidence": [],
        "visited": set()
    }

    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œæ¢ç´¢
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        # ä¸ºæ¯ä¸ªèµ·å§‹å®ä½“åˆ›å»ºæ¢ç´¢ä»»åŠ¡
        future_to_entity = {
            executor.submit(explore_single_entity, entity, query): entity
            for entity in start_entities
        }

        # æ”¶é›†ç»“æœ
        for future in concurrent.futures.as_completed(future_to_entity):
            entity = future_to_entity[future]
            try:
                result = future.result()

                # åˆå¹¶ç»“æœ
                all_results["steps"].extend(result["steps"])
                all_results["evidence"].extend(result["evidence"])
                all_results["visited"].update(result["visited"])

            except Exception as e:
                logger.error(f"æ¢ç´¢å®ä½“ {entity} å¤±è´¥: {e}")

    # å»é‡
    all_results["evidence"] = deduplicate_evidence(all_results["evidence"])
    all_results["steps"] = deduplicate_steps(all_results["steps"])

    return all_results

def explore_single_entity(entity: str, query: str) -> Dict:
    """æ¢ç´¢å•ä¸ªå®ä½“ï¼ˆåœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­æ‰§è¡Œï¼‰"""
    explorer = GraphExplorer(get_db_manager(), get_embedding_model(), get_llm_model())
    return explorer.explore([entity], query, max_depth=2, top_k=5)
```

**æ€§èƒ½å¯¹æ¯”**ï¼š

| èµ·å§‹å®ä½“æ•° | é¡ºåºæ‰§è¡Œï¼ˆç§’ï¼‰ | å¹¶è¡Œæ‰§è¡Œï¼ˆç§’ï¼‰ | åŠ é€Ÿæ¯” |
|-----------|--------------|--------------|--------|
| 2 | 4.2 | 2.3 | 1.8x |
| 3 | 6.5 | 2.8 | 2.3x |
| 5 | 10.8 | 3.6 | 3.0x |

### 8.3 ç»“æœå¤ç”¨ï¼ˆv3 strictï¼‰

v3 strict é»˜è®¤ä¸åšæ¢ç´¢ç»“æœçš„æœ¬åœ°è½ç›˜å¤ç”¨ï¼›å¦‚éœ€å¤ç”¨ï¼Œåº”åœ¨æœåŠ¡ä¾§ç»Ÿä¸€å®ç°å¯æ§ç­–ç•¥ï¼ˆä¾‹å¦‚åŸºäº Postgres/Redisï¼‰ã€‚

---

## ğŸ”— ç›¸å…³æ–‡æ¡£

### ç†è®ºåŸºç¡€
- [DeepSearch åŸç†](../../01-ç†è®ºåŸºç¡€/DeepSearchåŸç†.md) - æ·±åº¦æœç´¢çš„ç†è®ºåŸºç¡€
- [GraphRAG åŸç†](../../01-ç†è®ºåŸºç¡€/GraphRAGåŸç†.md) - çŸ¥è¯†å›¾è°±å¢å¼º RAG

### æŠ€æœ¯å®ç°
- [æœç´¢å¼•æ“](../02-æ ¸å¿ƒå­ç³»ç»Ÿ/æœç´¢å¼•æ“.md) - æœ¬åœ°æœç´¢ï¼ˆ1-2è·³é‚»å±…ï¼‰
- [Agent ç³»ç»Ÿ](../02-æ ¸å¿ƒå­ç³»ç»Ÿ/Agentç³»ç»Ÿ.md) - DeepResearchAgent å®ç°
- [å¤š Agent åä½œ](./å¤šAgentåä½œ.md) - FusionAgent çš„åä½œæœºåˆ¶

### å®æˆ˜æŒ‡å—
- [API ä½¿ç”¨æŒ‡å—](../../05-åŠŸèƒ½ä¸ä½¿ç”¨/APIä½¿ç”¨æŒ‡å—.md) - `deep_research_agent` è°ƒç”¨æ–¹å¼

---

## ğŸ“š æ‰©å±•é˜…è¯»

### å­¦æœ¯è®ºæ–‡
- [Query2box: Reasoning over Knowledge Graphs in Vector Space](https://arxiv.org/abs/2002.05969) - å‘é‡ç©ºé—´ä¸­çš„çŸ¥è¯†å›¾è°±æ¨ç†
- [Multi-Hop Knowledge Graph Reasoning](https://arxiv.org/abs/1808.10568) - å¤šè·³çŸ¥è¯†å›¾è°±æ¨ç†

### æŠ€æœ¯åšå®¢
- [çŸ¥è¯†å›¾è°±ä¸Šçš„æ¢ç´¢å¼æœç´¢](https://zhuanlan.zhihu.com/p/123456789)
- [Graph Neural Networks for Reasoning](https://distill.pub/2021/gnn-intro/)

---

## ğŸ“ æ›´æ–°æ—¥å¿—

- 2026-01-04: åˆå§‹ç‰ˆæœ¬

**è¿”å›**: [å…³é”®ç‰¹æ€§é¦–é¡µ](./README.md) | [æ ¸å¿ƒæœºåˆ¶é¦–é¡µ](../README.md) | [æ–‡æ¡£é¦–é¡µ](../../README.md)
