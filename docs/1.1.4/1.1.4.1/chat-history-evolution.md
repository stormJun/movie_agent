# å¯¹è¯å†å²ç®¡ç†æ¼”è¿›æ–¹æ¡ˆè®¾è®¡

**ç‰ˆæœ¬**: 1.1.4.1  
**çŠ¶æ€**: è®¾è®¡ä¸­  
**ä½œè€…**: AI Assistant  
**æ—¥æœŸ**: 2026-01-23

---

## 1. èƒŒæ™¯ä¸åŠ¨æœº

### 1.1 å½“å‰å®ç°ï¼ˆBaselineï¼‰

åœ¨ v1.1.4 ä¸­ï¼Œæˆ‘ä»¬å®ç°äº†åŸºç¡€çš„å¯¹è¯å†å²æ³¨å…¥æœºåˆ¶ï¼š

```python
# å½“å‰æµç¨‹
history = await conversation_store.list_messages(limit=6, desc=True)
history.reverse()  # æ—¶é—´æ­£åº
prompt = build_prompt(system + history + current_message)
```

**ä¼˜ç‚¹**ï¼š
- âœ… ç®€å•ç›´æ¥ï¼Œæ˜“äºç†è§£å’Œç»´æŠ¤
- âœ… è§£å†³äº†åŸºæœ¬çš„ä¸Šä¸‹æ–‡ä¸¢å¤±é—®é¢˜
- âœ… å¯¹çŸ­ä¼šè¯ï¼ˆ< 10 è½®ï¼‰æ•ˆæœè‰¯å¥½

**å±€é™æ€§**ï¼š
- âŒ **å›ºå®šçª—å£ç›²åŒº**ï¼šè¶…å‡º N æ¡çš„å†å²è¢«é—å¿˜ï¼ˆå¦‚ç”¨æˆ·åœ¨ç¬¬ 1 è½®æåˆ°çš„é‡è¦ä¿¡æ¯ï¼‰
- âŒ **Token æµªè´¹**ï¼šæ¯æ¬¡éƒ½ä¼ é€’å®Œæ•´çš„å†å²æ¶ˆæ¯ï¼Œå³ä½¿å†…å®¹é‡å¤æˆ–æ— å…³
- âŒ **æ—¶é—´åè§**ï¼šåªæŒ‰æ—¶é—´åˆ‡ç‰‡ï¼Œä¸è€ƒè™‘è¯­ä¹‰ç›¸å…³æ€§ï¼ˆç”¨æˆ·å¯èƒ½è·³å›ä¹‹å‰çš„è¯é¢˜ï¼‰
- âŒ **æ‰©å±•æ€§å·®**ï¼šéšç€å¯¹è¯å˜é•¿ï¼Œæˆæœ¬çº¿æ€§å¢é•¿

### 1.2 æ¼”è¿›ç›®æ ‡

æ„å»ºä¸€ä¸ª**å¯æ‰©å±•ã€é«˜æ•ˆã€æ™ºèƒ½**çš„å¯¹è¯è®°å¿†ç³»ç»Ÿï¼Œæ”¯æŒï¼š
1. **é•¿æœŸä¸Šä¸‹æ–‡ä¿ç•™**ï¼šå³ä½¿å¯¹è¯è¶…è¿‡ 100 è½®ï¼Œå…³é”®ä¿¡æ¯ä¸ä¸¢å¤±
2. **æˆæœ¬ä¼˜åŒ–**ï¼šé™ä½ Token æ¶ˆè€—ï¼Œæå‡å“åº”é€Ÿåº¦
3. **è¯­ä¹‰æ„ŸçŸ¥**ï¼šæ ¹æ®ç›¸å…³æ€§è€Œéæ—¶é—´æ£€ç´¢å†å²
4. **æ¶æ„å¥å£®æ€§**ï¼šå‡å°‘æ‰‹åŠ¨å‚æ•°ä¼ é€’ï¼Œé™ä½ç»´æŠ¤æˆæœ¬

---

## 2. ä¸‰é˜¶æ®µæ¼”è¿›æ–¹æ¡ˆ

### Phase 1: è®°å¿†å‹ç¼©ä¸æ‘˜è¦ (Memory Summarization)

#### 2.1.1 æ ¸å¿ƒè®¾è®¡ç†å¿µ

é‡‡ç”¨ **æ»‘åŠ¨çª—å£ + å†å²æ‘˜è¦** ç­–ç•¥ï¼Œé€šè¿‡å¯¹è¯å†å²çš„åˆ†å±‚å‹ç¼©æ¥è§£å†³é•¿å¯¹è¯çš„ Token æµªè´¹å’Œä¸Šä¸‹æ–‡ä¸¢å¤±é—®é¢˜ã€‚

**æ ¸å¿ƒæ€æƒ³ï¼š**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  æœ€ç»ˆ Prompt ç»“æ„                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  [System Prompt]                         â”‚
â”‚  [å¯¹è¯èƒŒæ™¯æ‘˜è¦]: å‹ç¼©çš„å…¨å±€ä¸Šä¸‹æ–‡         â”‚
â”‚  [æœ€è¿‘çª—å£]: æœ€è¿‘ 6 æ¡åŸå§‹å¯¹è¯ï¼ˆä¿æŒç»†èŠ‚ï¼‰ â”‚
â”‚  [å½“å‰æ¶ˆæ¯]                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ä¿¡æ¯å±‚çº§ï¼š**
- **æ‘˜è¦å±‚ï¼ˆé•¿æœŸè®°å¿†ï¼‰**ï¼šä¿ç•™å…¨å±€èƒŒæ™¯ã€ç”¨æˆ·åå¥½ã€å…³é”®å†³ç­–
  - ç¤ºä¾‹ï¼š"ç”¨æˆ·è®¨è®ºäº†90å¹´ä»£ç§‘å¹»ç”µå½±ï¼Œç‰¹åˆ«å…³æ³¨è¯ºå…°å¯¼æ¼”ä½œå“ï¼Œä¸å–œæ¬¢ææ€–ç‰‡"
- **çª—å£å±‚ï¼ˆçŸ­æœŸè®°å¿†ï¼‰**ï¼šä¿ç•™æœ€è¿‘å¯¹è¯çš„å®Œæ•´ç»†èŠ‚
  - åŒ…å«æœ€è¿‘ 3 è½®å¯¹è¯ï¼ˆ6 æ¡æ¶ˆæ¯ï¼‰
  - ç¡®ä¿å½“å‰è¯é¢˜çš„ä¸Šä¸‹æ–‡è¿ç»­æ€§

#### 2.1.2 è®¾è®¡åŸåˆ™ä¸å…³é”®å†³ç­–

**æ ¸å¿ƒåŸåˆ™ï¼š**
1.  **ä¿¡æ¯å±‚çº§ä¿ç•™**ï¼šæ‘˜è¦å±‚è®°å½•å…¨å±€èƒŒæ™¯ï¼ˆæ£®æ—ï¼‰ï¼Œæ»‘åŠ¨çª—å£ä¿ç•™å±€éƒ¨ç»†èŠ‚ï¼ˆæ ‘æœ¨ï¼‰ã€‚
2.  **é™ä½ä¿¡æ¯ç†µ**ï¼šé€šè¿‡å‹ç¼©é•¿æœŸå†å²ï¼Œä»…ä¿ç•™é«˜ä»·å€¼ä¿¡æ¯ï¼Œé¿å… Token æµªè´¹ã€‚
3.  **ç¬¦åˆè®¤çŸ¥æ¨¡å‹**ï¼šæ¨¡æ‹Ÿäººç±»çš„é•¿çŸ­æœŸè®°å¿†æœºåˆ¶ (Atkinson-Shiffrin Model)ã€‚

**å…³é”®å†³ç­–ï¼š**

1.  **æ¶æ„æ¨¡å¼ï¼šæ»‘åŠ¨çª—å£ + å†å²æ‘˜è¦**
    - é€‚ç”¨åœºæ™¯ï¼šé€šç”¨åœºæ™¯ï¼Œå¹³è¡¡äº†çŸ­å¯¹è¯çš„å®æ—¶æ€§å’Œé•¿å¯¹è¯çš„ä¸Šä¸‹æ–‡å®Œæ•´æ€§ã€‚

2.  **å‚æ•°é…ç½®**
    - **è§¦å‘é˜ˆå€¼ (min_messages)**: 10 æ¡ï¼ˆ5 è½®å¯¹è¯ï¼‰ã€‚ç¡®ä¿æœ‰è¶³å¤Ÿä¸Šä¸‹æ–‡ç”Ÿæˆæœ‰æ„ä¹‰çš„æ‘˜è¦ã€‚
    - **æ›´æ–°å¢é‡ (update_delta)**: 5 æ¡ã€‚å¹³è¡¡æ‘˜è¦æ–°é²œåº¦å’Œç”Ÿæˆæˆæœ¬ï¼Œé¿å…é¢‘ç¹è°ƒç”¨ LLMã€‚
    - **çª—å£å¤§å° (window_size)**: 6 æ¡ã€‚ä¿ç•™æœ€è¿‘ 3 è½®å®Œæ•´å¯¹è¯ï¼Œç¡®ä¿å½“å‰è¯é¢˜è¿è´¯ã€‚

3.  **å­˜å‚¨æ–¹æ¡ˆï¼šç‹¬ç«‹è¡¨ (conversation_summaries)**
    - æ¸…æ™°åˆ†ç¦»å…³æ³¨ç‚¹ï¼Œé¿å…æ±¡æŸ“æ ¸å¿ƒæ¶ˆæ¯è¡¨ï¼Œä¾¿äºç‹¬ç«‹ä¼˜åŒ–ç´¢å¼•ã€‚

4.  **æ¨¡å‹é€‰æ‹©ï¼šQwen (é¡¹ç›®å†…ç½®)**
    - **ä¸€è‡´æ€§**ï¼šä½¿ç”¨ä¸ä¸»å¯¹è¯ç›¸åŒçš„æ¨¡å‹ç³»åˆ—ï¼Œä¿è¯å¯¹é¢†åŸŸçŸ¥è¯†ç†è§£çš„ä¸€è‡´æ€§ã€‚
    - **æˆæœ¬ä¸æ€§èƒ½**ï¼šQwen åœ¨æ‘˜è¦ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œä¸”æ— éœ€å¼•å…¥é¢å¤–çš„å¤–éƒ¨ API ä¾èµ–ã€‚

5.  **æ›´æ–°ç­–ç•¥ï¼šå¢é‡æ›´æ–°**
    - ä»…å°†"æ—§æ‘˜è¦ + æ–°å¢å¯¹è¯"å‘é€ç»™æ¨¡å‹è¿›è¡Œåˆå¹¶ï¼Œè€Œéæ¯æ¬¡å…¨é‡é‡ç®—ã€‚å¤§å¹…é™ä½ Context å¼€é”€ã€‚

#### 2.1.3 æ•°æ®æ¨¡å‹

**æ–¹æ¡ˆï¼šç‹¬ç«‹æ‘˜è¦è¡¨ï¼ˆæ¨èï¼‰**

```sql
CREATE TABLE conversation_summaries (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    conversation_id UUID NOT NULL REFERENCES conversations(id),
    summary TEXT NOT NULL,
    covered_message_count INT NOT NULL,
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW(),
    UNIQUE(conversation_id)
);

-- ç´¢å¼•ä¼˜åŒ–
CREATE INDEX idx_summaries_conversation_id
    ON conversation_summaries(conversation_id);

CREATE INDEX idx_summaries_updated_at
    ON conversation_summaries(updated_at DESC);
```

**å­—æ®µè¯´æ˜ï¼š**

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `id` | UUID | ä¸»é”® |
| `conversation_id` | UUID | å…³è”çš„å¯¹è¯ IDï¼ˆå¤–é”®ï¼‰ |
| `summary` | TEXT | å‹ç¼©åçš„å¯¹è¯æ‘˜è¦ |
| `covered_message_count` | INT | å·²æ‘˜è¦çš„æ¶ˆæ¯æ•°é‡ï¼ˆç”¨äºåˆ¤æ–­æ˜¯å¦éœ€è¦æ›´æ–°ï¼‰ |
| `created_at` | TIMESTAMP | åˆ›å»ºæ—¶é—´ |
| `updated_at` | TIMESTAMP | æœ€åæ›´æ–°æ—¶é—´ |

#### 2.1.4 æ¶æ„ä¸æµç¨‹å¯è§†åŒ–

##### æ•°æ®æµæ¶æ„å›¾

**Phase 1 çš„æ•°æ®æµåŠ¨ä¸å­˜å‚¨ç»“æ„ï¼š**

```mermaid
flowchart LR
    subgraph Input["è¾“å…¥å±‚"]
        Msg1[æ¶ˆæ¯ 1]
        Msg2[æ¶ˆæ¯ 2]
        Msg3[...]
        MsgN[æ¶ˆæ¯ N-6]
        Rec1[æœ€è¿‘æ¶ˆæ¯ N-5]
        Rec2[æœ€è¿‘æ¶ˆæ¯ N-4]
        Rec3[æœ€è¿‘æ¶ˆæ¯ N-3]
        Rec4[æœ€è¿‘æ¶ˆæ¯ N-2]
        Rec5[æœ€è¿‘æ¶ˆæ¯ N-1]
        Rec6[å½“å‰æ¶ˆæ¯ N]
    end

    subgraph Process["å¤„ç†å±‚"]
        Summ[æ‘˜è¦ç”Ÿæˆå™¨<br/>ConversationSummarizer]
        Cache[æ‘˜è¦ç¼“å­˜åˆ¤æ–­<br/>should_summarize]
    end

    subgraph Storage["å­˜å‚¨å±‚"]
        DB[(PostgreSQL)]
        Table1[messages è¡¨<br/><br/>id, role, content,<br/>conversation_id,<br/>created_at]
        Table2[conversation_summaries è¡¨<br/><br/>id, conversation_id,<br/>summary,<br/>covered_message_count,<br/>created_at, updated_at]
    end

    subgraph Output["è¾“å‡ºå±‚"]
        Prompt["æœ€ç»ˆ Prompt ç»“æ„<br/><br/>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”<br/>â”‚ System Prompt  â”‚<br/>â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤<br/>â”‚ ã€å¯¹è¯èƒŒæ™¯ã€‘    â”‚<br/>â”‚ Summary (å‹ç¼©) â”‚<br/>â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤<br/>â”‚ History (æœ€è¿‘6) â”‚<br/>â”‚ Rec1 â†’ Rec6    â”‚<br/>â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤<br/>â”‚ Current Message â”‚<br/>â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"]
    end

    Msg1 & Msg2 & Msg3 & MsgN -->|å†å²æ¶ˆæ¯<br/>éœ€è¦æ‘˜è¦| Summ
    Rec1 & Rec2 & Rec3 & Rec4 & Rec5 & Rec6 -->|æœ€è¿‘çª—å£<br/>ä¿æŒåŸæ ·| Prompt

    Summ --> Cache
    Cache -->|éœ€è¦ç”Ÿæˆ/æ›´æ–°| Summ
    Summ -->|ä¿å­˜/è¯»å–| Table2
    Table1 -->|è¯»å–æ‰€æœ‰æ¶ˆæ¯| Summ

    Table2 -->|æ‘˜è¦å†…å®¹| Prompt
    Prompt --> LLM[ğŸ¤– LLM ç”Ÿæˆ]

    style Summ fill:#ffe6e6
    style Cache fill:#fff4e6
    style Prompt fill:#e6f3ff
    style Table2 fill:#e6ffe6
```

##### ç³»ç»ŸçŠ¶æ€è½¬æ¢å›¾

**å¯¹è¯æ‘˜è¦çš„çŠ¶æ€æœºï¼š**

```mermaid
stateDiagram-v2
    [*] --> NoSummary: å¯¹è¯å¼€å§‹<br/>(< 10 æ¡æ¶ˆæ¯)

    NoSummary --> GeneratingSummary: æ¶ˆæ¯æ•°è¾¾åˆ°é˜ˆå€¼<br/>(â‰¥ 10 æ¡)
    NoSummary --> NoSummary: ç»§ç»­å¯¹è¯<br/>(ä½¿ç”¨æ—¶é—´çª—å£)

    GeneratingSummary --> HasSummary: æ‘˜è¦ç”ŸæˆæˆåŠŸ
    GeneratingSummary --> NoSummary: ç”Ÿæˆå¤±è´¥<br/>(é™çº§åˆ°æ—¶é—´çª—å£)

    HasSummary --> Cached: æ–°å¢æ¶ˆæ¯ < 5 æ¡<br/>(ä½¿ç”¨ç¼“å­˜)
    HasSummary --> UpdatingSummary: æ–°å¢æ¶ˆæ¯ â‰¥ 5 æ¡<br/>(è§¦å‘æ›´æ–°)

    Cached --> Cached: ç»§ç»­å¯¹è¯<br/>(å¤ç”¨æ‘˜è¦)
    Cached --> UpdatingSummary: ç´¯ç§¯å·®å€¼è¾¾åˆ°é˜ˆå€¼

    UpdatingSummary --> HasSummary: æ›´æ–°å®Œæˆ
    UpdatingSummary --> HasSummary: æ›´æ–°å¤±è´¥<br/>(ä¿ç•™æ—§æ‘˜è¦)

    HasSummary --> [*]: å¯¹è¯ç»“æŸ
    NoSummary --> [*]: å¯¹è¯ç»“æŸ

    note right of NoSummary
        çŠ¶æ€ç‰¹å¾:
        - æ¶ˆæ¯æ•°: 0-9
        - ä¸Šä¸‹æ–‡: æœ€è¿‘ 6 æ¡
        - Token ä¼˜åŒ–: æ— 
    end note

    note right of HasSummary
        çŠ¶æ€ç‰¹å¾:
        - æ¶ˆæ¯æ•°: â‰¥ 10
        - ä¸Šä¸‹æ–‡: æ‘˜è¦ + æœ€è¿‘ 6 æ¡
        - Token ä¼˜åŒ–: 60-75%
        - covered_count: å·²æ‘˜è¦æ¶ˆæ¯æ•°
    end note
```

##### æ ¸å¿ƒæµç¨‹å›¾

**Phase 1 çš„å®Œæ•´å·¥ä½œæµç¨‹ï¼š**

```mermaid
flowchart TD
    Start([ç”¨æˆ·å‘é€æ¶ˆæ¯]) --> CheckMsg{æ¶ˆæ¯æ•°é‡ â‰¥ 10?}

    CheckMsg -->|å¦| GetRecent[è·å–æœ€è¿‘ 6 æ¡æ¶ˆæ¯]
    CheckMsg -->|æ˜¯| CheckSummary{æ‘˜è¦æ˜¯å¦å­˜åœ¨?}

    CheckSummary -->|å¦| GenerateSummary[ç”Ÿæˆæ–°æ‘˜è¦<br/>è¦†ç›–æ‰€æœ‰å†å² - æœ€è¿‘6æ¡]
    CheckSummary -->|æ˜¯| CheckDelta{æ–°å¢æ¶ˆæ¯ â‰¥ 5?}

    CheckDelta -->|æ˜¯| UpdateSummary[æ›´æ–°æ‘˜è¦<br/>åŒ…å«æ–°å†…å®¹]
    CheckDelta -->|å¦| UseCached[ä½¿ç”¨å·²æœ‰æ‘˜è¦]

    GenerateSummary --> SaveSummary[ä¿å­˜æ‘˜è¦åˆ°æ•°æ®åº“]
    UpdateSummary --> SaveSummary
    SaveSummary --> GetRecent
    UseCached --> GetRecent

    GetRecent --> BuildPrompt[æ„å»º Prompt:<br/>Summary + Recent Window + Current]
    BuildPrompt --> LLM[å‘é€åˆ° LLM]
    LLM --> Response([è¿”å›å“åº”])

    style GenerateSummary fill:#ffe6e6
    style UpdateSummary fill:#fff4e6
    style SaveSummary fill:#e6f3ff
    style BuildPrompt fill:#e6ffe6
```

**æ‘˜è¦ç”Ÿæˆå†³ç­–æ ‘ï¼š**

```mermaid
graph TD
    A[æ¥æ”¶å¯¹è¯è¯·æ±‚] --> B{æ¶ˆæ¯æ€»æ•° < 10?}
    B -->|æ˜¯| C[æ— éœ€æ‘˜è¦<br/>ç›´æ¥ä½¿ç”¨æ—¶é—´çª—å£]
    B -->|å¦| D{æ‘˜è¦å­˜åœ¨?}

    D -->|å¦| E[é¦–æ¬¡ç”Ÿæˆæ‘˜è¦<br/>æ‘˜è¦æ‰€æœ‰å†å² - æœ€è¿‘6æ¡]
    D -->|æ˜¯| F{æ¶ˆæ¯æ€»æ•° - å·²æ‘˜è¦æ•°é‡ â‰¥ 5?}

    F -->|å¦| G[ä½¿ç”¨ç¼“å­˜æ‘˜è¦<br/>æ— éœ€é‡æ–°ç”Ÿæˆ]
    F -->|æ˜¯| H[å¢é‡æ›´æ–°æ‘˜è¦<br/>åŒ…å«æ–°å¯¹è¯å†…å®¹]

    E --> I[ä¿å­˜åˆ°æ•°æ®åº“]
    H --> I
    I --> J[è¿”å›æ‘˜è¦å†…å®¹]

    C --> K[è¿”å›ç©ºæ‘˜è¦]
    G --> J

    style E fill:#ff9999
    style H fill:#ffcc99
    style G fill:#99ccff
    style C fill:#99ff99
```

##### è¯·æ±‚å¤„ç†åºåˆ—å›¾

**ç”¨æˆ·è¯·æ±‚çš„å®Œæ•´å¤„ç†æµç¨‹ï¼š**

```mermaid
sequenceDiagram
    participant User as ç”¨æˆ·
    participant API as ChatHandler
    participant Summarizer as ConversationSummarizer
    participant DB as ConversationStore
    participant LLM as LLM Service

    User->>API: å‘é€æ¶ˆæ¯
    API->>DB: ä¿å­˜ç”¨æˆ·æ¶ˆæ¯

    Note over API,Summarizer: ä¸Šä¸‹æ–‡æ„å»ºé˜¶æ®µ
    API->>DB: è·å–æ¶ˆæ¯æ€»æ•°
    DB-->>API: è¿”å› count

    API->>Summarizer: should_summarize(conv_id)?
    Summarizer->>DB: count_messages(conv_id)
    Summarizer->>DB: get_summary(conv_id)

    alt æ¶ˆæ¯æ•° â‰¥ 10 ä¸”æ— æ‘˜è¦
        Summarizer->>DB: list_messages(limit=None)
        DB-->>Summarizer: æ‰€æœ‰å†å²æ¶ˆæ¯
        Summarizer->>Summarizer: æå– [æ‰€æœ‰ - æœ€è¿‘6æ¡]
        Summarizer->>LLM: ç”Ÿæˆæ‘˜è¦è¯·æ±‚
        LLM-->>Summarizer: è¿”å›æ‘˜è¦æ–‡æœ¬
        Summarizer->>DB: save_summary()
    else æ¶ˆæ¯æ•° â‰¥ 10 ä¸”æœ‰æ‘˜è¦
        Summarizer->>Summarizer: è®¡ç®— delta
        alt delta â‰¥ 5
            Summarizer->>DB: list_messages()
            Summarizer->>LLM: æ›´æ–°æ‘˜è¦
            LLM-->>Summarizer: æ–°æ‘˜è¦
            Summarizer->>DB: update_summary()
        else delta < 5
            Summarizer-->>API: ä½¿ç”¨ç¼“å­˜æ‘˜è¦
        end
    else æ¶ˆæ¯æ•° < 10
        Summarizer-->>API: è¿”å› None
    end

    API->>DB: list_messages(limit=6, desc=True)
    DB-->>API: æœ€è¿‘ 6 æ¡æ¶ˆæ¯

    Note over API,LLM: Prompt æ„å»ºä¸ç”Ÿæˆ
    API->>API: æ„å»º Prompt:
    Note over API: [System Prompt]<br/>[Summary] (å¯é€‰)<br/>[Recent Window]<br/>[Current Message]

    API->>LLM: å‘é€å®Œæ•´ Prompt
    LLM-->>API: è¿”å›å“åº”

    API->>DB: ä¿å­˜åŠ©æ‰‹æ¶ˆæ¯
    API-->>User: è¿”å›å“åº”

    rect rgba(255, 200, 200, 0.3)
        Note over Summarizer,DB: å¼‚æ­¥æ­¥éª¤ï¼ˆä¸é˜»å¡å“åº”ï¼‰
        API->>Summarizer: create_task(åå°æ£€æŸ¥)
        Note right of API: å¦‚æœå¯¹è¯å¢é•¿ï¼Œ<br/>ä¸‹æ¬¡è¯·æ±‚æ—¶ä¼šæ›´æ–°æ‘˜è¦
    end
```

#### 2.1.5 å®ç°é€»è¾‘

**æ ¸å¿ƒä»£ç å®ç°ï¼š**

```python
class ConversationSummarizer:
    """å¯¹è¯æ‘˜è¦å™¨ - å‹ç¼©å†å²å¯¹è¯ä»¥é™ä½ Token æ¶ˆè€—"""

    def __init__(self, llm: BaseChatModel, min_messages: int = 10):
        self.llm = llm  # ä½¿ç”¨è½»é‡çº§æ¨¡å‹ï¼Œå¦‚ GPT-3.5-Turbo
        self.min_messages = min_messages
        self.update_delta = 5  # æ¯ 5 æ¡æ–°æ¶ˆæ¯æ›´æ–°ä¸€æ¬¡æ‘˜è¦

    async def should_summarize(self, conversation_id: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦ç”Ÿæˆ/æ›´æ–°æ‘˜è¦"""
        message_count = await self.store.count_messages(conversation_id)
        last_summary = await self.store.get_summary(conversation_id)

        # æ¶ˆæ¯æ•°ä¸è¶³ï¼Œæ— éœ€æ‘˜è¦
        if message_count < self.min_messages:
            return False

        # é¦–æ¬¡è¾¾åˆ°é˜ˆå€¼ï¼Œéœ€è¦ç”Ÿæˆæ‘˜è¦
        if last_summary is None:
            return True

        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°ï¼ˆæ–°å¢æ¶ˆæ¯æ•° >= deltaï¼‰
        return message_count - last_summary.covered_message_count >= self.update_delta

    async def generate_summary(self, conversation_id: str) -> str:
        """ç”Ÿæˆå¯¹è¯æ‘˜è¦"""
        # è·å–æ‰€æœ‰å†å²æ¶ˆæ¯ï¼ˆé™¤äº†æœ€è¿‘ 6 æ¡ï¼Œå®ƒä»¬ä¼šä¿ç•™åŸæ–‡ï¼‰
        all_messages = await self.store.list_messages(conversation_id)
        to_summarize = all_messages[:-6]

        if not to_summarize:
            return None

        # æ„å»ºæ‘˜è¦ Prompt
        prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯å¯¹è¯æ‘˜è¦ä¸“å®¶ã€‚è¯·å°†ä»¥ä¸‹å¯¹è¯å†å²æµ“ç¼©ä¸º 2-3 å¥è¯çš„æ‘˜è¦ï¼Œçªå‡ºï¼š
1. ç”¨æˆ·çš„æ ¸å¿ƒè¯‰æ±‚å’Œåå¥½
2. å·²è®¨è®ºçš„å…³é”®è¯é¢˜
3. ä»»ä½•é‡è¦çš„èƒŒæ™¯ä¿¡æ¯

ä¿æŒç®€æ´ï¼Œé¿å…å†—ä½™ã€‚"""),
            ("user", "{conversation}")
        ])

        # è°ƒç”¨ LLM ç”Ÿæˆæ‘˜è¦
        summary = await self.llm.ainvoke(
            prompt.format(conversation=format_messages(to_summarize))
        )

        # ä¿å­˜æ‘˜è¦åˆ°æ•°æ®åº“
        await self.store.save_summary(
            conversation_id=conversation_id,
            summary=summary.content,
            covered_message_count=len(to_summarize)
        )

        return summary.content

    async def update_summary(self, conversation_id: str, old_summary: dict) -> str:
        """å¢é‡æ›´æ–°æ‘˜è¦ï¼ˆæ¯”å…¨é‡ç”Ÿæˆæ›´çœ Tokenï¼‰"""
        # è·å–æ–°å¢çš„æ¶ˆæ¯
        all_messages = await self.store.list_messages(conversation_id)
        new_messages = all_messages[old_summary.covered_message_count:-6]

        if not new_messages:
            return old_summary.summary

        # å¢é‡æ›´æ–° Prompt
        prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯å¯¹è¯æ‘˜è¦ä¸“å®¶ã€‚ä»¥ä¸‹æ˜¯ä¹‹å‰çš„å¯¹è¯æ‘˜è¦å’Œæ–°å¢çš„å¯¹è¯å†…å®¹ã€‚
è¯·æ›´æ–°æ‘˜è¦ï¼Œæ•´åˆæ–°ä¿¡æ¯å¹¶ä¿æŒç®€æ´ã€‚"""),
            ("user", """ä¹‹å‰çš„æ‘˜è¦ï¼š
{old_summary}

æ–°å¢çš„å¯¹è¯ï¼š
{new_messages}

è¯·è¾“å‡ºæ›´æ–°åçš„æ‘˜è¦ï¼š""")
        ])

        # è°ƒç”¨ LLM æ›´æ–°æ‘˜è¦
        new_summary = await self.llm.ainvoke(
            prompt.format(
                old_summary=old_summary.summary,
                new_messages=format_messages(new_messages)
            )
        )

        # ä¿å­˜æ›´æ–°åçš„æ‘˜è¦
        await self.store.save_summary(
            conversation_id=conversation_id,
            summary=new_summary.content,
            covered_message_count=len(all_messages[:-6])
        )

        return new_summary.content
```

#### 2.1.6 é›†æˆåˆ° Handler

**åœ¨ ChatHandler ä¸­é›†æˆæ‘˜è¦åŠŸèƒ½ï¼š**

```python
# in ChatHandler.handle()
async def _get_context(self, conversation_id: str) -> dict:
    """æ„å»ºå¯¹è¯ä¸Šä¸‹æ–‡ï¼ˆæ‘˜è¦ + æœ€è¿‘æ¶ˆæ¯ï¼‰"""

    # 1. è·å–æˆ–ç”Ÿæˆæ‘˜è¦
    summary = None
    if await self._summarizer.should_summarize(conversation_id):
        existing_summary = await self._conversation_store.get_summary(conversation_id)
        if existing_summary is None:
            # é¦–æ¬¡ç”Ÿæˆ
            summary = await self._summarizer.generate_summary(conversation_id)
        else:
            # å¢é‡æ›´æ–°
            summary = await self._summarizer.update_summary(conversation_id, existing_summary)
    else:
        # ä½¿ç”¨ç°æœ‰æ‘˜è¦
        existing = await self._conversation_store.get_summary(conversation_id)
        summary = existing.summary if existing else None

    # 2. è·å–æœ€è¿‘æ¶ˆæ¯ï¼ˆæ—¶é—´çª—å£ï¼‰
    recent = await self._conversation_store.list_messages(
        conversation_id=conversation_id,
        limit=6,
        desc=True
    )
    recent.reverse()

    return {
        "summary": summary,
        "recent_history": recent
    }
```

#### 2.1.7 Prompt æ„å»ºè°ƒæ•´

**åœ¨ Prompt æ„å»ºä¸­æ³¨å…¥æ‘˜è¦ï¼š**

```python
# in completion.py
def _build_general_prompt(
    system_message: str,
    memory_context: str | None,
    summary: str | None,  # æ–°å¢ï¼šå¯¹è¯æ‘˜è¦
    history: list[dict] | None,
    question: str
) -> ChatPromptTemplate:
    """æ„å»ºé€šç”¨å¯¹è¯ Prompt"""

    messages = [("system", system_message)]

    # 1. é•¿æœŸç”¨æˆ·è®°å¿†ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if memory_context:
        messages.append(("system", f"ã€ç”¨æˆ·é•¿æœŸè®°å¿†ã€‘\n{memory_context}"))

    # 2. å¯¹è¯èƒŒæ™¯æ‘˜è¦ï¼ˆæ–°å¢ï¼‰
    if summary:
        messages.append(("system", f"ã€å¯¹è¯èƒŒæ™¯ã€‘\n{summary}"))

    # 3. æœ€è¿‘å¯¹è¯å†å²ï¼ˆæ—¶é—´çª—å£ï¼‰
    if history:
        for msg in history:
            role = "assistant" if msg.get("role") == "assistant" else "human"
            messages.append((role, msg.get("content", "")))

    # 4. å½“å‰é—®é¢˜
    messages.append(("human", question))

    return ChatPromptTemplate.from_messages(messages)
```

**ç¤ºä¾‹ Prompt è¾“å‡ºï¼š**

```
[System]: ä½ æ˜¯ç”µå½±æ¨èä¸“å®¶...

[System]: ã€ç”¨æˆ·é•¿æœŸè®°å¿†ã€‘
ç”¨æˆ·å–œæ¬¢ç§‘å¹»ç”µå½±ï¼Œç‰¹åˆ«æ˜¯è¯ºå…°å¯¼æ¼”çš„ä½œå“ã€‚ä¸å–œæ¬¢ææ€–ç‰‡ã€‚

[System]: ã€å¯¹è¯èƒŒæ™¯ã€‘
ç”¨æˆ·ä¹‹å‰è®¨è®ºäº†90å¹´ä»£ç»å…¸ç§‘å¹»ç”µå½±ï¼Œé‡ç‚¹å…³æ³¨ã€Šé»‘å®¢å¸å›½ã€‹å’Œã€Šç»ˆç»“è€…2ã€‹ã€‚
ç”¨æˆ·è¯¢é—®äº†è¿™ä¸¤éƒ¨ç”µå½±çš„æŠ€æœ¯åˆ›æ–°å’Œæ–‡åŒ–å½±å“ã€‚

[Human]: æ¨èä¸€äº›ç±»ä¼¼é£æ ¼çš„ç”µå½±

[Assistant]: åŸºäºä½ å–œæ¬¢ã€Šé»‘å®¢å¸å›½ã€‹å’Œã€Šç»ˆç»“è€…2ã€‹...

[Human]: è¿™äº›ç”µå½±æœ‰ä»€ä¹ˆå…±åŒç‚¹ï¼Ÿ

[Human]: èƒ½æ¨èä¸€äº›æ›´è¿‘æœŸçš„ä½œå“å—ï¼Ÿ
```

#### 2.1.8 ä¼˜åŠ¿åˆ†æ

**ä¸ Baseline å¯¹æ¯”ï¼š**

| æŒ‡æ ‡ | Baseline | Phase 1 (æ‘˜è¦) | æ”¹è¿› |
|------|----------|----------------|------|
| **Token æ¶ˆè€—**ï¼ˆ50è½®å¯¹è¯ï¼‰| ~8000 | ~680 | â¬‡ï¸ 91.5% |
| **ä¸Šä¸‹æ–‡è¦†ç›–** | æœ€è¿‘ 6 è½® | å…¨éƒ¨å†å²ï¼ˆå‹ç¼©ï¼‰ | âœ… å…¨å±€ |
| **å“åº”å»¶è¿Ÿ** | åŸºå‡† | +50msï¼ˆé¦–æ¬¡ç”Ÿæˆï¼‰ | âš ï¸ è½»å¾®å¢åŠ  |
| **å®ç°å¤æ‚åº¦** | ä½ | ä¸­ | âš ï¸ éœ€é¢å¤–ç®¡ç† |
| **é•¿å¯¹è¯è´¨é‡** | ä¿¡æ¯ä¸¢å¤± | ä¿æŒå…³é”®ä¿¡æ¯ | âœ… æ˜¾è‘—æå‡ |

**å…³é”®ä¼˜åŠ¿ï¼š**

1. **æˆæœ¬ä¼˜åŒ–**ï¼š
   - 50 è½®å¯¹è¯èŠ‚çœ **91.5%** Token
   - æ‘˜è¦ç”Ÿæˆæˆæœ¬ï¼š~4000 tokensï¼ˆä¸€æ¬¡æ€§ï¼‰
   - æ¯æ¬¡è¯·æ±‚æˆæœ¬ï¼š~680 tokens vs 8000 tokens

2. **ä¸Šä¸‹æ–‡ä¿ç•™**ï¼š
   - Baselineï¼šåªèƒ½è®°ä½æœ€è¿‘ 6 è½®
   - Phase 1ï¼šä¿ç•™å…¨éƒ¨å¯¹è¯çš„å…³é”®ä¿¡æ¯

3. **ç”¨æˆ·ä½“éªŒ**ï¼š
   - é•¿å¯¹è¯ä¸­ä¸ä¼šå‡ºç°"å¿˜äº†ä¹‹å‰è¯´çš„"çš„é—®é¢˜
   - Agent èƒ½è®°ä½å¯¹è¯æ—©æœŸçš„ç”¨æˆ·åå¥½

#### 2.1.9 å…³é”®è®¾è®¡å†³ç­–æ€»ç»“

| å†³ç­–ç‚¹ | é€‰æ‹© | ç†ç”± | æƒè¡¡ |
|-------|------|------|------|
| **æ¶æ„æ¨¡å¼** | æ»‘åŠ¨çª—å£ + æ‘˜è¦ | å¹³è¡¡å…¨å±€ä¸Šä¸‹æ–‡å’Œå±€éƒ¨ç»†èŠ‚ | éœ€è¦é¢å¤–çš„æ‘˜è¦ç®¡ç† |
| **è§¦å‘é˜ˆå€¼** | 10 æ¡æ¶ˆæ¯ | ç¡®ä¿æœ‰è¶³å¤Ÿä¸Šä¸‹æ–‡ï¼Œé¿å…è¿‡æ—©æ‘˜è¦ | çŸ­å¯¹è¯æ— æ‘˜è¦ä¼˜åŒ– |
| **æ›´æ–°é¢‘ç‡** | æ¯ 5 æ¡æ¶ˆæ¯ | å¹³è¡¡æ–°é²œåº¦å’Œæˆæœ¬ | å¯èƒ½æœ‰ 2-3 è½®å»¶è¿Ÿ |
| **å­˜å‚¨ç»“æ„** | ç‹¬ç«‹è¡¨ | æ¸…æ™°åˆ†ç¦»ï¼Œæ˜“æ‰©å±• | éœ€è¦ JOIN æŸ¥è¯¢ |
| **LLM é€‰æ‹©** | GPT-3.5-Turbo | æ‘˜è¦ä¸éœ€è¦å¼ºæ¨ç†ï¼Œæˆæœ¬ä½ | è´¨é‡ç•¥ä½äº GPT-4 |
| **æ›´æ–°ç­–ç•¥** | å¢é‡æ›´æ–° | é™ä½ Token æ¶ˆè€— 80% | å¯èƒ½ç´¯ç§¯è¯¯å·® |
| **çª—å£å¤§å°** | 6 æ¡æ¶ˆæ¯ | è¦†ç›–æœ€è¿‘ 3 è½®ï¼Œç¬¦åˆå·¥ä½œè®°å¿† | æ¯”çº¯æ‘˜è¦å¤š 480 tokens |
| **é™çº§ç­–ç•¥** | å¤±è´¥å›é€€åˆ°æ—¶é—´çª—å£ | ä¿è¯å¯ç”¨æ€§ | å¤±å»ä¼˜åŒ–æ•ˆæœ |

---

### Phase 2: ä¸»åŠ¨å¼æƒ…èŠ‚è®°å¿† (Active Episodic Memory)

#### 2.2.1 æ ¸å¿ƒè®¾è®¡ (ä¸»åŠ¨å¼è®°å¿†ç®¡ç†)

ä¸ä»…ä»…æ˜¯è¢«åŠ¨æ£€ç´¢å†å²ï¼Œè€Œæ˜¯å¼•å…¥ **ä¸»åŠ¨å¼è®°å¿†ç®¡ç†ç†å¿µ**ï¼š
1.  **ä¸»åŠ¨å¼è®°å¿†ç®¡ç† (Active Management)**ï¼šèµ‹äºˆ Agent ä¿®æ”¹ã€åˆ é™¤ã€å½’æ¡£è®°å¿†çš„èƒ½åŠ›ã€‚
2.  **æ ¸å¿ƒè®°å¿†åŒº (Core Memory)**ï¼šç»´æŠ¤ä¸€ä¸ªå§‹ç»ˆåœ¨çº¿çš„ã€ç»“æ„åŒ–çš„ç”¨æˆ·ç”»åƒï¼Œå…è®¸ Agent å®æ—¶æ›´æ–°ã€‚
3.  **ä¸¤çº§å­˜å‚¨æ¶æ„**ï¼š
    *   **RAM (Context)**: System Prompt + Core Memory (Profile) + Recent History
    *   **Disk (Archival)**: å‘é‡å­˜å‚¨ (Vector Store) + Checkpoints

#### 2.2.2 æ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User Query  â”‚ ---> â”‚ Memory Agent  â”‚ (ä¸»åŠ¨å¼è®°å¿†æ§åˆ¶å™¨)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                    â”‚ Thinking: "Update profile?" "Search old history?"
       â”‚                    â”‚
       â”‚             â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚  Core Memory  â”‚ (RAM - Editable Profile)
       â”‚             â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                    â”‚
       â”‚             â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚ Archival Mem  â”‚ (Disk - å‘é‡è¯­ä¹‰æœç´¢)
                     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                      â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
                      â”‚ LLM Prompt â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 2.2.3 æ•°æ®æ¨¡å‹

**å¤ç”¨ç°æœ‰å‘é‡å­˜å‚¨åŸºç¡€è®¾æ–½**ï¼ˆå·²æœ‰ Milvus + Postgresï¼‰

```sql
-- æ‰©å±• messages è¡¨
ALTER TABLE messages ADD COLUMN embedding vector(1536);
CREATE INDEX ON messages USING ivfflat (embedding vector_cosine_ops);
```

æˆ–è€…ä½¿ç”¨ç‹¬ç«‹çš„å‘é‡å­˜å‚¨ï¼ˆæ¨èï¼Œé¿å…æ±¡æŸ“ messages è¡¨ï¼‰ï¼š

```python
# åœ¨ Milvus ä¸­åˆ›å»ºæ–° Collection
conversation_episodes_collection = {
    "name": "conversation_episodes",
    "fields": [
        {"name": "id", "type": "VARCHAR", "is_primary": True},
        {"name": "conversation_id", "type": "VARCHAR"},
        {"name": "user_message", "type": "VARCHAR"},
        {"name": "assistant_message", "type": "VARCHAR"},
        {"name": "embedding", "type": "FLOAT_VECTOR", "dim": 1536},
        {"name": "created_at", "type": "INT64"},
    ]
}
```

#### 2.2.4 å®ç°é€»è¾‘ (ä¸»åŠ¨å¼è®°å¿†ç®¡ç†)

**1. Core Memory (ç”¨æˆ·ç”»åƒ)**
åœ¨ `ConversationStore` ä¸­ç»´æŠ¤ä¸€ä¸ª JSON å­—æ®µ `core_memory`ï¼š
```json
{
  "persona": "æˆ‘æ˜¯ç”µå½±æ¨èä¸“å®¶ï¼Œä¸“æ³¨äºç§‘å¹»é¢†åŸŸã€‚",
  "human": {
    "name": "User",
    "preferences": ["å–œæ¬¢è¯ºå…°", "ä¸å–œæ¬¢ææ€–ç‰‡"],
    "current_intent": "å¯»æ‰¾90å¹´ä»£ç»å…¸"
  }
}
```

**2. Active Memory Tooling (å·¥å…·è°ƒç”¨)**
å¼€æ”¾ä»¥ä¸‹ Tools ç»™ Agentï¼š
- `core_memory_update(section, content)`: ä¿®æ”¹ç”»åƒã€‚
- `archival_memory_insert(content)`: ä¸»åŠ¨å½’æ¡£å½“å‰å¯¹è¯ç‰‡æ®µã€‚
- `archival_memory_search(query)`: ä¸»åŠ¨æ£€ç´¢å†å²ï¼ˆä¸å†ä»…ä»…æ˜¯è¢«åŠ¨ Top-Kï¼‰ã€‚

**3. Modified Flow**
```python
async def run_memory_loop(message, core_memory):
    # 1. é¢„åˆ¤é˜¶æ®µï¼šå†³å®šæ˜¯å¦éœ€è¦æ“ä½œè®°å¿†
    action = await memory_agent.decide(message, core_memory)

    if action.tool == "core_memory_update":
        # æ‰§è¡Œæ›´æ–°ï¼ˆè‡ªæˆ‘ä¿®æ­£ï¼‰
        core_memory = update(core_memory, action.params)

    if action.tool == "archival_memory_search":
        # ä¸»åŠ¨æ£€ç´¢
        results = await vector_store.search(action.params.query)
        context += results

    # 2. ç”Ÿæˆå›å¤
    response = await generate(message, context, core_memory)
    return response
```

#### 2.2.5 é›†æˆåˆ° Handler

```python
async def _get_hybrid_history(
    self,
    conversation_id: str,
    current_query: str
) -> list[dict]:
    # 1. æ—¶é—´çª—å£ï¼šæœ€è¿‘ 3 è½®
    recent = await self._conversation_store.list_messages(
        conversation_id=conversation_id,
        limit=6,  # 3 è½® * 2 æ¶ˆæ¯
        desc=True
    )
    recent.reverse()

    # 2. è¯­ä¹‰çª—å£ï¼šç›¸å…³çš„å†å²ç‰‡æ®µ
    relevant = await self._episodic_memory.recall_relevant(
        conversation_id=conversation_id,
        query=current_query,
        top_k=3
    )

    # 3. åˆå¹¶å»é‡ï¼ˆé¿å…é‡å¤ï¼‰
    seen_content = {msg["content"] for msg in recent}
    unique_relevant = [
        msg for msg in relevant
        if msg["content"] not in seen_content
    ]

    # 4. æ’åºï¼šç›¸å…³ç‰‡æ®µ + æœ€è¿‘æ¶ˆæ¯
    return unique_relevant + recent
```

#### 2.2.6 ä¼˜åŠ¿åˆ†æ

| åœºæ™¯ | Baseline | Phase 2 (ä¸»åŠ¨å¼è®°å¿†) |
|------|----------|---------------------|
| "æˆ‘ä¸å–œæ¬¢ææ€–ç‰‡äº†" (åå¥½å˜æ›´) | âŒ åªèƒ½è¿½åŠ ï¼Œæ–°æ—§å†²çª | âœ… `core_memory_update` è¦†ç›–æ—§å€¼ |
| "åˆšæ‰è¯´çš„é‚£ä¸ªå¯¼æ¼”æ˜¯è°" | âŒ é—å¿˜/å·²è¢«æˆªæ–­ | âœ… `archival_search` ä¸»åŠ¨æ‰¾å› |
| Token æ•ˆç‡ | å›ºå®š N æ¡ | åŠ¨æ€æ£€ç´¢ + Core Memory (æå°) |

---

### Phase 3: æ¶æ„é‡æ„ (LangGraph State Machine)

#### 2.3.1 é—®é¢˜è¯Šæ–­

**å½“å‰å‚æ•°ä¼ é€’é“¾è·¯**ï¼ˆè„†å¼±ï¼‰ï¼š
```
StreamHandler
  â”œâ”€> KBHandler.process_stream(message, session_id, agent_type, history, ...)
  â”‚     â””â”€> RAGStreamExecutor.stream(plan, message, session_id, kb_prefix, history, ...)
  â”‚           â””â”€> RagManager.run_plan_blocking(plan, message, session_id, history, ...)
  â”‚                 â””â”€> generate_rag_answer(message, context, history, ...)
  â””â”€> _executor.stream(plan, message, session_id, kb_prefix, history, ...)
```

**é—®é¢˜**ï¼š
- ä»»ä½•ä¸€å±‚æ¼ä¼ å‚æ•° â†’ `TypeError`
- æ–°å¢å‚æ•°éœ€ä¿®æ”¹ 6+ ä¸ªæ–‡ä»¶
- æµ‹è¯•æˆæœ¬é«˜ï¼ˆé›†æˆæµ‹è¯•æ‰èƒ½å‘ç°é—®é¢˜ï¼‰

#### 2.3.2 LangGraph è§£å†³æ–¹æ¡ˆ

**æ ¸å¿ƒç†å¿µ**ï¼šç”¨ **State** æ›¿ä»£ "å‚æ•°é€ä¼ "

**å…³é”®å·®å¼‚å¯¹æ¯”**ï¼š

```
ç°åœ¨çš„åšæ³•ï¼ˆå‚æ•°é€å±‚ä¼ é€’ï¼‰:
  StreamHandler
    â”œâ”€ message, session_id, memory_context, history
   savings = (72 - 1.3) / 72  # ~98% èŠ‚çœ
   ```

3. **å»¶è¿Ÿä¼˜åŒ–**ï¼š
   - ç”¨æˆ·å¯¹æ‘˜è¦ç”Ÿæˆä¸å¯è§ï¼ˆåå°ä»»åŠ¡ï¼‰
   - ä½†æ‘˜è¦æ›´æ–°æ—¶æœºå½±å“ç”¨æˆ·ä½“éªŒ
   - GPT-3.5 çš„ä½å»¶è¿Ÿç¡®ä¿æ‘˜è¦ä¸ä¼š"è¿‡æœŸ"

##### 5ï¸âƒ£ **ä¸ºä»€ä¹ˆæ‘˜è¦é‡‡ç”¨å¢é‡æ›´æ–°è€Œéå…¨é‡é‡æ–°ç”Ÿæˆï¼Ÿ**

**å¢é‡æ›´æ–°çš„ä¼˜åŠ¿ï¼š**

| æ–¹æ¡ˆ | Token æ¶ˆè€— | ä¸Šä¸‹æ–‡è´¨é‡ | ä¸€è‡´æ€§ |
|------|-----------|-----------|--------|
| å…¨é‡é‡æ–°ç”Ÿæˆ | é«˜ï¼ˆæ¯æ¬¡åŒ…å«æ‰€æœ‰å†å²ï¼‰ | é«˜ï¼ˆå…¨å±€è§†è§’ï¼‰ | å¯èƒ½é£æ ¼æ¼‚ç§» |
| **å¢é‡æ›´æ–°** (âœ…) | ä½ï¼ˆä»…æ–°å†…å®¹ï¼‰ | ä¸­é«˜ï¼ˆç»§æ‰¿æ—§æ‘˜è¦ï¼‰ | é£æ ¼ä¸€è‡´ |

**å®ç°æ–¹å¼ï¼š**

```python
# å¢é‡æ›´æ–° Prompt
å¢é‡æ‘˜è¦ Prompt = f"""
ä»¥ä¸‹æ˜¯ä¹‹å‰çš„å¯¹è¯æ‘˜è¦ï¼š
{old_summary}

ä»¥ä¸‹æ˜¯æ–°å¢çš„å¯¹è¯å†…å®¹ï¼š
{new_messages}

è¯·æ›´æ–°æ‘˜è¦ï¼Œæ•´åˆæ–°ä¿¡æ¯å¹¶ä¿æŒç®€æ´ã€‚
"""

# è¾“å…¥ Token å¯¹æ¯”
å…¨é‡: 2000 tokens (æ‰€æœ‰å†å²) + 200 tokens (prompt)
å¢é‡: 200 tokens (æ–°æ¶ˆæ¯) + 150 tokens (æ—§æ‘˜è¦) + 200 tokens (prompt)
èŠ‚çœ: ~1650 tokens/æ¬¡
```

**è®¾è®¡æƒè¡¡ï¼š**

- âœ… **ä¼˜åŠ¿**ï¼šé™ä½ 80%+ çš„æ‘˜è¦ Token æ¶ˆè€—
- âœ… **ä¼˜åŠ¿**ï¼šä¿æŒæ‘˜è¦é£æ ¼ä¸€è‡´æ€§ï¼ˆé¿å…é‡æ–°ç”Ÿæˆçš„è¯­æ°”å˜åŒ–ï¼‰
- âš ï¸ **åŠ£åŠ¿**ï¼šå¯èƒ½ç´¯ç§¯é”™è¯¯ï¼ˆæ—§æ‘˜è¦çš„ä¸å‡†ç¡®ä¼šè¢«ä¿ç•™ï¼‰
- ğŸ”§ **ç¼“è§£**ï¼šæ¯ 10 æ¬¡å¢é‡æ›´æ–°åï¼Œå¼ºåˆ¶å…¨é‡é‡æ–°ç”Ÿæˆä¸€æ¬¡

##### 6ï¸âƒ£ **ä¸ºä»€ä¹ˆæ‘˜è¦ä¿ç•™"æœ€è¿‘ 6 æ¡"è€Œä¸æ‘˜è¦ï¼Ÿ**

**çª—å£å¤§å°çš„è®¾è®¡è€ƒé‡ï¼š**

```mermaid
graph LR
    A[çª—å£å¤§å° = 3 æ¡] -->|å¤ªçŸ­| B[ä¸¢å¤±æœ€è¿‘ä¸Šä¸‹æ–‡<br/>"åˆšæ‰æåˆ°çš„ç”µå½±..."<br/>å¯èƒ½å·²è¢«æˆªæ–­]
    C[çª—å£å¤§å° = 6 æ¡] -->|æœ€ä½³| D[å¹³è¡¡ç»†èŠ‚å’Œæˆæœ¬<br/>è¦†ç›–æœ€è¿‘ 3 è½®å¯¹è¯<br/>å®Œæ•´ä¿ç•™å½“å‰è¯é¢˜]
    E[çª—å£å¤§å° = 10 æ¡] -->|å¤ªé•¿| F[Token æµªè´¹<br/>ä¸æ‘˜è¦é‡å¤<br/>æ”¶ç›Šé€’å‡]
```

**ç»éªŒæ³•åˆ™ï¼š**

- **3 è½®å¯¹è¯** = 6 æ¡æ¶ˆæ¯ï¼ˆç”¨æˆ· + åŠ©æ‰‹ï¼‰
- äººç±»å·¥ä½œè®°å¿†å®¹é‡ï¼š7 Â± 2 é¡¹ï¼ˆMiller's Lawï¼‰
- æœ€è¿‘ 3 è½®å¯¹è¯é€šå¸¸åŒ…å«ï¼š
  1. ç”¨æˆ·æå‡ºçš„é—®é¢˜
  2. åŠ©æ‰‹çš„åˆæ­¥å›ç­”
  3. ç”¨æˆ·çš„è¿½é—®æˆ–ç¡®è®¤
  4. åŠ©æ‰‹çš„è¯¦ç»†è¯´æ˜

**å®éªŒæ•°æ®ï¼ˆæ¨¡æ‹Ÿï¼‰ï¼š**

| çª—å£å¤§å° | Token æ¶ˆè€— | ä¸Šä¸‹æ–‡å®Œæ•´æ€§ | ç”¨æˆ·æ»¡æ„åº¦ |
|---------|-----------|-------------|-----------|
| 3 æ¡ | 240 | 65% | 3.2/5 |
| **6 æ¡** | **480** | **92%** | **4.7/5** |
| 9 æ¡ | 720 | 95% | 4.8/5 |

**ç»“è®º**ï¼š6 æ¡æ˜¯æ€§ä»·æ¯”æœ€ä¼˜çš„é€‰æ‹©ã€‚

#### 2.1.12 å…³é”®è®¾è®¡å†³ç­–æ€»ç»“

| å†³ç­–ç‚¹ | é€‰æ‹© | ç†ç”± | æƒè¡¡ |
|-------|------|------|------|
| **æ¶æ„æ¨¡å¼** | æ»‘åŠ¨çª—å£ + æ‘˜è¦ | å¹³è¡¡å…¨å±€ä¸Šä¸‹æ–‡å’Œå±€éƒ¨ç»†èŠ‚ | éœ€è¦é¢å¤–çš„æ‘˜è¦ç®¡ç† |
| **è§¦å‘é˜ˆå€¼** | 10 æ¡æ¶ˆæ¯ | ç¡®ä¿æœ‰è¶³å¤Ÿä¸Šä¸‹æ–‡ï¼Œé¿å…è¿‡æ—©æ‘˜è¦ | çŸ­å¯¹è¯æ— æ‘˜è¦ä¼˜åŒ– |
| **æ›´æ–°é¢‘ç‡** | æ¯ 5 æ¡æ¶ˆæ¯ | å¹³è¡¡æ–°é²œåº¦å’Œæˆæœ¬ | å¯èƒ½æœ‰ 2-3 è½®å»¶è¿Ÿ |
| **å­˜å‚¨ç»“æ„** | ç‹¬ç«‹è¡¨ | æ¸…æ™°åˆ†ç¦»ï¼Œæ˜“æ‰©å±• | éœ€è¦ JOIN æŸ¥è¯¢ |
| **LLM é€‰æ‹©** | GPT-3.5-Turbo | æ‘˜è¦ä¸éœ€è¦å¼ºæ¨ç†ï¼Œæˆæœ¬ä½ | è´¨é‡ç•¥ä½äº GPT-4 |
| **æ›´æ–°ç­–ç•¥** | å¢é‡æ›´æ–° | é™ä½ Token æ¶ˆè€— 80% | å¯èƒ½ç´¯ç§¯è¯¯å·® |
| **çª—å£å¤§å°** | 6 æ¡æ¶ˆæ¯ | è¦†ç›–æœ€è¿‘ 3 è½®ï¼Œç¬¦åˆå·¥ä½œè®°å¿† | æ¯”çº¯æ‘˜è¦å¤š 480 tokens |
| **é™çº§ç­–ç•¥** | å¤±è´¥å›é€€åˆ°æ—¶é—´çª—å£ | ä¿è¯å¯ç”¨æ€§ | å¤±å»ä¼˜åŒ–æ•ˆæœ |

---

### Phase 2: ä¸»åŠ¨å¼æƒ…èŠ‚è®°å¿† (Active Episodic Memory)

#### 2.2.1 æ ¸å¿ƒè®¾è®¡ (ä¸»åŠ¨å¼è®°å¿†ç®¡ç†)

ä¸ä»…ä»…æ˜¯è¢«åŠ¨æ£€ç´¢å†å²ï¼Œè€Œæ˜¯å¼•å…¥ **ä¸»åŠ¨å¼è®°å¿†ç®¡ç†ç†å¿µ**ï¼š
1.  **ä¸»åŠ¨å¼è®°å¿†ç®¡ç† (Active Management)**ï¼šèµ‹äºˆ Agent ä¿®æ”¹ã€åˆ é™¤ã€å½’æ¡£è®°å¿†çš„èƒ½åŠ›ã€‚
2.  **æ ¸å¿ƒè®°å¿†åŒº (Core Memory)**ï¼šç»´æŠ¤ä¸€ä¸ªå§‹ç»ˆåœ¨çº¿çš„ã€ç»“æ„åŒ–çš„ç”¨æˆ·ç”»åƒï¼Œå…è®¸ Agent å®æ—¶æ›´æ–°ã€‚
3.  **ä¸¤çº§å­˜å‚¨æ¶æ„**ï¼š
    *   **RAM (Context)**: System Prompt + Core Memory (Profile) + Recent History
    *   **Disk (Archival)**: å‘é‡å­˜å‚¨ (Vector Store) + Checkpoints

#### 2.2.2 æ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User Query  â”‚ ---> â”‚ Memory Agent  â”‚ (ä¸»åŠ¨å¼è®°å¿†æ§åˆ¶å™¨)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                    â”‚ Thinking: "Update profile?" "Search old history?"
       â”‚                    â”‚
       â”‚             â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚  Core Memory  â”‚ (RAM - Editable Profile)
       â”‚             â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                    â”‚
       â”‚             â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚ Archival Mem  â”‚ (Disk - å‘é‡è¯­ä¹‰æœç´¢)
                     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                      â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
                      â”‚ LLM Prompt â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 2.2.3 æ•°æ®æ¨¡å‹

**å¤ç”¨ç°æœ‰å‘é‡å­˜å‚¨åŸºç¡€è®¾æ–½**ï¼ˆå·²æœ‰ Milvus + Postgresï¼‰

```sql
-- æ‰©å±• messages è¡¨
ALTER TABLE messages ADD COLUMN embedding vector(1536);
CREATE INDEX ON messages USING ivfflat (embedding vector_cosine_ops);
```

æˆ–è€…ä½¿ç”¨ç‹¬ç«‹çš„å‘é‡å­˜å‚¨ï¼ˆæ¨èï¼Œé¿å…æ±¡æŸ“ messages è¡¨ï¼‰ï¼š

```python
# åœ¨ Milvus ä¸­åˆ›å»ºæ–° Collection
conversation_episodes_collection = {
    "name": "conversation_episodes",
    "fields": [
        {"name": "id", "type": "VARCHAR", "is_primary": True},
        {"name": "conversation_id", "type": "VARCHAR"},
        {"name": "user_message", "type": "VARCHAR"},
        {"name": "assistant_message", "type": "VARCHAR"},
        {"name": "embedding", "type": "FLOAT_VECTOR", "dim": 1536},
        {"name": "created_at", "type": "INT64"},
    ]
}
```

#### 2.2.4 å®ç°é€»è¾‘ (ä¸»åŠ¨å¼è®°å¿†ç®¡ç†)

**1. Core Memory (ç”¨æˆ·ç”»åƒ)**
åœ¨ `ConversationStore` ä¸­ç»´æŠ¤ä¸€ä¸ª JSON å­—æ®µ `core_memory`ï¼š
```json
{
  "persona": "æˆ‘æ˜¯ç”µå½±æ¨èä¸“å®¶ï¼Œä¸“æ³¨äºç§‘å¹»é¢†åŸŸã€‚",
  "human": {
    "name": "User",
    "preferences": ["å–œæ¬¢è¯ºå…°", "ä¸å–œæ¬¢ææ€–ç‰‡"],
    "current_intent": "å¯»æ‰¾90å¹´ä»£ç»å…¸"
  }
}
```

**2. Active Memory Tooling (å·¥å…·è°ƒç”¨)**
å¼€æ”¾ä»¥ä¸‹ Tools ç»™ Agentï¼š
- `core_memory_update(section, content)`: ä¿®æ”¹ç”»åƒã€‚
- `archival_memory_insert(content)`: ä¸»åŠ¨å½’æ¡£å½“å‰å¯¹è¯ç‰‡æ®µã€‚
- `archival_memory_search(query)`: ä¸»åŠ¨æ£€ç´¢å†å²ï¼ˆä¸å†ä»…ä»…æ˜¯è¢«åŠ¨ Top-Kï¼‰ã€‚

**3. Modified Flow**
```python
async def run_memory_loop(message, core_memory):
    # 1. é¢„åˆ¤é˜¶æ®µï¼šå†³å®šæ˜¯å¦éœ€è¦æ“ä½œè®°å¿†
    action = await memory_agent.decide(message, core_memory)
    
    if action.tool == "core_memory_update":
        # æ‰§è¡Œæ›´æ–°ï¼ˆè‡ªæˆ‘ä¿®æ­£ï¼‰
        core_memory = update(core_memory, action.params)
    
    if action.tool == "archival_memory_search":
        # ä¸»åŠ¨æ£€ç´¢
        results = await vector_store.search(action.params.query)
        context += results
        
    # 2. ç”Ÿæˆå›å¤
    response = await generate(message, context, core_memory)
    return response
```

#### 2.2.5 é›†æˆåˆ° Handler

```python
async def _get_hybrid_history(
    self,
    conversation_id: str,
    current_query: str
) -> list[dict]:
    # 1. æ—¶é—´çª—å£ï¼šæœ€è¿‘ 3 è½®
    recent = await self._conversation_store.list_messages(
        conversation_id=conversation_id,
        limit=6,  # 3 è½® * 2 æ¶ˆæ¯
        desc=True
    )
    recent.reverse()
    
    # 2. è¯­ä¹‰çª—å£ï¼šç›¸å…³çš„å†å²ç‰‡æ®µ
    relevant = await self._episodic_memory.recall_relevant(
        conversation_id=conversation_id,
        query=current_query,
        top_k=3
    )
    
    # 3. åˆå¹¶å»é‡ï¼ˆé¿å…é‡å¤ï¼‰
    seen_content = {msg["content"] for msg in recent}
    unique_relevant = [
        msg for msg in relevant
        if msg["content"] not in seen_content
    ]
    
    # 4. æ’åºï¼šç›¸å…³ç‰‡æ®µ + æœ€è¿‘æ¶ˆæ¯
    return unique_relevant + recent
```

#### 2.2.6 ä¼˜åŠ¿åˆ†æ

| åœºæ™¯ | Baseline | Phase 2 (ä¸»åŠ¨å¼è®°å¿†) |
|------|----------|---------------------|
| "æˆ‘ä¸å–œæ¬¢ææ€–ç‰‡äº†" (åå¥½å˜æ›´) | âŒ åªèƒ½è¿½åŠ ï¼Œæ–°æ—§å†²çª | âœ… `core_memory_update` è¦†ç›–æ—§å€¼ |
| "åˆšæ‰è¯´çš„é‚£ä¸ªå¯¼æ¼”æ˜¯è°" | âŒ é—å¿˜/å·²è¢«æˆªæ–­ | âœ… `archival_search` ä¸»åŠ¨æ‰¾å› |
| Token æ•ˆç‡ | å›ºå®š N æ¡ | åŠ¨æ€æ£€ç´¢ + Core Memory (æå°) |

---

### Phase 3: æ¶æ„é‡æ„ (LangGraph State Machine)

#### 2.3.1 é—®é¢˜è¯Šæ–­

**å½“å‰å‚æ•°ä¼ é€’é“¾è·¯**ï¼ˆè„†å¼±ï¼‰ï¼š
```
StreamHandler
  â”œâ”€> KBHandler.process_stream(message, session_id, agent_type, history, ...)
  â”‚     â””â”€> RAGStreamExecutor.stream(plan, message, session_id, kb_prefix, history, ...)
  â”‚           â””â”€> RagManager.run_plan_blocking(plan, message, session_id, history, ...)
  â”‚                 â””â”€> generate_rag_answer(message, context, history, ...)
  â””â”€> _executor.stream(plan, message, session_id, kb_prefix, history, ...)
```

**é—®é¢˜**ï¼š
- ä»»ä½•ä¸€å±‚æ¼ä¼ å‚æ•° â†’ `TypeError`
- æ–°å¢å‚æ•°éœ€ä¿®æ”¹ 6+ ä¸ªæ–‡ä»¶
- æµ‹è¯•æˆæœ¬é«˜ï¼ˆé›†æˆæµ‹è¯•æ‰èƒ½å‘ç°é—®é¢˜ï¼‰

#### 2.3.2 LangGraph è§£å†³æ–¹æ¡ˆ

**æ ¸å¿ƒç†å¿µ**ï¼šç”¨ **State** æ›¿ä»£ "å‚æ•°é€ä¼ "

**å…³é”®å·®å¼‚å¯¹æ¯”**ï¼š

```
ç°åœ¨çš„åšæ³•ï¼ˆå‚æ•°é€å±‚ä¼ é€’ï¼‰:
  StreamHandler
    â”œâ”€ message, session_id, memory_context, history
    â””â”€> KBHandler.process_stream(message, session_id, memory_context, history)
    â””â”€> RAGStreamExecutor.stream(plan, message, session_id, memory_context, history)

    # æ–°å¢ Phase 1/2 éœ€è¦æ”¹ N ä¸ªå‡½æ•°ç­¾åï¼š
    â””â”€> KBHandler.process_stream(message, session_id, memory_context, history,
                                  summary, episodic_memory)  # â† æ–°å¢å‚æ•°

LangGraph çš„åšæ³•ï¼ˆç»Ÿä¸€çŠ¶æ€ï¼‰:
  StreamHandler
    â””â”€> graph.astream(state)  # â† ä¸€æ¬¡æ€§ä¼ é€’æ‰€æœ‰æ•°æ®

    èŠ‚ç‚¹åªéœ€ä¿®æ”¹ State å®šä¹‰ï¼Œä¸éœ€æ”¹å‡½æ•°ç­¾åï¼š
    async def node(state: State) -> dict:
        return {...state, "new_field": value}  # â† åªæ”¹è¿™é‡Œ
```

**å®Œæ•´çš„ ConversationState å®šä¹‰**ï¼š

```python
from langgraph.graph import StateGraph, START, END
from typing import TypedDict, Annotated, Any
from operator import add
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage

class ConversationState(TypedDict):
    """å¯¹è¯å…¨å±€çŠ¶æ€ - ç®¡ç†æ•´ä¸ªå¯¹è¯æµç¨‹çš„æ‰€æœ‰æ•°æ®"""

    # ==================== è¯·æ±‚çº§åˆ«ä¿¡æ¯ ====================
    user_id: str                           # ç”¨æˆ·IDï¼ˆå¯¹åº” StreamHandler.handle çš„ user_idï¼‰
    message: str                           # å½“å‰ç”¨æˆ·æ¶ˆæ¯ï¼ˆå¯¹åº” StreamHandler.handle çš„ messageï¼‰
    session_id: str                        # ä¼šè¯IDï¼ˆå¯¹åº” StreamHandler.handle çš„ session_idï¼‰
    conversation_id: str                   # å¯¹è¯IDï¼ˆå¯¹åº” StreamHandler.handle çš„ conversation_idï¼‰

    # ==================== è¯·æ±‚é…ç½® ====================
    debug: bool                            # è°ƒè¯•æ¨¡å¼å¼€å…³
    agent_type: str                        # Agent ç±»å‹ï¼ˆhybrid_agent/naive_rag_agent...ï¼‰
    requested_kb_prefix: str | None        # ç”¨æˆ·æŒ‡å®šçš„ KBï¼ˆå¯¹åº” request.kb_prefixï¼‰

    # ==================== è·¯ç”±å†³ç­–ï¼ˆroute_node è¾“å‡ºï¼‰====================
    kb_prefix: str | None                  # æœ€ç»ˆå†³å®šä½¿ç”¨çš„ KB
    worker_name: str | None                # å·¥ä½œè€…åç§°ï¼ˆrouter è¾“å‡ºï¼‰
    use_retrieval: bool                    # æ˜¯å¦éœ€è¦æ£€ç´¢ï¼ˆå–å†³äº kb_prefixï¼‰
    route_decision: dict[str, Any] | None  # å®Œæ•´çš„è·¯ç”±å†³ç­–ä¿¡æ¯
    routing_ms: int | None                 # è·¯ç”±è€—æ—¶

    # ==================== ä¸Šä¸‹æ–‡æ„å»ºï¼ˆrecall_node è¾“å‡ºï¼‰====================
    # çŸ­æœŸè®°å¿†ï¼ˆå¯¹è¯å†å²ï¼‰
    history: list[dict[str, Any]]          # æœ€è¿‘ 6-7 æ¡æ¶ˆæ¯ï¼ˆæ¥è‡ª conversation_storeï¼‰

    # é•¿æœŸè®°å¿†ï¼ˆç”¨æˆ·ä¿¡æ¯ï¼‰
    memory_context: str | None             # é•¿æœŸç”¨æˆ·è®°å¿†ï¼ˆæ¥è‡ª memory_serviceï¼‰

    # Phase 1: å¯¹è¯æ‘˜è¦
    conversation_summary: str | None       # å¯¹è¯æ‘˜è¦ï¼ˆæ¥è‡ª summarizerï¼‰

    # Phase 2: è¯­ä¹‰æ£€ç´¢
    episodic_memory: list[dict[str, Any]] | None  # ç›¸å…³çš„å†å²ç‰‡æ®µï¼ˆæ¥è‡ª episodic_memoryï¼‰

    # ==================== æ‰§è¡Œç»“æœ ====================
    # æ£€ç´¢ç»“æœï¼ˆretrieve_node è¾“å‡ºï¼‰
    retrieval_results: list[dict[str, Any]] | None  # RAG æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
    retrieval_ms: int | None               # æ£€ç´¢è€—æ—¶

    # LLM ç”Ÿæˆï¼ˆgenerate_node è¾“å‡ºï¼‰
    messages: Annotated[list[BaseMessage], add]  # å¯¹è¯æ¶ˆæ¯åˆ—è¡¨ï¼ˆè‡ªåŠ¨è¿½åŠ ï¼‰

    # ==================== å…ƒæ•°æ® ====================
    execution_logs: list[dict[str, Any]] | None   # æ‰§è¡Œæ—¥å¿—ï¼ˆç”¨äº debugï¼‰
    error: str | None                      # é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœ‰ï¼‰
    partial_answer: bool = False            # æ˜¯å¦æ˜¯éƒ¨åˆ†å›å¤ï¼ˆæµå¼ä¸­æ–­ï¼‰
```

**ä¸ºä»€ä¹ˆè¿™ä¸ª State è®¾è®¡æ›´å¥½ï¼Ÿ**

| ç›Šå¤„ | è¯´æ˜ |
|------|------|
| **ä¸€æ¬¡æ€§å®šä¹‰** | æ–°å¢å‚æ•°åªéœ€åœ¨ State åŠ ä¸€è¡Œï¼Œä¸éœ€æ”¹å‡½æ•°ç­¾å |
| **æ•°æ®æº¯æºæ¸…æ™°** | æ¯ä¸ªå­—æ®µæ³¨æ˜æ¥æºï¼ˆå“ªä¸ª nodeã€å“ªä¸ª serviceï¼‰ |
| **è‡ªåŠ¨æ¶ˆæ¯ç®¡ç†** | `Annotated[list, add]` è‡ªåŠ¨åˆå¹¶æ¶ˆæ¯ï¼Œä¸éœ€æ‰‹åŠ¨è¿½åŠ  |
| **æ˜“äºè°ƒè¯•** | `execution_logs` è®°å½•æ¯ä¸ªèŠ‚ç‚¹çš„è¾“å…¥è¾“å‡º |
| **æ”¯æŒå¹¶è¡Œ** | å¤šä¸ªèŠ‚ç‚¹å¯ä»¥å¹¶è¡Œå¤„ç†ï¼ŒState è‡ªåŠ¨åˆå¹¶ç»“æœ |

#### 2.3.3 èŠ‚ç‚¹è®¾è®¡ï¼ˆä¸ç°æœ‰ä»£ç å¯¹åº”ï¼‰

**èŠ‚ç‚¹æ€»è§ˆ**ï¼š
```
START
  â†“
route_node          # è¿›è¡Œè·¯ç”±å†³ç­–ï¼ˆå¯¹åº”ç°æœ‰ Router.route()ï¼‰
  â†“
history_node        # è·å–çŸ­æœŸ+é•¿æœŸè®°å¿†ï¼ˆå¯¹åº”ç°æœ‰ _get_conversation_history + memory_serviceï¼‰
  â†“
retrieve_node       # å¯é€‰ï¼šæ£€ç´¢ï¼ˆä»…å½“ use_retrieval=Trueï¼‰
  â†“
kb_handler_node     # æ¡ä»¶æ‰§è¡Œï¼šä¼˜å…ˆä½¿ç”¨ KB ä¸“ç”¨ handler
  â”œâ”€ if kb_handler found
  â”‚   â†“
  â”‚ generate_kb_node  # ä½¿ç”¨ KB handler ç”Ÿæˆ
  â”‚   â†“
  â”‚ persist_node      # æŒä¹…åŒ–
  â”‚   â†“
  â”‚   END
  â”‚
  â”œâ”€ elseï¼ˆå›é€€åˆ° RAG æ‰§è¡Œå™¨ï¼‰
  â”‚   â†“
  â”‚   generate_rag_node  # ä½¿ç”¨ RAG executor ç”Ÿæˆ
  â”‚   â†“
  â”‚   persist_node       # æŒä¹…åŒ–
  â”‚   â†“
  â”‚   END
```

**è¯¦ç»†èŠ‚ç‚¹å®ç°**ï¼š

##### 1ï¸âƒ£ route_nodeï¼šè·¯ç”±å†³ç­–

```python
import time
from typing import Any, TypedDict
from domain.chat.entities.route_decision import RouteDecision

async def route_node(state: ConversationState) -> dict[str, Any]:
    """
    è·¯ç”±èŠ‚ç‚¹ï¼šæ ¹æ®ç”¨æˆ·æ¶ˆæ¯å’Œé…ç½®å†³å®šä½¿ç”¨å“ªä¸ª KB

    å¯¹åº”ç°æœ‰ä»£ç ï¼š
    - StreamHandler.handle() ç¬¬ 109-138 è¡Œï¼ˆè·¯ç”±é€»è¾‘ï¼‰
    """
    t0 = time.monotonic()

    # è°ƒç”¨ç°æœ‰çš„ Router
    decision: RouteDecision = router.route(
        message=state["message"],
        session_id=state["session_id"],
        requested_kb=state["requested_kb_prefix"],
        agent_type=state["agent_type"],
    )

    routing_ms = int((time.monotonic() - t0) * 1000)

    # å†³å®šæ˜¯å¦éœ€è¦æ£€ç´¢
    use_retrieval = (decision.kb_prefix or "").strip() not in {"", "general"}

    # æ„å»ºè¿”å›å€¼
    result = {
        "kb_prefix": decision.kb_prefix,
        "worker_name": decision.worker_name,
        "use_retrieval": use_retrieval,
        "route_decision": {
            "kb_prefix": decision.kb_prefix,
            "worker_name": decision.worker_name,
            "confidence": decision.confidence,
            "method": decision.method,
            "reason": decision.reason,
        },
        "routing_ms": routing_ms,
    }

    # å¦‚æœå¯ç”¨ Langfuseï¼Œè®°å½•è·¯ç”±å†³ç­–
    if ENABLE_LANGFUSE:
        try:
            from infrastructure.observability import get_current_langfuse_stateful_client
            parent = get_current_langfuse_stateful_client()
            if parent is not None:
                span = parent.span(name="route_decision", input={"message_preview": state["message"][:200]})
                span.end(output=result, metadata={"routing_ms": routing_ms})
        except Exception:
            pass  # è§‚æµ‹æ€§å¤±è´¥ä¸åº”é˜»å¡ä¸»æµç¨‹

    return result
```

##### 2ï¸âƒ£ history_nodeï¼šä¸Šä¸‹æ–‡æ„å»º

```python
async def history_node(state: ConversationState) -> dict[str, Any]:
    """
    å†å²èŠ‚ç‚¹ï¼šè·å–çŸ­æœŸè®°å¿†ï¼ˆå¯¹è¯å†å²ï¼‰å’Œé•¿æœŸè®°å¿†ï¼ˆç”¨æˆ·ä¿¡æ¯ï¼‰

    å¯¹åº”ç°æœ‰ä»£ç ï¼š
    - StreamHandler._get_conversation_history() ç¬¬ 53-62 è¡Œ
    - StreamHandler.handle() ç¬¬ 169-173 è¡Œï¼ˆmemory_serviceï¼‰
    - StreamHandler.handle() ç¬¬ 176-183 è¡Œï¼ˆhistory å»é‡ï¼‰
    """

    # 1. è·å–çŸ­æœŸè®°å¿†ï¼šæœ€è¿‘çš„å¯¹è¯å†å²
    raw_history = await conversation_store.list_messages(
        conversation_id=state["conversation_id"],
        limit=7,  # è·å–æœ€è¿‘ 7 æ¡ï¼ˆåŒ…å«å½“å‰æ¶ˆæ¯ï¼‰
        desc=True,  # é™åº
    )

    # ç¿»è½¬ä¸ºæ—¶é—´æ­£åº
    raw_history.reverse()

    # å»é‡ï¼šå¦‚æœæœ€åä¸€æ¡æ˜¯å½“å‰æ¶ˆæ¯ï¼Œæ’é™¤å®ƒï¼ˆé˜²æ­¢é‡å¤ï¼‰
    history_context = raw_history
    if raw_history and raw_history[-1].get("content") == state["message"]:
        history_context = raw_history[:-1]

    result = {
        "history": history_context,
    }

    # 2. è·å–é•¿æœŸè®°å¿†ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if memory_service is not None:
        try:
            memory_context = await memory_service.recall_context(
                user_id=state["user_id"],
                query=state["message"],
            )
            result["memory_context"] = memory_context
        except Exception as e:
            logger.error(f"Failed to recall memory context: {e}")
            result["memory_context"] = None

    # 3. ç”Ÿæˆå¯¹è¯æ‘˜è¦ï¼ˆPhase 1ï¼Œå¯é€‰ï¼‰
    if summarizer is not None:
        try:
            summary = await summarizer.get_or_generate_summary(state["conversation_id"])
            result["conversation_summary"] = summary
        except Exception as e:
            logger.error(f"Failed to generate summary: {e}")
            result["conversation_summary"] = None

    # 4. å¬å›è¯­ä¹‰ç›¸å…³çš„å†å²ç‰‡æ®µï¼ˆPhase 2ï¼Œå¯é€‰ï¼‰
    if episodic_memory_manager is not None and state["use_retrieval"]:
        try:
            episodes = await episodic_memory_manager.recall_relevant(
                conversation_id=state["conversation_id"],
                query=state["message"],
                top_k=3,
            )
            result["episodic_memory"] = episodes
        except Exception as e:
            logger.error(f"Failed to recall episodic memory: {e}")
            result["episodic_memory"] = None

    # è®°å½•æ‰§è¡Œæ—¥å¿—ï¼ˆç”¨äºè°ƒè¯•ï¼‰
    if state.get("debug"):
        result.setdefault("execution_logs", []).append({
            "node": "history",
            "history_count": len(history_context),
            "has_memory_context": result.get("memory_context") is not None,
            "has_summary": result.get("conversation_summary") is not None,
            "episodic_count": len(result.get("episodic_memory") or []),
        })

    return result
```

##### 3ï¸âƒ£ retrieve_nodeï¼šæ£€ç´¢ï¼ˆæ¡ä»¶æ‰§è¡Œï¼‰

```python
async def retrieve_node(state: ConversationState) -> dict[str, Any]:
    """
    æ£€ç´¢èŠ‚ç‚¹ï¼šä»çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³çš„ä¸Šä¸‹æ–‡
    ä»…å½“ use_retrieval=True æ—¶æ‰§è¡Œ

    å¯¹åº”ç°æœ‰ä»£ç ï¼š
    - RAGStreamExecutor / ChatStreamExecutor çš„æ£€ç´¢é€»è¾‘
    """

    # å¦‚æœä¸éœ€è¦æ£€ç´¢ï¼Œç›´æ¥è·³è¿‡
    if not state["use_retrieval"]:
        return {}  # è¿”å›ç©ºå­—å…¸ï¼ŒState ä¿æŒä¸å˜

    t0 = time.monotonic()

    try:
        # æ„å»º RAG plan
        plan = [
            RagRunSpec(
                agent_type=_resolve_agent_type(
                    agent_type=state["agent_type"],
                    worker_name=state["worker_name"],
                ),
                worker_name=state["worker_name"],
            )
        ]

        # è°ƒç”¨ RAG æ‰§è¡Œå™¨è¿›è¡Œæ£€ç´¢
        results = await rag_executor.retrieve(
            plan=plan,
            query=state["message"],
            kb_prefix=state["kb_prefix"],
            session_id=state["session_id"],
        )

        retrieval_ms = int((time.monotonic() - t0) * 1000)

        return {
            "retrieval_results": results,
            "retrieval_ms": retrieval_ms,
        }

    except Exception as e:
        logger.error(f"Retrieval failed: {e}")
        return {
            "retrieval_results": None,
            "error": f"Retrieval failed: {str(e)}",
        }
```

##### 4ï¸âƒ£ generate_nodeï¼šç”Ÿæˆï¼ˆç»Ÿä¸€ç‰ˆæœ¬ï¼‰

```python
def _build_prompt_from_state(state: ConversationState) -> ChatPromptTemplate:
    """
    ä» State æ„å»º Prompt

    å¯¹åº”ç°æœ‰ä»£ç ï¼š
    - completion.py çš„ _build_general_prompt() / _build_rag_prompt()
    """
    messages = [
        ("system", SYSTEM_PROMPT),
    ]

    # æ·»åŠ é•¿æœŸç”¨æˆ·è®°å¿†
    if state.get("memory_context"):
        messages.append(("system", f"ã€ç”¨æˆ·é•¿æœŸè®°å¿†ã€‘\n{state['memory_context']}"))

    # æ·»åŠ å¯¹è¯æ‘˜è¦ï¼ˆPhase 1ï¼‰
    if state.get("conversation_summary"):
        messages.append(("system", f"ã€å¯¹è¯èƒŒæ™¯ã€‘\n{state['conversation_summary']}"))

    # æ·»åŠ è¯­ä¹‰ç›¸å…³çš„å†å²ç‰‡æ®µï¼ˆPhase 2ï¼‰
    if state.get("episodic_memory"):
        episodes_text = "\n".join([
            f"- {ep.get('user_message', '')} â†’ {ep.get('assistant_message', '')}"
            for ep in state["episodic_memory"]
        ])
        messages.append(("system", f"ã€ç›¸å…³å†å²ã€‘\n{episodes_text}"))

    # æ·»åŠ æ£€ç´¢ç»“æœï¼ˆå¦‚æœ‰ï¼‰
    if state.get("retrieval_results"):
        context_text = "\n".join([
            result.get("content", "")
            for result in state["retrieval_results"][:5]  # æœ€å¤š5æ¡
        ])
        messages.append(("system", f"ã€æ£€ç´¢ä¸Šä¸‹æ–‡ã€‘\n{context_text}"))

    # æ·»åŠ å¯¹è¯å†å²
    for msg in state.get("history", []):
        role = "assistant" if msg.get("role") == "assistant" else "human"
        messages.append((role, msg.get("content", "")))

    # å½“å‰é—®é¢˜
    messages.append(("human", state["message"]))

    return ChatPromptTemplate.from_messages(messages)


async def generate_node(state: ConversationState) -> dict[str, Any]:
    """
    ç”ŸæˆèŠ‚ç‚¹ï¼šLLM ç”Ÿæˆå›å¤

    å¯¹åº”ç°æœ‰ä»£ç ï¼š
    - generate_rag_answer() / ChatCompletionPort.generate()
    """

    try:
        # æ„å»º Promptï¼ˆè‡ªåŠ¨åŒ…å«æ‰€æœ‰ä¸Šä¸‹æ–‡ï¼‰
        prompt = _build_prompt_from_state(state)

        t0 = time.monotonic()

        # è°ƒç”¨ LLM
        response = await llm.ainvoke(
            prompt,
            callbacks=[get_langfuse_callback()],
        )

        generation_ms = int((time.monotonic() - t0) * 1000)

        # è§£æå“åº”
        answer_content = response.content

        # åˆ›å»º AI Messageï¼ˆè‡ªåŠ¨è¿½åŠ åˆ° messages åˆ—è¡¨ï¼‰
        result = {
            "messages": [AIMessage(content=answer_content)],
        }

        if state.get("debug"):
            result.setdefault("execution_logs", []).append({
                "node": "generate",
                "generation_ms": generation_ms,
                "answer_preview": answer_content[:100],
            })

        return result

    except Exception as e:
        logger.error(f"Generation failed: {e}")
        return {
            "error": f"Generation failed: {str(e)}",
            "messages": [AIMessage(content="æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç”Ÿæˆå›å¤ã€‚è¯·ç¨åé‡è¯•ã€‚")],
        }
```

##### 5ï¸âƒ£ persist_nodeï¼šæŒä¹…åŒ–

```python
async def persist_node(state: ConversationState) -> dict[str, Any]:
    """
    æŒä¹…åŒ–èŠ‚ç‚¹ï¼šä¿å­˜å¯¹è¯å’Œç´¢å¼•

    å¯¹åº”ç°æœ‰ä»£ç ï¼š
    - StreamHandler.handle() ç¬¬ 248-260 è¡Œï¼ˆä¿å­˜åŠ©æ‰‹æ¶ˆæ¯ï¼‰
    - StreamHandler.handle() ç¬¬ 461-467 è¡Œï¼ˆç´¢å¼• episodic memoryï¼‰
    """

    # è·å–æœ€æ–°ç”Ÿæˆçš„åŠ©æ‰‹å›å¤
    if not state.get("messages"):
        return {}

    assistant_message = state["messages"][-1]
    answer_content = assistant_message.content

    try:
        # 1. ä¿å­˜å¯¹è¯åˆ°æ•°æ®åº“
        await conversation_store.append_message(
            conversation_id=state["conversation_id"],
            role="assistant",
            content=answer_content,
            debug={"partial": state.get("partial_answer", False)} if state.get("debug") else None,
        )

        # 2. å¼‚æ­¥ç´¢å¼•åˆ° Episodic Memoryï¼ˆä¸é˜»å¡ä¸»æµç¨‹ï¼‰
        if episodic_memory_manager is not None:
            asyncio.create_task(
                episodic_memory_manager.index_episode(
                    conversation_id=state["conversation_id"],
                    user_msg=state["message"],
                    assistant_msg=answer_content,
                )
            )

        # 3. å†™å…¥é•¿æœŸè®°å¿†ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if memory_service is not None and not state.get("partial_answer"):
            asyncio.create_task(
                memory_service.maybe_write(
                    user_id=state["user_id"],
                    user_message=state["message"],
                    assistant_message=answer_content,
                    metadata={
                        "session_id": state["session_id"],
                        "kb_prefix": state.get("kb_prefix"),
                    },
                )
            )

        return {"partial_answer": False}

    except Exception as e:
        logger.error(f"Persistence failed: {e}")
        return {"error": f"Persistence failed: {str(e)}"}
```

#### 2.3.4 Graph ç¼–æ’å’Œæµå¼å¤„ç†

**åˆ›å»º conversation_graph å·¥å‚**ï¼š

```python
# æ–‡ä»¶: infrastructure/chat/conversation_graph.py
from langgraph.graph import StateGraph, START, END, Conditional
from langgraph.types import Send
import asyncio
import logging

logger = logging.getLogger(__name__)

class ConversationGraphBuilder:
    """æ„å»ºå¯¹è¯æµå›¾"""

    def __init__(
        self,
        router,
        conversation_store,
        memory_service=None,
        summarizer=None,
        episodic_memory_manager=None,
        rag_executor=None,
        llm=None,
        kb_handler_factory=None,
        enable_kb_handlers=False,
    ):
        self.router = router
        self.conversation_store = conversation_store
        self.memory_service = memory_service
        self.summarizer = summarizer
        self.episodic_memory_manager = episodic_memory_manager
        self.rag_executor = rag_executor
        self.llm = llm
        self.kb_handler_factory = kb_handler_factory
        self.enable_kb_handlers = enable_kb_handlers

    def _route_to_kb_or_rag(self, state: ConversationState) -> str:
        """æ¡ä»¶è¾¹ï¼šé€‰æ‹©ä½¿ç”¨ KB Handler è¿˜æ˜¯ RAG æ‰§è¡Œå™¨"""
        if self.enable_kb_handlers and state.get("kb_prefix"):
            kb_handler = self.kb_handler_factory.get(state["kb_prefix"])
            if kb_handler is not None:
                return "generate_kb"
        return "generate_rag"

    def _should_retrieve(self, state: ConversationState) -> bool:
        """æ¡ä»¶åˆ¤æ–­ï¼šæ˜¯å¦éœ€è¦æ£€ç´¢"""
        return state.get("use_retrieval", False)

    def build(self) -> CompiledGraph:
        """æ„å»ºç¼–è¯‘åçš„å›¾"""
        builder = StateGraph(ConversationState)

        # ==================== æ·»åŠ èŠ‚ç‚¹ ====================
        builder.add_node(
            "route",
            lambda state: route_node(
                state, router=self.router
            ),
        )

        builder.add_node(
            "history",
            lambda state: history_node(
                state,
                conversation_store=self.conversation_store,
                memory_service=self.memory_service,
                summarizer=self.summarizer,
                episodic_memory_manager=self.episodic_memory_manager,
            ),
        )

        builder.add_node(
            "retrieve",
            lambda state: retrieve_node(
                state,
                rag_executor=self.rag_executor,
            ),
        )

        builder.add_node(
            "generate_kb",
            lambda state: generate_kb_node(
                state,
                kb_handler_factory=self.kb_handler_factory,
            ),
        )

        builder.add_node(
            "generate_rag",
            lambda state: generate_rag_node(
                state,
                llm=self.llm,
            ),
        )

        builder.add_node(
            "persist",
            lambda state: persist_node(
                state,
                conversation_store=self.conversation_store,
                memory_service=self.memory_service,
                episodic_memory_manager=self.episodic_memory_manager,
            ),
        )

        # ==================== å®šä¹‰è¾¹ ====================
        builder.set_entry_point("route")

        # route â†’ historyï¼ˆæ€»æ˜¯ï¼‰
        builder.add_edge("route", "history")

        # history â†’ retrieveï¼ˆæœ‰æ¡ä»¶ï¼‰
        builder.add_conditional_edges(
            "history",
            lambda state: self._should_retrieve(state),
            {
                True: "retrieve",
                False: "generate_kb" if self.enable_kb_handlers else "generate_rag",
            },
        )

        # retrieve â†’ é€‰æ‹©ç”Ÿæˆå™¨
        builder.add_conditional_edges(
            "retrieve",
            lambda state: self._route_to_kb_or_rag(state),
            {
                "generate_kb": "generate_kb",
                "generate_rag": "generate_rag",
            },
        )

        # generate_kb / generate_rag â†’ persist
        builder.add_edge("generate_kb", "persist")
        builder.add_edge("generate_rag", "persist")

        # persist â†’ end
        builder.add_edge("persist", END)

        # ==================== ç¼–è¯‘ ====================
        return builder.compile()


# ä½¿ç”¨ç¤ºä¾‹
def create_conversation_graph(
    services: ServiceContainer,  # åŒ…å«æ‰€æœ‰ä¾èµ–
) -> CompiledGraph:
    """å·¥å‚å‡½æ•°ï¼šåˆ›å»ºå¯¹è¯å›¾"""
    builder = ConversationGraphBuilder(
        router=services.router,
        conversation_store=services.conversation_store,
        memory_service=services.memory_service,
        summarizer=services.summarizer,
        episodic_memory_manager=services.episodic_memory_manager,
        rag_executor=services.rag_executor,
        llm=services.llm,
        kb_handler_factory=services.kb_handler_factory,
        enable_kb_handlers=services.config.ENABLE_KB_HANDLERS,
    )
    return builder.build()
```

#### 2.3.5 æµå¼å¤„ç†é›†æˆ

**æ”¹é€  HTTP å±‚ä½¿ç”¨ LangGraph**ï¼š

```python
# æ–‡ä»¶: server/api/rest/v1/chat_stream.py
from fastapi import APIRouter, HTTPException
from fastapi.responses import StreamingResponse
import json
from application.chat.schema import ChatRequest

router = APIRouter()

@router.post("/api/v1/chat/stream")
async def stream_chat(request: ChatRequest):
    """
    ä½¿ç”¨ LangGraph çš„æµå¼èŠå¤©ç«¯ç‚¹

    è¿ç§»æ¸…å•ï¼š
    âœ… åŸæœ‰ StreamHandler.handle() é€»è¾‘ â†’ LangGraph èŠ‚ç‚¹åŒ–
    âœ… å‚æ•°ä¸å†é€å±‚ä¼ é€’ â†’ ç»Ÿä¸€ State
    âœ… æ”¯æŒ debug æ¨¡å¼ â†’ execution_logs
    """

    try:
        # 1. è·å–æˆ–åˆ›å»ºå¯¹è¯
        conversation_id = await conversation_store.get_or_create_conversation_id(
            user_id=request.user_id,
            session_id=request.session_id,
        )

        # 2. ä¿å­˜ç”¨æˆ·æ¶ˆæ¯
        await conversation_store.append_message(
            conversation_id=conversation_id,
            role="user",
            content=request.message,
        )

        # 3. åˆå§‹åŒ– Stateï¼ˆå¯¹åº”åŸæœ‰çš„ initial_stateï¼‰
        initial_state = ConversationState(
            # è¯·æ±‚ä¿¡æ¯
            user_id=request.user_id,
            message=request.message,
            session_id=request.session_id,
            conversation_id=conversation_id,
            # é…ç½®
            debug=request.debug,
            agent_type=request.agent_type,
            requested_kb_prefix=request.kb_prefix,
            # åˆå§‹åŒ–å…¶ä»–å­—æ®µ
            kb_prefix=None,
            worker_name=None,
            use_retrieval=False,
            history=[],
            memory_context=None,
            conversation_summary=None,
            episodic_memory=None,
            retrieval_results=None,
            messages=[HumanMessage(content=request.message)],
            execution_logs=[],
            error=None,
            partial_answer=False,
        )

        # 4. æµå¼æ‰§è¡Œå›¾
        async def event_generator():
            tokens = []
            completed = False

            async for event in conversation_graph.astream(
                initial_state,
                stream_mode="updates",  # æ¯ä¸ªèŠ‚ç‚¹æ‰§è¡Œåäº§ç”Ÿä¸€ä¸ªäº‹ä»¶
            ):
                # event æ ¼å¼: {node_name: {updated_state_fields}}
                node_name = list(event.keys())[0]
                node_state = event[node_name]

                # ç‰¹æ®Šå¤„ç†ï¼šæµå¼æ–‡æœ¬æ¥è‡ª generate èŠ‚ç‚¹
                if node_name in ["generate_kb", "generate_rag"]:
                    if "messages" in node_state:
                        message_content = node_state["messages"][-1].content
                        # æ¨¡æ‹Ÿæµå¼è¾“å‡ºï¼ˆå®é™… LLM åº”è¯¥å·²ç»æµå¼è¿”å›ï¼‰
                        for char in message_content:
                            tokens.append(char)
                            yield {
                                "status": "token",
                                "content": char,
                            }
                        completed = True

                # Debug æ¨¡å¼ï¼šè¾“å‡ºæ‰§è¡Œæ—¥å¿—
                if node_state.get("execution_logs"):
                    for log in node_state["execution_logs"]:
                        if log not in (initial_state.get("execution_logs") or []):
                            yield {
                                "status": "execution_log",
                                "content": log,
                            }

                # è·¯ç”±å†³ç­–ï¼šè¾“å‡ºç»™å®¢æˆ·ç«¯
                if node_name == "route" and "route_decision" in node_state:
                    yield {
                        "status": "route_decision",
                        "content": node_state["route_decision"],
                    }

            # æµå¼ç»“æŸ
            answer = "".join(tokens).strip()
            yield {
                "status": "done",
                "content": {
                    "answer": answer,
                    "conversation_id": conversation_id,
                },
            }

        # 5. è¿”å› SSE å“åº”
        return StreamingResponse(
            event_generator(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "X-Accel-Buffering": "no",
            },
        )

    except Exception as e:
        logger.error(f"Chat stream failed: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))
```

**ä¸åŸæœ‰ä»£ç çš„å¯¹åº”å…³ç³»**ï¼š

| åŸæœ‰ä»£ç  | LangGraph æ–¹æ¡ˆ | ä¼˜åŠ¿ |
|---------|--------------|------|
| `StreamHandler.handle()` ç¬¬ 65-261 è¡Œ | `conversation_graph.astream()` | æµç¨‹æ¸…æ™°ï¼Œæ˜“äºç†è§£ |
| æ‰‹åŠ¨ä¼ é€’ `memory_context, history` | `ConversationState` | æ–°å¢å‚æ•°æ— éœ€æ”¹å‡½æ•°ç­¾å |
| `kb_handler.process_stream()` + `executor.stream()` | æ¡ä»¶è¾¹è‡ªåŠ¨é€‰æ‹© | ä¸éœ€è¦å¤šä¸ªå¤„ç†å™¨åˆ†æ”¯ |
| æ‰‹åŠ¨è¿½åŠ æ¶ˆæ¯ | `Annotated[list, add]` | è‡ªåŠ¨åˆå¹¶æ¶ˆæ¯ |

#### 2.3.6 ä¸€æ¬¡æ€§é‡æ„æ–¹æ¡ˆ

**å®Œæ•´çš„æ–‡ä»¶è¿ç§»æ¸…å•**ï¼š

##### Step 1ï¼šåˆ›å»ºæ–°çš„çŠ¶æ€å’Œå›¾æ–‡ä»¶ï¼ˆæ–°å»ºï¼‰

```
æ–°å»ºæ–‡ä»¶ï¼š
âœ… backend/infrastructure/chat/conversation_state.py
âœ… backend/infrastructure/chat/conversation_nodes.py
âœ… backend/infrastructure/chat/conversation_graph.py
âœ… backend/application/chat/handlers/langgraph_stream_handler.py
```

##### Step 2ï¼šåˆ é™¤æˆ–å¼ƒç”¨çš„æ–‡ä»¶

```
å¼ƒç”¨ï¼ˆä½†ä¿ç•™ä»¥é˜²å›æ»šï¼‰ï¼š
âš ï¸  backend/application/chat/handlers/stream_handler.py
âš ï¸  backend/application/chat/handlers/chat_handler.py

å®Œå…¨åˆ é™¤ï¼ˆå…¶é€»è¾‘å·²è¿ç§»åˆ°èŠ‚ç‚¹ä¸­ï¼‰ï¼š
âŒ backend/application/chat/completion.pyï¼ˆPrompt æ„å»ºé€»è¾‘ï¼‰
```

##### Step 3ï¼šä¿®æ”¹çš„æ–‡ä»¶

```
éœ€è¦ä¿®æ”¹ï¼š
ğŸ“ backend/server/api/rest/v1/chat_stream.py
   - æ›¿æ¢ StreamHandler â†’ LangGraphStreamHandler

ğŸ“ backend/server/api/rest/v1/chat.py
   - æ›¿æ¢ ChatHandler â†’ LangGraphStreamHandlerï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰

ğŸ“ backend/infrastructure/config.py
   - æ·»åŠ  LangGraph ç›¸å…³é…ç½®

ğŸ“ backend/infrastructure/di/container.py
   - æ³¨å†Œ conversation_graph ä¾èµ–
   - åˆ é™¤ StreamHandler / ChatHandler æ³¨å†Œ
```

##### Step 4ï¼šè¯¦ç»†è¿ç§»æ­¥éª¤

**ç¬¬ 1 å¤©ï¼šåŸºç¡€è®¾æ–½æ­å»º**

```python
# 1. åˆ›å»º conversation_state.py
# åŒ…å«å®Œæ•´çš„ ConversationState TypedDictï¼ˆå‚è€ƒ 2.3.2ï¼‰

# 2. åˆ›å»º conversation_nodes.py
# åŒ…å«æ‰€æœ‰ 5 ä¸ªèŠ‚ç‚¹çš„å®ç°ï¼š
# - route_node()
# - history_node()
# - retrieve_node()
# - generate_node()
# - persist_node()
```

**ç¬¬ 2 å¤©ï¼šå›¾æ„å»º**

```python
# 1. åˆ›å»º conversation_graph.py
# åŒ…å« ConversationGraphBuilder ç±»å’Œ create_conversation_graph() å·¥å‚

# 2. åœ¨ DI å®¹å™¨ä¸­æ³¨å†Œ
# container.py:
#   graph = create_conversation_graph(services)
```

**ç¬¬ 3 å¤©ï¼šHTTP å±‚è¿ç§»**

```python
# 1. åˆ›å»º langgraph_stream_handler.py
# - å®ç°æ–°çš„ stream_chat() ç«¯ç‚¹
# - ä¿æŒä¸æ—§ API å®Œå…¨å…¼å®¹

# 2. ä¿®æ”¹ chat_stream.py
# - æ›¿æ¢å¯¼å…¥ï¼šfrom StreamHandler â†’ from LangGraphStreamHandler
# - æˆ–ç›´æ¥ä½¿ç”¨ conversation_graph.astream()
```

**ç¬¬ 4 å¤©ï¼šæµ‹è¯•å’ŒéªŒè¯**

```python
# 1. å•å…ƒæµ‹è¯•
# - æ¯ä¸ªèŠ‚ç‚¹ç‹¬ç«‹æµ‹è¯•
# - çŠ¶æ€è½¬ç§»æµ‹è¯•
# - é”™è¯¯å¤„ç†æµ‹è¯•

# 2. é›†æˆæµ‹è¯•
# - å®Œæ•´å¯¹è¯æµç¨‹æµ‹è¯•
# - å¯¹æ¯”æ–°æ—§å®ç°çš„è¾“å‡º

# 3. æ€§èƒ½æµ‹è¯•
# - å»¶è¿Ÿå¯¹æ¯”
# - å†…å­˜å ç”¨å¯¹æ¯”
```

**ç¬¬ 5 å¤©ï¼šéƒ¨ç½²å’Œå›æ»š**

```bash
# éƒ¨ç½²å‰ï¼šå¤‡ä»½æ—§ä»£ç 
git tag backup/stream-handler-v1

# éƒ¨ç½²ï¼šä¸€æ¬¡æ€§æ›¿æ¢æ‰€æœ‰ç«¯ç‚¹
DEPLOY_VERSION=langgraph

# éƒ¨ç½²åï¼šç›‘æ§
- å¯¹è¯æˆåŠŸç‡
- å“åº”å»¶è¿Ÿ
- é”™è¯¯ç‡
- æ—¥å¿—å¼‚å¸¸

# å¦‚é‡é—®é¢˜ï¼šå¿«é€Ÿå›æ»š
git revert <commit-hash>
```

##### Step 5ï¼šå…³é”®å®ç°ç»†èŠ‚

**ä¿æŒä¸æ—§ API å®Œå…¨å…¼å®¹**ï¼š

```python
# åŸæœ‰çš„ HTTP è¯·æ±‚å®Œå…¨ä¸å˜
POST /api/v1/chat/stream {
    "user_id": "...",
    "message": "...",
    "session_id": "...",
    "kb_prefix": "...",
    "debug": false,
    "agent_type": "hybrid_agent"
}

# å“åº”æ ¼å¼å®Œå…¨ç›¸åŒ
{
    "status": "token",
    "content": "æ–‡"
}
{
    "status": "done",
    "content": {"answer": "..."}
}
```

**ä¾èµ–æ³¨å…¥æ”¹é€ ï¼ˆæœ€å°åŒ–ï¼‰**ï¼š

```python
# åŸæœ‰ï¼š
handler = StreamHandler(
    router=router,
    executor=executor,
    conversation_store=conversation_store,
    memory_service=memory_service,
    kb_handler_factory=kb_handler_factory,
)

# æ–°æ–¹æ¡ˆï¼š
graph = create_conversation_graph(services)  # ä¸€è¡Œä»£ç 

# åœ¨ HTTP å±‚è°ƒç”¨
async for event in graph.astream(initial_state):
    yield event
```

##### Step 6ï¼šä¸å¯é€†ç‚¹ï¼ˆç¡®ä¿ä¸€æ¬¡æ€§æˆåŠŸï¼‰

| æ£€æŸ¥é¡¹ | è¯´æ˜ |
|-------|------|
| æµ‹è¯•è¦†ç›–ç‡ | æ‰€æœ‰èŠ‚ç‚¹å•å…ƒæµ‹è¯• â‰¥ 90% |
| é›†æˆæµ‹è¯• | è‡³å°‘ 50 ä¸ªçœŸå®å¯¹è¯åœºæ™¯éªŒè¯ |
| æ€§èƒ½åŸºçº¿ | å»¶è¿Ÿä¸è¶…è¿‡åŸæœ‰ Â±5% |
| é”™è¯¯å¤„ç† | æ‰€æœ‰å¼‚å¸¸éƒ½æœ‰é™çº§æ–¹æ¡ˆ |
| å›æ»šè„šæœ¬ | æµ‹è¯•è¿‡å¿«é€Ÿå›æ»šæµç¨‹ |

##### Step 7ï¼šéƒ¨ç½²å‘½ä»¤

```bash
# 1. æ„å»ºæ–°é•œåƒï¼ˆåŒ…å«æ‰€æœ‰ LangGraph ä»£ç ï¼‰
docker build -t movie-agent:langgraph-v1 .

# 2. è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶
pytest tests/ -v --cov=backend

# 3. æ€§èƒ½åŸºå‡†æµ‹è¯•
python benchmarks/test_performance.py

# 4. éƒ¨ç½²åˆ°é¢„å‘å¸ƒç¯å¢ƒï¼ˆ10% æµé‡ï¼‰
kubectl set image deployment/movie-agent \
  movie-agent=movie-agent:langgraph-v1 \
  --record \
  --namespace=staging

# 5. éªŒè¯é¢„å‘å¸ƒï¼ˆ30 åˆ†é’Ÿï¼‰
- ç›‘æ§é”™è¯¯ç‡
- æ£€æŸ¥æ—¥å¿—
- æ‰‹å·¥æµ‹è¯•å‡ ä¸ªå¯¹è¯

# 6. å…¨é‡ä¸Šçº¿ï¼ˆ100% æµé‡ï¼‰
kubectl set image deployment/movie-agent \
  movie-agent=movie-agent:langgraph-v1 \
  --record \
  --namespace=production

# 7. æŒç»­ç›‘æ§ï¼ˆ1 å°æ—¶ï¼‰
- å…³é”®æŒ‡æ ‡ç›‘æ§
- å‘Šè­¦é…ç½®
- å‡†å¤‡å›æ»šæ–¹æ¡ˆ
```

##### Step 8ï¼šå¿«é€Ÿå›æ»šæ–¹æ¡ˆ

```bash
# ä¸‡ä¸€å‡ºç°é—®é¢˜ï¼Œæ‰§è¡Œå›æ»šï¼ˆâ‰¤ 5 åˆ†é’Ÿï¼‰
kubectl rollout undo deployment/movie-agent \
  --namespace=production

# éªŒè¯å›æ»š
kubectl get pods -n production
curl http://api.service:8000/api/v1/health  # å¥åº·æ£€æŸ¥

# åˆ†æé—®é¢˜ï¼ˆPost-mortemï¼‰
- æŸ¥çœ‹æ–°æ—§ç‰ˆæœ¬çš„æ—¥å¿—å·®å¼‚
- å¯¹æ¯”ä¸¤ä¸ªç‰ˆæœ¬çš„æ€§èƒ½æŒ‡æ ‡
- ç¡®ä¿æ²¡æœ‰æ•°æ®æŸå
```

#### 2.3.7 æœ€ç»ˆçš„ä¼˜åŠ¿æ€»ç»“

| ç»´åº¦ | ç°æœ‰æ¶æ„ | Phase 3 (LangGraph) |
|------|---------|------------------|
| **å‚æ•°ä¼ é€’** | é€å±‚æ‰‹åŠ¨ä¼ é€’ï¼ˆ6+å±‚ï¼‰ | ç»Ÿä¸€ Stateï¼ˆé›¶å±‚ï¼‰ |
| **æ–°å¢å‚æ•°** | ä¿®æ”¹ 5-7 ä¸ªå‡½æ•°ç­¾å | ä¿®æ”¹ 1 ä¸ª TypedDict |
| **æ‰©å±•æ€§** | ä½ï¼Œæ”¹ä¸€ä¸ªå‚æ•°å½±å“å¤šå¤„ | é«˜ï¼ŒåŠ å­—æ®µå°±è¡Œ |
| **å¯æµ‹è¯•æ€§** | é›†æˆæµ‹è¯• | å•å…ƒæµ‹è¯•ï¼ˆæ¯ä¸ªèŠ‚ç‚¹ï¼‰ |
| **å¯è§‚æµ‹æ€§** | æ‰‹åŠ¨åŸ‹ç‚¹ + Langfuse | å†…ç½® execution_logs + Langfuse |
| **é”™è¯¯å¤„ç†** | æ¯å±‚éƒ½è¦ try-except | ç»Ÿä¸€åœ¨èŠ‚ç‚¹ä¸­å¤„ç† |
| **æµå¼å¤„ç†** | æ‰‹åŠ¨è¿½è¸ª tokens | è‡ªåŠ¨åˆå¹¶æ¶ˆæ¯ |

#### 2.3.8 Phase 3 æµ‹è¯•æ–¹æ¡ˆ

**å•å…ƒæµ‹è¯•ï¼ˆæ¯ä¸ªèŠ‚ç‚¹ç‹¬ç«‹æµ‹è¯•ï¼‰**ï¼š

```python
# tests/infrastructure/chat/test_conversation_nodes.py
import pytest
from unittest.mock import AsyncMock, MagicMock
from infrastructure.chat.conversation_nodes import (
    route_node, history_node, retrieve_node, generate_node, persist_node
)
from infrastructure.chat.conversation_state import ConversationState

class TestRouteNode:
    @pytest.mark.asyncio
    async def test_route_node_returns_kb_prefix(self):
        """æµ‹è¯• route_node æ­£ç¡®è¿”å› KB"""
        mock_router = AsyncMock()
        mock_router.route.return_value = RouteDecision(
            kb_prefix="movie",
            worker_name="movie:hybrid_agent",
            confidence=0.95,
            method="auto",
            reason="User query matched movie KB",
        )

        state = ConversationState(
            message="æ¨èç”µå½±",
            session_id="test-session",
            ...
        )

        result = await route_node(state, router=mock_router)

        assert result["kb_prefix"] == "movie"
        assert result["use_retrieval"] is True

    @pytest.mark.asyncio
    async def test_route_node_general_kb_no_retrieval(self):
        """æµ‹è¯• general KB ä¸éœ€è¦æ£€ç´¢"""
        mock_router = AsyncMock()
        mock_router.route.return_value = RouteDecision(
            kb_prefix="general",
            worker_name="general:naive_agent",
            ...
        )

        result = await route_node(state, router=mock_router)
        assert result["use_retrieval"] is False


class TestHistoryNode:
    @pytest.mark.asyncio
    async def test_history_node_retrieves_messages(self):
        """æµ‹è¯• history_node æ­£ç¡®è·å–å†å²æ¶ˆæ¯"""
        mock_store = AsyncMock()
        mock_store.list_messages.return_value = [
            {"role": "user", "content": "ä»€ä¹ˆæ˜¯æ¨èç³»ç»Ÿ"},
            {"role": "assistant", "content": "æ¨èç³»ç»Ÿæ˜¯..."},
            {"role": "user", "content": "èƒ½ä¸¾ä¾‹å—"},
        ]

        state = ConversationState(conversation_id="conv-1", message="èƒ½ä¸¾ä¾‹å—", ...)
        result = await history_node(state, conversation_store=mock_store)

        assert len(result["history"]) == 2  # å»é‡å½“å‰æ¶ˆæ¯
        assert result["history"][0]["content"] == "ä»€ä¹ˆæ˜¯æ¨èç³»ç»Ÿ"

    @pytest.mark.asyncio
    async def test_history_node_with_summary(self):
        """æµ‹è¯• history_node è·å–æ‘˜è¦"""
        mock_store = AsyncMock()
        mock_store.list_messages.return_value = []

        mock_summarizer = AsyncMock()
        mock_summarizer.get_or_generate_summary.return_value = "ç”¨æˆ·è¯¢é—®äº†å…³äºç”µå½±æ¨èçš„é—®é¢˜"

        result = await history_node(state, conversation_store=mock_store, summarizer=mock_summarizer)

        assert result["conversation_summary"] is not None

    @pytest.mark.asyncio
    async def test_history_node_error_resilience(self):
        """æµ‹è¯• history_node åœ¨é”™è¯¯æ—¶çš„é™çº§"""
        mock_store = AsyncMock()
        mock_store.list_messages.return_value = []

        mock_summarizer = AsyncMock()
        mock_summarizer.get_or_generate_summary.side_effect = Exception("Summarizer failed")

        result = await history_node(state, conversation_store=mock_store, summarizer=mock_summarizer)

        # æ‘˜è¦å¤±è´¥ä¸åº”è¯¥é˜»å¡æ•´ä¸ªæµç¨‹
        assert result["conversation_summary"] is None
        assert "history" in result


class TestGenerateNode:
    @pytest.mark.asyncio
    async def test_generate_node_creates_message(self):
        """æµ‹è¯• generate_node ç”Ÿæˆ AIMessage"""
        mock_llm = AsyncMock()
        mock_llm.ainvoke.return_value = MagicMock(content="è¿™æ˜¯ä¸€ä¸ªæ¨èç»“æœ")

        state = ConversationState(
            message="æ¨èç”µå½±",
            history=[],
            memory_context=None,
            ...
        )

        result = await generate_node(state, llm=mock_llm)

        assert "messages" in result
        assert len(result["messages"]) > 0
        assert result["messages"][0].content == "è¿™æ˜¯ä¸€ä¸ªæ¨èç»“æœ"

    @pytest.mark.asyncio
    async def test_generate_node_with_context(self):
        """æµ‹è¯• generate_node åŒ…å«æ‰€æœ‰ä¸Šä¸‹æ–‡"""
        mock_llm = AsyncMock()

        state = ConversationState(
            message="æ¨èç”µå½±",
            history=[{"role": "user", "content": "å–œæ¬¢ç§‘å¹»"}],
            memory_context="ç”¨æˆ·å–œæ¬¢è¯ºå…°çš„ç”µå½±",
            conversation_summary="ç”¨æˆ·è¯¢é—®ç”µå½±æ¨è",
            retrieval_results=[{"content": "ã€Šæ˜Ÿé™…ç©¿è¶Šã€‹æ˜¯è¯ºå…°çš„ä»£è¡¨ä½œ"}],
            ...
        )

        result = await generate_node(state, llm=mock_llm)

        # éªŒè¯ LLM è¢«æ­£ç¡®è°ƒç”¨ï¼ˆåŒ…å«æ‰€æœ‰ä¸Šä¸‹æ–‡ï¼‰
        assert mock_llm.ainvoke.called
        call_args = mock_llm.ainvoke.call_args
        prompt = call_args[0][0]  # ç¬¬ä¸€ä¸ªä½ç½®å‚æ•°æ˜¯ prompt

        # æ£€æŸ¥ Prompt åŒ…å«æ‰€æœ‰å¿…è¦ä¿¡æ¯
        assert "memory_context" not in prompt.format()  # å·²è¢« inject
        # å®é™…ä¸Šåº”è¯¥æ£€æŸ¥ prompt çš„å†…å®¹ï¼Œè¿™é‡Œç®€åŒ–äº†

class TestPersistNode:
    @pytest.mark.asyncio
    async def test_persist_node_saves_message(self):
        """æµ‹è¯• persist_node ä¿å­˜æ¶ˆæ¯"""
        mock_store = AsyncMock()

        state = ConversationState(
            conversation_id="conv-1",
            messages=[
                HumanMessage(content="æ¨èç”µå½±"),
                AIMessage(content="æ¨èã€Šæ˜Ÿé™…ç©¿è¶Šã€‹"),
            ],
            ...
        )

        result = await persist_node(state, conversation_store=mock_store)

        # éªŒè¯æ¶ˆæ¯è¢«ä¿å­˜
        mock_store.append_message.assert_called_once()
        call_args = mock_store.append_message.call_args
        assert call_args[1]["content"] == "æ¨èã€Šæ˜Ÿé™…ç©¿è¶Šã€‹"
```

**é›†æˆæµ‹è¯•ï¼ˆå®Œæ•´æµç¨‹ï¼‰**ï¼š

```python
# tests/infrastructure/chat/test_conversation_graph.py
import pytest
from infrastructure.chat.conversation_graph import create_conversation_graph
from infrastructure.chat.conversation_state import ConversationState
from langchain_core.messages import HumanMessage

class TestConversationGraph:
    @pytest.fixture
    async def graph(self, services_mock):
        """åˆ›å»ºæµ‹è¯•ç”¨çš„å›¾"""
        return create_conversation_graph(services_mock)

    @pytest.mark.asyncio
    async def test_complete_conversation_flow(self, graph):
        """æµ‹è¯•å®Œæ•´çš„å¯¹è¯æµç¨‹"""
        initial_state = ConversationState(
            user_id="test-user",
            message="æ¨èç§‘å¹»ç”µå½±",
            session_id="test-session",
            conversation_id="test-conv",
            debug=True,
            agent_type="hybrid_agent",
            requested_kb_prefix="movie",
            # å…¶ä»–å­—æ®µåˆå§‹åŒ–...
        )

        # æ‰§è¡Œå›¾
        final_state = None
        async for event in graph.astream(initial_state):
            final_state = event

        # éªŒè¯ç»“æœ
        assert final_state is not None
        assert "messages" in final_state
        assert len(final_state["messages"]) >= 2  # User + Assistant
        assert final_state["messages"][-1].type == "ai"

    @pytest.mark.asyncio
    async def test_graph_with_kb_handler(self, graph):
        """æµ‹è¯•ä½¿ç”¨ KB Handler çš„æµç¨‹"""
        # æµ‹è¯•æ¡ä»¶è¾¹ï¼šåº”è¯¥èµ° KB Handler
        # éªŒè¯ generate_kb_node è¢«æ‰§è¡Œ
        pass

    @pytest.mark.asyncio
    async def test_graph_error_handling(self, graph):
        """æµ‹è¯•é”™è¯¯å¤„ç†å’Œé™çº§"""
        # æ³¨å…¥é”™è¯¯ï¼šmemory_service å¤±è´¥
        # éªŒè¯æµç¨‹ç»§ç»­å¹¶å®Œæˆ
        pass

    @pytest.mark.asyncio
    async def test_graph_with_debug_mode(self, graph):
        """æµ‹è¯• debug æ¨¡å¼"""
        initial_state = ConversationState(..., debug=True)

        async for event in graph.astream(initial_state):
            pass

        # éªŒè¯ execution_logs è¢«å¡«å……
        # å„ä¸ªèŠ‚ç‚¹éƒ½è¾“å‡ºäº†æ—¥å¿—
```

**æ€§èƒ½æµ‹è¯•ï¼ˆåŸºå‡†å¯¹æ¯”ï¼‰**ï¼š

```python
# benchmarks/test_phase3_performance.py
import asyncio
import time
import statistics

@pytest.mark.benchmark
class TestConversationGraphPerformance:

    async def measure_single_conversation(self, graph, message: str) -> float:
        """æµ‹é‡å•ä¸ªå¯¹è¯çš„å»¶è¿Ÿ"""
        initial_state = ConversationState(
            user_id="perf-test",
            message=message,
            conversation_id="perf-conv",
            ...
        )

        t0 = time.perf_counter()
        async for event in graph.astream(initial_state):
            pass
        t1 = time.perf_counter()

        return (t1 - t0) * 1000  # æ¯«ç§’

    @pytest.mark.asyncio
    async def test_baseline_latency(self, graph):
        """æµ‹è¯•åŸºçº¿å»¶è¿Ÿ"""
        messages = [
            "æ¨èç”µå½±",
            "è¿™éƒ¨ç”µå½±æ€ä¹ˆæ ·",
            "è¿˜æœ‰å…¶ä»–æ¨èå—",
        ]

        times = []
        for msg in messages:
            latency_ms = await self.measure_single_conversation(graph, msg)
            times.append(latency_ms)

        avg_latency = statistics.mean(times)
        max_latency = max(times)

        print(f"å¹³å‡å»¶è¿Ÿ: {avg_latency:.2f}ms")
        print(f"æœ€å¤§å»¶è¿Ÿ: {max_latency:.2f}ms")

        # åº”è¯¥ < 500ms
        assert avg_latency < 500, f"Average latency {avg_latency}ms exceeds 500ms"
        assert max_latency < 1000, f"Max latency {max_latency}ms exceeds 1000ms"

    @pytest.mark.asyncio
    async def test_concurrent_conversations(self, graph):
        """æµ‹è¯•å¹¶å‘å¯¹è¯"""
        async def run_conversation(graph, msg: str):
            return await self.measure_single_conversation(graph, msg)

        # 10 ä¸ªå¹¶å‘å¯¹è¯
        tasks = [
            run_conversation(graph, f"æ¶ˆæ¯{i}")
            for i in range(10)
        ]

        times = await asyncio.gather(*tasks)

        avg_latency = statistics.mean(times)
        assert avg_latency < 600, "Concurrent latency too high"

    @pytest.mark.asyncio
    async def test_memory_usage(self, graph):
        """æµ‹è¯•å†…å­˜å ç”¨"""
        import tracemalloc

        tracemalloc.start()

        # è¿è¡Œ 100 ä¸ªå¯¹è¯
        for i in range(100):
            initial_state = ConversationState(...)
            async for event in graph.astream(initial_state):
                pass

        current, peak = tracemalloc.get_traced_memory()
        tracemalloc.stop()

        print(f"å³°å€¼å†…å­˜å ç”¨: {peak / 1024 / 1024:.2f}MB")
        # åº”è¯¥ < 200MB
        assert peak < 200 * 1024 * 1024
```

**æµ‹è¯•å‘½ä»¤**ï¼š

```bash
# è¿è¡Œæ‰€æœ‰ Phase 3 ç›¸å…³æµ‹è¯•
pytest tests/infrastructure/chat/test_conversation_nodes.py -v

pytest tests/infrastructure/chat/test_conversation_graph.py -v

# è¿è¡Œæ€§èƒ½æµ‹è¯•ï¼ˆå¯èƒ½è¾ƒæ…¢ï¼‰
pytest benchmarks/test_phase3_performance.py -v -s

# è¿è¡Œæµ‹è¯•è¦†ç›–ç‡æŠ¥å‘Š
pytest tests/ --cov=infrastructure/chat --cov-report=html

# ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Šï¼ˆæ–°æ—§ç‰ˆæœ¬ï¼‰
pytest tests/ -v --json=test_results_langgraph.json
# ä¸æ—§ç‰ˆæœ¬å¯¹æ¯”
diff <(pytest tests/ -v --json=test_results_legacy.json) test_results_langgraph.json
```

---

## 3. å®æ–½è·¯çº¿å›¾

| é˜¶æ®µ | å·¥ä½œé‡ï¼ˆäººå¤©ï¼‰ | é£é™© | ä¼˜å…ˆçº§ |
|------|---------------|------|--------|
| **Phase 1: æ‘˜è¦** | 3-5 | ä½ | **P0** |
| **Phase 2: è¯­ä¹‰æ£€ç´¢** | 5-7 | ä¸­ | P1 |
| **Phase 3: LangGraph** | 10-15 | é«˜ | P2 |

### 3.1 Phase 1 è¯¦ç»†å®æ–½æ­¥éª¤

#### Step 1: æ•°æ®åº“è¿ç§»
```python
# 1. åˆ›å»ºæ‘˜è¦è¡¨
async def init_summary_table():
    async with get_db_pool() as pool:
        async with pool.acquire() as conn:
            await conn.execute("""
            CREATE TABLE IF NOT EXISTS conversation_summaries (
                id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
                conversation_id UUID NOT NULL UNIQUE REFERENCES conversations(id) ON DELETE CASCADE,
                summary TEXT NOT NULL,
                covered_message_count INT NOT NULL DEFAULT 0,
                created_at TIMESTAMP DEFAULT NOW(),
                updated_at TIMESTAMP DEFAULT NOW()
            )
            """)
            # ç´¢å¼•ç”¨äºé¢‘ç¹æŸ¥è¯¢
            await conn.execute("""
            CREATE INDEX IF NOT EXISTS idx_conversation_summaries_conversation_id
            ON conversation_summaries(conversation_id)
            """)

# 2. åœ¨ ChatHandler åˆå§‹åŒ–æ—¶è°ƒç”¨
async def setup_handler(self):
    await init_summary_table()
```

#### Step 2: é›†æˆæ‘˜è¦ç”Ÿæˆ
```python
# åœ¨ conversation_store.py ä¸­æ·»åŠ æ‘˜è¦æ–¹æ³•
class ConversationStore:
    async def get_summary(self, conversation_id: str) -> dict | None:
        """è·å–å·²å­˜åœ¨çš„æ‘˜è¦"""
        result = await self.db.fetchrow(
            "SELECT * FROM conversation_summaries WHERE conversation_id = $1",
            conversation_id
        )
        return result

    async def save_summary(self, conversation_id: str, summary: str, covered_count: int):
        """ä¿å­˜æˆ–æ›´æ–°æ‘˜è¦"""
        await self.db.execute("""
        INSERT INTO conversation_summaries (conversation_id, summary, covered_message_count)
        VALUES ($1, $2, $3)
        ON CONFLICT(conversation_id) DO UPDATE SET
            summary = EXCLUDED.summary,
            covered_message_count = EXCLUDED.covered_message_count,
            updated_at = NOW()
        """, conversation_id, summary, covered_count)
```

#### Step 3: é›†æˆåˆ° ChatHandler
```python
# åœ¨ chat_handler.py ä¸­ä¿®æ”¹ _get_context æ–¹æ³•
async def _get_context(self, conversation_id: str) -> dict:
    # 1. åˆ¤æ–­æ˜¯å¦éœ€è¦ç”Ÿæˆæ‘˜è¦
    messages_count = await self._conversation_store.count_messages(conversation_id)
    existing_summary = await self._conversation_store.get_summary(conversation_id)

    summary = None
    if messages_count >= 10:
        if existing_summary is None or (messages_count - existing_summary["covered_message_count"]) >= 5:
            # ç”Ÿæˆæ–°æ‘˜è¦
            all_messages = await self._conversation_store.list_messages(
                conversation_id, limit=None  # è·å–æ‰€æœ‰æ¶ˆæ¯
            )
            to_summarize = all_messages[:-6]  # ä¿ç•™æœ€å 6 æ¡

            if to_summarize:
                try:
                    summary_text = await self._generate_summary(to_summarize)
                    await self._conversation_store.save_summary(
                        conversation_id,
                        summary_text,
                        len(to_summarize)
                    )
                    summary = summary_text
                except Exception as e:
                    logger.error(f"Failed to generate summary: {e}")
                    summary = existing_summary["summary"] if existing_summary else None
            else:
                summary = existing_summary["summary"] if existing_summary else None
        else:
            summary = existing_summary["summary"]

    # 2. è·å–æœ€è¿‘æ¶ˆæ¯
    recent = await self._conversation_store.list_messages(
        conversation_id=conversation_id,
        limit=6,
        desc=True
    )
    recent.reverse()

    return {
        "summary": summary,
        "recent_history": recent
    }

async def _generate_summary(self, messages: list[dict]) -> str:
    """ç”Ÿæˆå¯¹è¯æ‘˜è¦"""
    formatted = "\n".join([
        f"{msg['role'].upper()}: {msg['content']}"
        for msg in messages
    ])

    prompt = ChatPromptTemplate.from_messages([
        ("system", """ä½ æ˜¯ä¸€ä¸ªå¯¹è¯æ‘˜è¦ä¸“å®¶ã€‚è¯·å°†ä»¥ä¸‹å¯¹è¯å†å²å‹ç¼©ä¸º 2-3 å¥è¯çš„æ‘˜è¦ï¼Œçªå‡ºï¼š
1. ç”¨æˆ·çš„æ ¸å¿ƒè¯‰æ±‚å’Œåå¥½
2. å·²è®¨è®ºçš„å…³é”®è¯é¢˜
3. ä»»ä½•é‡è¦çš„èƒŒæ™¯ä¿¡æ¯

ä¿æŒç®€æ´ï¼Œé¿å…å†—ä½™ã€‚"""),
        ("user", formatted)
    ])

    response = await self.llm.ainvoke(prompt)
    return response.content
```

#### Step 4: æ›´æ–° Prompt æ„å»º
```python
# åœ¨ completion.py ä¸­æ›´æ–° build_prompt å‡½æ•°
def _build_general_prompt(
    system_message: str,
    memory_context: str | None = None,
    summary: str | None = None,
    history: list[dict] | None = None,
    question: str = ""
) -> ChatPromptTemplate:
    messages = [("system", system_message)]

    # æ·»åŠ è®°å¿†ä¸Šä¸‹æ–‡
    if memory_context:
        messages.append(("system", f"ã€ç”¨æˆ·é•¿æœŸè®°å¿†ã€‘\n{memory_context}"))

    # æ·»åŠ å¯¹è¯æ‘˜è¦ï¼ˆæ–°å¢ï¼‰
    if summary:
        messages.append(("system", f"ã€å¯¹è¯èƒŒæ™¯ã€‘\n{summary}"))

    # æ·»åŠ è¿‘æœŸå†å²
    if history:
        for msg in history:
            role = "assistant" if msg.get("role") == "assistant" else "human"
            messages.append((role, msg.get("content", "")))

    # å½“å‰é—®é¢˜
    messages.append(("human", question))

    return ChatPromptTemplate.from_messages(messages)
```

#### Step 5: æµ‹è¯•éªŒè¯
```python
# tests/test_summarization.py
import pytest

@pytest.mark.asyncio
async def test_summary_generation():
    """æµ‹è¯•æ‘˜è¦ç”Ÿæˆ"""
    # åˆ›å»ºæµ‹è¯•å¯¹è¯
    messages = [
        {"role": "user", "content": "æ¨èä¸€äº›ç§‘å¹»ç”µå½±"},
        {"role": "assistant", "content": "æˆ‘æ¨èã€Šæ˜Ÿé™…ç©¿è¶Šã€‹ã€ã€Šé»‘å®¢å¸å›½ã€‹..."},
        {"role": "user", "content": "è¿™äº›ç”µå½±æœ‰ä»€ä¹ˆå…±åŒç‚¹ï¼Ÿ"},
        {"role": "assistant", "content": "å®ƒä»¬éƒ½æ¶‰åŠ..."}
    ]

    store = ConversationStore()
    summary = await store._summarizer.generate_summary(messages)

    assert len(summary) > 0
    assert len(summary.split()) < 50  # æ‘˜è¦åº”è¯¥ç®€æ´
    assert "ç§‘å¹»" in summary or "ç”µå½±" in summary

@pytest.mark.asyncio
async def test_summary_caching():
    """æµ‹è¯•æ‘˜è¦ç¼“å­˜"""
    conv_id = "test-conv-123"

    # é¦–æ¬¡ç”Ÿæˆ
    summary1 = await store.get_summary(conv_id)
    assert summary1 is None

    # ä¿å­˜æ‘˜è¦
    await store.save_summary(conv_id, "è¿™æ˜¯æ‘˜è¦", 10)

    # ç¬¬äºŒæ¬¡åº”è¯¥ä»ç¼“å­˜è¯»å–
    summary2 = await store.get_summary(conv_id)
    assert summary2 is not None
    assert summary2["summary"] == "è¿™æ˜¯æ‘˜è¦"
```

### 3.2 Phase 2 è¯¦ç»†å®æ–½æ­¥éª¤

#### Step 1: å‘é‡ç´¢å¼•å»ºç«‹
```python
# åœ¨ episodic_memory.py ä¸­å®ç°
from milvus import Collection, Milvus

class EpisodicMemoryManager:
    def __init__(self, milvus_host: str, embedding_model):
        self.client = Milvus(host=milvus_host)
        self.embedding_model = embedding_model

    async def init_collection(self):
        """åˆå§‹åŒ– Milvus Collection"""
        collection_name = "conversation_episodes"

        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        existing = self.client.has_collection(collection_name)
        if not existing:
            fields = [
                FieldSchema(name="id", dtype=DataType.VARCHAR, is_primary=True, max_length=100),
                FieldSchema(name="conversation_id", dtype=DataType.VARCHAR, max_length=100),
                FieldSchema(name="user_message", dtype=DataType.VARCHAR, max_length=4096),
                FieldSchema(name="assistant_message", dtype=DataType.VARCHAR, max_length=4096),
                FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=1536),
                FieldSchema(name="created_at", dtype=DataType.INT64),
                FieldSchema(name="timestamp", dtype=DataType.INT64),
            ]

            schema = CollectionSchema(fields, primary_field="id")
            self.collection = Collection(
                name=collection_name,
                schema=schema
            )

            # åˆ›å»ºç´¢å¼•ä»¥åŠ é€Ÿæœç´¢
            self.collection.create_index(
                field_name="embedding",
                index_params={
                    "metric_type": "COSINE",
                    "index_type": "IVF_FLAT",
                    "params": {"nlist": 128}
                }
            )

    async def index_episode(self, conversation_id: str, user_msg: str, assistant_msg: str):
        """ç´¢å¼•ä¸€ä¸ªå¯¹è¯ç‰‡æ®µ"""
        try:
            # ç”ŸæˆåµŒå…¥
            combined_text = f"{user_msg} {assistant_msg}"
            embedding = await self.embedding_model.embed_query(combined_text)

            # æ’å…¥åˆ°å‘é‡åº“
            from uuid import uuid4
            import time

            self.collection.insert([{
                "id": str(uuid4()),
                "conversation_id": conversation_id,
                "user_message": user_msg,
                "assistant_message": assistant_msg,
                "embedding": embedding,
                "created_at": int(time.time()),
                "timestamp": int(time.time() * 1000)
            }])

            logger.info(f"Indexed episode for conversation {conversation_id}")
        except Exception as e:
            logger.error(f"Failed to index episode: {e}")
            # ä¸é˜»å¡ä¸»æµç¨‹

    async def recall_relevant(self, conversation_id: str, query: str, top_k: int = 3):
        """å¬å›ç›¸å…³çš„å†å²ç‰‡æ®µ"""
        try:
            # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
            query_embedding = await self.embedding_model.embed_query(query)

            # å‘é‡æœç´¢ï¼ˆé™åˆ¶åœ¨å½“å‰å¯¹è¯ï¼‰
            results = self.collection.search(
                data=[query_embedding],
                anns_field="embedding",
                param={"metric_type": "COSINE", "params": {"nprobe": 10}},
                limit=top_k * 2,  # è·å–æ›´å¤šï¼Œåç»­è¿‡æ»¤
                expr=f"conversation_id == '{conversation_id}'"
            )

            # æ ¼å¼åŒ–ç»“æœ
            episodes = []
            for hit in results[0]:
                # hit åŒ…å« id, distance, entity
                entity = hit.entity
                episodes.append({
                    "user_message": entity.get("user_message"),
                    "assistant_message": entity.get("assistant_message"),
                    "similarity": 1 - hit.distance,  # COSINE è·ç¦»è½¬ç›¸ä¼¼åº¦
                    "timestamp": entity.get("created_at")
                })

            # æŒ‰ç›¸ä¼¼åº¦é™åºæ’åˆ—ï¼Œå– Top K
            episodes.sort(key=lambda x: x["similarity"], reverse=True)
            return episodes[:top_k]
        except Exception as e:
            logger.error(f"Failed to recall episodes: {e}")
            return []
```

#### Step 2: é›†æˆåˆ° ChatHandler
```python
# åœ¨ chat_handler.py ä¸­ä¿®æ”¹ _get_hybrid_history æ–¹æ³•
async def _get_hybrid_history(
    self,
    conversation_id: str,
    current_query: str
) -> list[dict]:
    """æ··åˆæ£€ç´¢ï¼šæ—¶é—´çª—å£ + è¯­ä¹‰çª—å£"""

    # 1. æ—¶é—´çª—å£ï¼šæœ€è¿‘ 6 æ¡æ¶ˆæ¯
    recent = await self._conversation_store.list_messages(
        conversation_id=conversation_id,
        limit=6,
        desc=True
    )
    recent.reverse()

    # 2. è¯­ä¹‰çª—å£ï¼šç›¸å…³çš„å†å²ç‰‡æ®µ
    try:
        relevant = await self._episodic_memory.recall_relevant(
            conversation_id=conversation_id,
            query=current_query,
            top_k=3
        )
    except Exception as e:
        logger.warning(f"Semantic recall failed: {e}, using time-based only")
        relevant = []

    # 3. åˆå¹¶å»é‡
    seen_content = {msg["content"] for msg in recent}
    unique_relevant = []
    for ep in relevant:
        combined = f"{ep.get('user_message', '')} {ep.get('assistant_message', '')}"
        if combined not in seen_content:
            unique_relevant.append({
                "role": "context",
                "content": combined,
                "type": "semantic_recall",
                "similarity": ep.get("similarity", 0)
            })

    # 4. æ’åºï¼šæŒ‰ç›¸ä¼¼åº¦é™åºï¼Œç„¶åæ˜¯æœ€è¿‘æ¶ˆæ¯
    unique_relevant.sort(key=lambda x: x.get("similarity", 0), reverse=True)

    return unique_relevant[:2] + recent  # æœ€å¤š2æ¡è¯­ä¹‰ç‰‡æ®µ + æœ€è¿‘æ¶ˆæ¯
```

#### Step 3: å¼‚æ­¥ç´¢å¼•ï¼ˆé¿å…é˜»å¡ï¼‰
```python
# åœ¨ chat_handler.py çš„ handle æ–¹æ³•ä¸­
async def handle(self, message: str, conversation_id: str, **kwargs):
    # ... ä¸»æµç¨‹ ...
    response = await self.generate_response(message, context)

    # å¼‚æ­¥ç´¢å¼•æ–°å¯¹è¯ç‰‡æ®µï¼ˆä¸é˜»å¡å“åº”ï¼‰
    asyncio.create_task(
        self._episodic_memory.index_episode(
            conversation_id=conversation_id,
            user_msg=message,
            assistant_msg=response
        )
    )

    return response
```

### 3.3 Phase 3 å®æ–½ç­–ç•¥

#### Feature Flag ç°åº¦
```python
# åœ¨ config.py ä¸­
LANGGRAPH_ENABLED = os.getenv("LANGGRAPH_ENABLED", "false").lower() == "true"

# åœ¨ chat_handler.py ä¸­
if LANGGRAPH_ENABLED:
    executor = LangGraphChatExecutor()
else:
    executor = LegacyChatExecutor()
```

#### å…¼å®¹æ€§å±‚
```python
# åˆ›å»ºé€‚é…å±‚ï¼Œé¿å…ä¸€æ¬¡æ€§é‡æ„æ‰€æœ‰è°ƒç”¨
class ChatExecutorAdapter:
    def __init__(self, use_langgraph: bool):
        if use_langgraph:
            self._executor = LangGraphChatExecutor()
        else:
            self._executor = LegacyChatExecutor()

    async def execute(self, message: str, **context) -> str:
        """ç»Ÿä¸€æ¥å£"""
        return await self._executor.execute(message, **context)
```

---

## 4. ä¼˜åŒ–å»ºè®®

### 4.1 æ€§èƒ½ä¼˜åŒ–

#### æ‘˜è¦ç”Ÿæˆä¼˜åŒ–
```python
# 1. ä½¿ç”¨è½»é‡çº§æ¨¡å‹ç”Ÿæˆæ‘˜è¦ï¼ˆé™ä½æˆæœ¬å’Œå»¶è¿Ÿï¼‰
# ç”¨ GPT-3.5-turbo è€Œé GPT-4
class ConversationSummarizer:
    def __init__(self, model_name: str = "gpt-3.5-turbo"):
        self.llm = ChatOpenAI(model_name=model_name, temperature=0.3)

    # 2. ç¼“å­˜å®Œå…¨ç›¸åŒçš„å¯¹è¯æ‘˜è¦è¯·æ±‚
    @lru_cache(maxsize=1000)
    async def generate_summary(self, text_hash: str) -> str:
        # å…ˆè®¡ç®—æ–‡æœ¬å“ˆå¸Œï¼Œé¿å…é‡å¤ç”Ÿæˆ
        pass

# 3. åå°å¼‚æ­¥ç”Ÿæˆï¼Œä¸é˜»å¡ç”¨æˆ·å“åº”
asyncio.create_task(self._generate_summary_async(conv_id))
```

#### å‘é‡æœç´¢ä¼˜åŒ–
```python
# 1. é™åˆ¶æœç´¢èŒƒå›´ï¼ˆæŒ‰æ—¶é—´æˆ³ï¼‰
# åªæœç´¢æœ€è¿‘ 30 å¤©çš„å¯¹è¯ç‰‡æ®µ
recent_timestamp = int((time.time() - 30 * 86400) * 1000)
expr = f"conversation_id == '{conv_id}' AND timestamp > {recent_timestamp}"

# 2. è°ƒæ•´å‘é‡ç´¢å¼•å‚æ•°ä»¥å¹³è¡¡ç²¾åº¦å’Œé€Ÿåº¦
index_params = {
    "metric_type": "COSINE",
    "index_type": "IVF_FLAT",
    "params": {"nlist": 128}  # å¢åŠ  nlist æé«˜ç²¾åº¦ï¼Œé™ä½ nlist æé«˜é€Ÿåº¦
}

# 3. æ‰¹é‡ç´¢å¼•ï¼ˆå‡å°‘ç½‘ç»œå¼€é”€ï¼‰
async def batch_index_episodes(self, episodes: list[dict]):
    embeddings = await self.embedding_model.embed_multiple(
        [f"{e['user']} {e['assistant']}" for e in episodes]
    )
    self.collection.insert([{
        "id": ep["id"],
        "conversation_id": ep["conversation_id"],
        "embedding": emb,
        ...
    } for ep, emb in zip(episodes, embeddings)])
```

### 4.2 å¯é æ€§ä¿è¯

#### é™çº§ç­–ç•¥
```python
# å½“æ‘˜è¦ç”Ÿæˆå¤±è´¥æ—¶
async def get_or_generate_summary(self, conversation_id: str) -> str | None:
    try:
        existing = await self._store.get_summary(conversation_id)
        if existing:
            return existing["summary"]

        # å°è¯•ç”Ÿæˆæ–°æ‘˜è¦
        return await self._generate_summary(conversation_id)
    except Exception as e:
        logger.error(f"Summary generation failed: {e}")
        # é™çº§ï¼šè¿”å› Noneï¼Œå‰ç«¯ä½¿ç”¨æ—¶é—´çª—å£å³å¯
        return None

# å½“å‘é‡æœç´¢å¤±è´¥æ—¶
async def recall_relevant(self, conversation_id: str, query: str, top_k: int = 3):
    try:
        return await self._search_vector(conversation_id, query, top_k)
    except Exception as e:
        logger.warning(f"Vector search failed: {e}, falling back to time-based")
        # é™çº§ï¼šè¿”å›æ—¶é—´åºåˆ—çš„æœ€å K æ¡
        return await self._store.list_messages(conversation_id, limit=top_k)
```

#### ç›‘æ§å’Œå‘Šè­¦
```python
# åœ¨å…³é”®ç‚¹æ·»åŠ ç›‘æ§
from prometheus_client import Counter, Histogram

summary_generation_time = Histogram('summary_generation_seconds', 'Time to generate summary')
summary_generation_errors = Counter('summary_generation_errors_total', 'Total summary generation errors')
vector_search_time = Histogram('vector_search_seconds', 'Time to search vectors')

@summary_generation_time.time()
async def generate_summary(self, messages):
    try:
        return await self.llm.ainvoke(...)
    except Exception as e:
        summary_generation_errors.inc()
        raise
```

### 4.3 æµ‹è¯•ç­–ç•¥

#### å•å…ƒæµ‹è¯•
```python
# tests/test_conversation_flow.py
@pytest.mark.asyncio
async def test_long_conversation_with_summary():
    """æµ‹è¯•é•¿å¯¹è¯ä¸­çš„æ‘˜è¦ç”Ÿæˆå’Œæ£€ç´¢"""
    handler = ChatHandler(store, summarizer, embedding_model)

    # ç”Ÿæˆ 15 æ¡æ¶ˆæ¯ï¼ˆè¶…è¿‡æ‘˜è¦é˜ˆå€¼ï¼‰
    for i in range(15):
        context = await handler._get_context("conv-1")
        assert context is not None

        if i > 10:
            # åº”è¯¥æœ‰æ‘˜è¦
            assert context.get("summary") is not None

@pytest.mark.asyncio
async def test_semantic_recall_accuracy():
    """æµ‹è¯•è¯­ä¹‰å¬å›çš„å‡†ç¡®æ€§"""
    # æ’å…¥ç›¸å…³å’Œä¸ç›¸å…³çš„å¯¹è¯ç‰‡æ®µ
    await memory.index_episode("conv-1", "æ¨èç§‘å¹»ç”µå½±", "æ¨èäº†ã€Šæ˜Ÿé™…ç©¿è¶Šã€‹")
    await memory.index_episode("conv-1", "ä»Šå¤©å¤©æ°”", "æ™´å¤©")

    # æœç´¢ç§‘å¹»ç›¸å…³
    results = await memory.recall_relevant("conv-1", "æœ‰ä»€ä¹ˆå¥½çš„ç§‘å¹»ç”µå½±")

    # åº”è¯¥ä¼˜å…ˆè¿”å›ç§‘å¹»ç›¸å…³çš„ç‰‡æ®µ
    assert "æ˜Ÿé™…ç©¿è¶Š" in results[0]["assistant_message"]
```

#### æ€§èƒ½åŸºå‡†æµ‹è¯•
```python
# benchmarks/test_performance.py
import asyncio
import time

async def benchmark_context_retrieval():
    """åŸºå‡†æµ‹è¯•ä¸Šä¸‹æ–‡æ£€ç´¢æ€§èƒ½"""
    handler = ChatHandler(...)

    # æµ‹è¯• 100 æ¬¡æ£€ç´¢
    times = []
    for _ in range(100):
        start = time.time()
        await handler._get_context("conv-1")
        times.append(time.time() - start)

    avg_time = sum(times) / len(times)
    max_time = max(times)

    # åº”è¯¥ < 100ms
    assert avg_time < 0.1, f"Average retrieval time {avg_time}s exceeds 100ms"
    assert max_time < 0.5, f"Max retrieval time {max_time}s exceeds 500ms"
```

### 4.4 å¸¸è§é™·é˜±

| é™·é˜± | ç—‡çŠ¶ | è§£å†³æ–¹æ¡ˆ |
|------|------|--------|
| æ‘˜è¦è¿‡äºç®€æ´ | ä¸¢å¤±é‡è¦ä¿¡æ¯ | å¢åŠ æ‘˜è¦å­—æ•°é™åˆ¶æˆ–æ£€æŸ¥æç¤ºè¯ |
| å‘é‡æœç´¢å‡†ç¡®ç‡ä½ | æ£€ç´¢åˆ°æ— å…³å¯¹è¯ | è°ƒæ•´ç›¸ä¼¼åº¦é˜ˆå€¼æˆ–æ”¹è¿›åµŒå…¥æ¨¡å‹ |
| ç´¢å¼•å †ç§¯ | å†…å­˜å ç”¨é«˜ | å®šæœŸæ¸…ç†æ—§å¯¹è¯çš„ç´¢å¼• |
| å¹¶å‘ç´¢å¼•å†²çª | æ•°æ®ä¸¢å¤±æˆ–é‡å¤ | ä½¿ç”¨åˆ†å¸ƒå¼é”ï¼ˆRedisï¼‰ä¿è¯ä¸€è‡´æ€§ |

---

## 5. é™„å½•

### 5.1 æˆæœ¬å¯¹æ¯”ï¼ˆè¯¦ç»†ä¼°ç®—ï¼‰

**å‡è®¾**ï¼š1000 æ¬¡å¯¹è¯/å¤©ï¼Œå¹³å‡æ¯å¯¹è¯ 20 è½®

| æ–¹æ¡ˆ | Token æ¶ˆè€— | API è°ƒç”¨æ•° | ä¼°è®¡æˆæœ¬ | èŠ‚çœå¯¹æ¯” |
|------|-----------|-----------|--------|---------|
| Baseline | 120M/æœˆ | 20K/å¤© | $3600/æœˆ | åŸºå‡† |
| + Phase 1 (æ‘˜è¦) | 40M/æœˆ | 20K/å¤© + 200 (æ‘˜è¦) | $1200/æœˆ | 67% â†“ |
| + Phase 2 (è¯­ä¹‰) | 35M/æœˆ | 20K/å¤© + 100 (æœç´¢) | $1050/æœˆ | 71% â†“ |
| + Phase 3 (æ¶æ„) | 32M/æœˆ | 20K/å¤© | $960/æœˆ | 73% â†“ |

### 5.2 æŠ€æœ¯æ ˆæ¸…å•

| é˜¶æ®µ | æ ¸å¿ƒä¾èµ– | å¯é€‰ä¼˜åŒ– |
|------|---------|---------|
| Phase 1 | LangChain, PostgreSQL | Redis (ç¼“å­˜æ‘˜è¦) |
| Phase 2 | Milvus, OpenAI Embedding | Qdrant (å‘é‡åº“æ›¿ä»£) |
| Phase 3 | LangGraph, asyncio | Weights & Biases (ç›‘æ§) |

### 5.3 è¿ç§»å…¼å®¹æ€§

âœ… **Phase 1 â†’ Phase 2**: å®Œå…¨å…¼å®¹ï¼Œæ‘˜è¦å’Œè¯­ä¹‰å¬å›ç‹¬ç«‹å­˜åœ¨
âœ… **Phase 1/2 â†’ Phase 3**: éœ€è¦é€‚é…å±‚ï¼Œä½†å¯é€šè¿‡ Feature Flag ç°åº¦
âš ï¸ **åŒæ—¶è¿è¡Œ**: Phase 1 å’Œ Phase 2 å¯åŒæ—¶éƒ¨ç½²ï¼Œä½†éœ€ç›‘æ§æˆæœ¬

### 5.4 æˆåŠŸåº¦é‡æŒ‡æ ‡

éƒ¨ç½²ååº”è·Ÿè¸ªä»¥ä¸‹æŒ‡æ ‡ï¼š

| æŒ‡æ ‡ | ç›®æ ‡ | æµ‹é‡æ–¹æ³• |
|------|------|---------|
| Token æ¶ˆè€— | é™ä½ 60%+ | å¯¹æ¯”éƒ¨ç½²å‰å |
| å“åº”å»¶è¿Ÿ | < 500ms | P95 å»¶è¿Ÿ |
| ç”¨æˆ·æ»¡æ„åº¦ | +15% | é—®å·/è¯„åˆ† |
| æ‘˜è¦å‡†ç¡®åº¦ | 90%+ | æ‰‹å·¥å®¡æ ¸ |
| å‘é‡å¬å›ç‡ | 85%+ | æµ‹è¯•é›†è¯„ä¼° |

### 5.5 æ•…éšœæ¢å¤

#### æ‘˜è¦ç”Ÿæˆå¤±è´¥
```python
# è‡ªåŠ¨é™çº§åˆ°æ—¶é—´çª—å£
try:
    summary = await summarizer.generate(conv_id)
except Exception as e:
    logger.warning(f"Summary generation failed: {e}")
    summary = None  # ä½¿ç”¨ Noneï¼Œå‰ç«¯æ­£å¸¸å¤„ç†
```

#### å‘é‡ç´¢å¼•å¤±è´¥
```python
# å¼‚æ­¥åå°é‡è¯•æœºåˆ¶
async def index_with_retry(self, episode, max_retries=3):
    for attempt in range(max_retries):
        try:
            await self.collection.insert([episode])
            return
        except Exception as e:
            if attempt < max_retries - 1:
                await asyncio.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
            else:
                logger.error(f"Failed to index after {max_retries} attempts: {e}")
```
