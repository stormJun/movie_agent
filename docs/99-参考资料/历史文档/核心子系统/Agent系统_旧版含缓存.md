# Agentç³»ç»Ÿ

# âš ï¸ æ³¨æ„ï¼šç¼“å­˜æè¿°å·²è¿‡æ—¶

æœ¬æ–‡æ¡£é‡Œå‡ºç°çš„ `cache_manager/global_cache_manager/CacheManager` å±äºå†å²å®ç°ï¼›v3 strict å·²å°†ç¼“å­˜ç³»ç»Ÿ**ç‰©ç†ä¸‹çº¿**ã€‚
ç°è¡Œé—­ç¯ä»¥ Postgresï¼ˆä¼šè¯/æ¶ˆæ¯/åé¦ˆï¼‰+ mem0ï¼ˆé•¿æœŸè®°å¿†ï¼‰ä¸ºå‡†ã€‚

---

## ğŸ“‹ å…ƒä¿¡æ¯

- **ç›®æ ‡è¯»è€…**ï¼šå¼€å‘è€…ã€æ¶æ„å¸ˆ
- **é˜…è¯»æ—¶é—´**ï¼š60åˆ†é’Ÿ
- **éš¾åº¦**ï¼šâ­â­â­
- **å‰ç½®çŸ¥è¯†**ï¼šLangChainã€LangGraphã€å¤§æ¨¡å‹è°ƒç”¨ã€Pythonå¼‚æ­¥ç¼–ç¨‹
- **æœ€åæ›´æ–°**ï¼š2026-01-04

---

## ğŸ“– æœ¬æ–‡å¤§çº²

- [ç³»ç»Ÿæ¦‚è§ˆ](#ç³»ç»Ÿæ¦‚è§ˆ)
- [BaseAgent æ¶æ„è®¾è®¡](#baseagent-æ¶æ„è®¾è®¡)
- [LangGraph çŠ¶æ€å›¾è®¾è®¡](#langgraph-çŠ¶æ€å›¾è®¾è®¡)
- [äº”ç§ Agent è¯¦è§£](#äº”ç§-agent-è¯¦è§£)
- [å·¥å…·æ³¨å†Œæœºåˆ¶](#å·¥å…·æ³¨å†Œæœºåˆ¶)
- [æµå¼å“åº”å®ç°](#æµå¼å“åº”å®ç°)
- [ç¼“å­˜ç³»ç»Ÿ](#ç¼“å­˜ç³»ç»Ÿ)
- [Agent å¯¹æ¯”](#agent-å¯¹æ¯”)
- [å¼€å‘æŒ‡å—](#å¼€å‘æŒ‡å—)
- [ç›¸å…³æ–‡æ¡£](#ç›¸å…³æ–‡æ¡£)

---

## ç³»ç»Ÿæ¦‚è§ˆ

### æ ¸å¿ƒç†å¿µ

æœ¬é¡¹ç›®çš„ Agent ç³»ç»Ÿé‡‡ç”¨åŸºäº **LangGraph** çš„å·¥ä½œæµç¼–æ’æ¶æ„ï¼Œå®ç°äº†å¯æ‰©å±•ã€å¯è§‚æµ‹ã€æ”¯æŒå¤šè½®å¯¹è¯çš„æ™ºèƒ½é—®ç­”ç³»ç»Ÿã€‚

**æ ¸å¿ƒè®¾è®¡æ€æƒ³**ï¼š
1. **ç»Ÿä¸€æ¶æ„**ï¼šæ‰€æœ‰ Agent ç»§æ‰¿è‡ª `BaseAgent`ï¼Œå…±äº«åŒ LLM å®ä¾‹ã€åŒå±‚ç¼“å­˜å’ŒçŠ¶æ€ç®¡ç†
2. **å·¥ä½œæµç¼–æ’**ï¼šä½¿ç”¨ LangGraph æ„å»º DAGï¼ˆæœ‰å‘æ— ç¯å›¾ï¼‰å¼çš„å¤„ç†æµç¨‹
3. **å·¥å…·å¢å¼º**ï¼šé€šè¿‡å·¥å…·æ³¨å†Œæœºåˆ¶ï¼ŒAgent å¯è°ƒç”¨ä¸åŒçš„æœç´¢ç­–ç•¥
4. **æ¸è¿›å¢å¼º**ï¼šä»ç®€å•çš„ NaiveRagAgent åˆ°å¤æ‚çš„ FusionGraphRAGAgentï¼Œé€æ­¥å¢åŠ èƒ½åŠ›

### Agent æ¶æ„å±‚çº§

```mermaid
graph TB
    subgraph åŸºç¡€æ¶æ„å±‚[åŸºç¡€æ¶æ„å±‚]
        BA[BaseAgent<br/>åŒLLM + åŒå±‚ç¼“å­˜ + LangGraph]
    end

    subgraph ç®€å•Agent[ç®€å• Agent]
        NA[NaiveRagAgent<br/>å‘é‡æ£€ç´¢]
        GA[GraphAgent<br/>å›¾ç»“æ„æ¨ç†]
        HA[HybridAgent<br/>æ··åˆæœç´¢]
    end

    subgraph é«˜çº§Agent[é«˜çº§ Agent]
        DA[DeepResearchAgent<br/>å¤šæ­¥è¿­ä»£æœç´¢]
        FA[FusionGraphRAGAgent<br/>å¤šæ™ºèƒ½ä½“åä½œ]
    end

    BA --> NA
    BA --> GA
    BA --> HA
    BA --> DA
    BA --> FA

    style BA fill:#e1f5ff,stroke:#0066cc,stroke-width:3px
    style ç®€å•Agent fill:#fff4e1,stroke:#ff9800
    style é«˜çº§Agent fill:#ffe1f5,stroke:#cc00cc
```

---

## BaseAgent æ¶æ„è®¾è®¡

### æ ¸å¿ƒç»„ä»¶

`BaseAgent` æ˜¯æ‰€æœ‰ Agent çš„åŸºç±»ï¼Œä½äº `backend/graphrag_agent/agents/base.py`ï¼Œæä¾›ä»¥ä¸‹æ ¸å¿ƒèƒ½åŠ›ï¼š

```mermaid
graph LR
    subgraph BaseAgentæ ¸å¿ƒç»„ä»¶
        A[åŒLLMå®ä¾‹]
        B[åŒå±‚ç¼“å­˜]
        C[LangGraphç¼–æ’]
        D[å·¥å…·ç®¡ç†]
        E[æ€§èƒ½ç›‘æ§]
        F[çŠ¶æ€æŒä¹…åŒ–]
    end

    A --> C
    B --> C
    D --> C
    E --> C
    F --> C

    style A fill:#ffebee
    style B fill:#e8f5e9
    style C fill:#e1f5ff
    style D fill:#fff3e0
    style E fill:#f3e5f5
    style F fill:#e0f2f1
```

### åŒ LLM å®ä¾‹

**è®¾è®¡ç†å¿µ**ï¼šåˆ†ç¦»åŒæ­¥å’Œæµå¼åœºæ™¯çš„ LLM è°ƒç”¨ï¼Œä¼˜åŒ–æ€§èƒ½å’Œç”¨æˆ·ä½“éªŒã€‚

```python
class BaseAgent(ABC):
    def __init__(self, cache_dir="./cache", memory_only=False):
        # 1. æ™®é€š LLM - ç”¨äºæ ‡å‡†è°ƒç”¨
        self.llm = get_llm_model()

        # 2. æµå¼ LLM - ç”¨äºæµå¼å“åº”
        self.stream_llm = get_stream_llm_model()

        # 3. åµŒå…¥æ¨¡å‹ - ç”¨äºå‘é‡ç”Ÿæˆ
        self.embeddings = get_embeddings_model()
```

**ä½¿ç”¨åœºæ™¯**ï¼š
- `self.llm`ï¼šç”¨äº `ask()` æ–¹æ³•ã€å…³é”®è¯æå–ã€å·¥å…·è°ƒç”¨å†³ç­–
- `self.stream_llm`ï¼šç”¨äº `ask_stream()` æ–¹æ³•ï¼Œæä¾›æµå¼å“åº”
- `self.embeddings`ï¼šç”¨äºç¼“å­˜ç›¸ä¼¼åº¦åŒ¹é…ã€å…³é”®è¯å‘é‡åŒ–

### åŒå±‚ç¼“å­˜æ¶æ„

```mermaid
graph TB
    Q[ç”¨æˆ·æŸ¥è¯¢]

    subgraph ç¼“å­˜å±‚[åŒå±‚ç¼“å­˜ç³»ç»Ÿ]
        GC[å…¨å±€ç¼“å­˜<br/>global_cache_manager<br/>è·¨ä¼šè¯å…±äº«]
        SC[ä¼šè¯ç¼“å­˜<br/>cache_manager<br/>ä¸Šä¸‹æ–‡æ„ŸçŸ¥]
    end

    subgraph å¤„ç†å±‚[æ‰§è¡Œå±‚]
        AG[Agentå¤„ç†]
        LLM[LLMè°ƒç”¨]
    end

    Q --> GC
    GC -->|å‘½ä¸­| R1[è¿”å›ç»“æœ]
    GC -->|æœªå‘½ä¸­| SC
    SC -->|å‘½ä¸­| R2[è¿”å›ç»“æœ<br/>+åŒæ­¥åˆ°å…¨å±€ç¼“å­˜]
    SC -->|æœªå‘½ä¸­| AG
    AG --> LLM
    LLM --> AG
    AG --> SC
    AG --> GC

    style GC fill:#e8f5e9,stroke:#4caf50,stroke-width:2px
    style SC fill:#fff3e0,stroke:#ff9800,stroke-width:2px
    style LLM fill:#ffebee,stroke:#f44336
```

**ç¼“å­˜åˆå§‹åŒ–ä»£ç **ï¼š

```python
# ä¼šè¯çº§ç¼“å­˜ï¼ˆä¸Šä¸‹æ–‡æ„ŸçŸ¥ï¼‰
self.cache_manager = CacheManager(
    key_strategy=ContextAwareCacheKeyStrategy(),  # è€ƒè™‘å¯¹è¯å†å²
    storage_backend=HybridCacheBackend(
        cache_dir=cache_dir,
        memory_max_size=200,   # å†…å­˜ç¼“å­˜200æ¡
        disk_max_size=2000     # ç£ç›˜ç¼“å­˜2000æ¡
    ),
    cache_dir=cache_dir,
    memory_only=memory_only
)

# å…¨å±€ç¼“å­˜ï¼ˆè·¨ä¼šè¯ï¼‰
self.global_cache_manager = CacheManager(
    key_strategy=GlobalCacheKeyStrategy(),  # ä»…åŸºäºæŸ¥è¯¢æ–‡æœ¬
    storage_backend=HybridCacheBackend(
        cache_dir=f"{cache_dir}/global",
        memory_max_size=500,   # æ›´å¤§çš„å†…å­˜ç¼“å­˜
        disk_max_size=5000     # æ›´å¤§çš„ç£ç›˜ç¼“å­˜
    ),
    cache_dir=f"{cache_dir}/global",
    memory_only=memory_only
)
```

**ç¼“å­˜æŸ¥è¯¢ä¼˜å…ˆçº§**ï¼š
1. å…¨å±€ç¼“å­˜ï¼ˆæœ€å¿«ï¼Œè·¨ä¼šè¯ï¼‰
2. å¿«é€Ÿè·¯å¾„ç¼“å­˜ï¼ˆé«˜è´¨é‡æ ‡è®°çš„ç¼“å­˜ï¼Œè·³è¿‡éªŒè¯ï¼‰
3. å¸¸è§„ä¼šè¯ç¼“å­˜ï¼ˆå¸¦éªŒè¯ï¼‰
4. æ‰§è¡Œå®Œæ•´å¤„ç†æµç¨‹

### çŠ¶æ€æŒä¹…åŒ–

ä½¿ç”¨ LangGraph çš„ `MemorySaver` å®ç°å¤šè½®å¯¹è¯è®°å¿†ï¼š

```python
self.memory = MemorySaver()  # ä¼šè¯çŠ¶æ€æŒä¹…åŒ–

# ç¼–è¯‘å›¾æ—¶ç»‘å®š checkpointer
self.graph = workflow.compile(checkpointer=self.memory)

# ä½¿ç”¨æ—¶é€šè¿‡ thread_id éš”ç¦»ä¸åŒä¼šè¯
config = {
    "configurable": {
        "thread_id": thread_id,  # ä¼šè¯ID
        "recursion_limit": recursion_limit
    }
}
```

---

## LangGraph çŠ¶æ€å›¾è®¾è®¡

### å·¥ä½œæµæ ¸å¿ƒæ¦‚å¿µ

LangGraph æ˜¯ä¸€ä¸ªåŸºäºçŠ¶æ€å›¾çš„å·¥ä½œæµç¼–æ’æ¡†æ¶ï¼Œæ ¸å¿ƒæ¦‚å¿µåŒ…æ‹¬ï¼š

1. **Stateï¼ˆçŠ¶æ€ï¼‰**ï¼šåœ¨å·¥ä½œæµä¸­æµåŠ¨çš„æ•°æ®ç»“æ„
2. **Nodeï¼ˆèŠ‚ç‚¹ï¼‰**ï¼šå¤„ç†çŠ¶æ€çš„å‡½æ•°
3. **Edgeï¼ˆè¾¹ï¼‰**ï¼šèŠ‚ç‚¹ä¹‹é—´çš„è¿æ¥è·¯å¾„
4. **Conditional Edgeï¼ˆæ¡ä»¶è¾¹ï¼‰**ï¼šæ ¹æ®çŠ¶æ€åŠ¨æ€é€‰æ‹©ä¸‹ä¸€ä¸ªèŠ‚ç‚¹

### æ ‡å‡†å·¥ä½œæµç¨‹

```mermaid
graph LR
    START([å¼€å§‹]) --> Agent[AgentèŠ‚ç‚¹<br/>LLMå†³ç­–]

    Agent --> ToolsCondition{éœ€è¦å·¥å…·?}

    ToolsCondition -->|æ˜¯| Retrieve[RetrieveèŠ‚ç‚¹<br/>æ‰§è¡Œå·¥å…·]
    ToolsCondition -->|å¦| END1([ç›´æ¥ç»“æŸ])

    Retrieve --> Generate[GenerateèŠ‚ç‚¹<br/>ç”Ÿæˆç­”æ¡ˆ]
    Generate --> END2([ç»“æŸ])

    style Agent fill:#e3f2fd
    style Retrieve fill:#fff3e0
    style Generate fill:#e8f5e9
    style ToolsCondition fill:#fce4ec
```

### çŠ¶æ€å®šä¹‰

```python
class AgentState(TypedDict):
    """
    Agent çš„çŠ¶æ€ç»“æ„

    å­—æ®µè¯´æ˜:
        messages: æ¶ˆæ¯åºåˆ—ï¼Œè®°å½•æ•´ä¸ªå¯¹è¯å†å²å’Œä¸­é—´ç»“æœ

    æ•°æ®ç»“æ„ç¤ºä¾‹:
        [
            HumanMessage("ç”¨æˆ·æé—®"),           # ç”¨æˆ·è¾“å…¥
            AIMessage("", tool_calls=[...]),   # LLM å†³ç­–è°ƒç”¨å·¥å…·
            ToolMessage("å·¥å…·è¿”å›ç»“æœ"),        # å·¥å…·æ‰§è¡Œç»“æœ
            AIMessage("æœ€ç»ˆç­”æ¡ˆ")              # LLM ç”Ÿæˆçš„å›ç­”
        ]
    """
    messages: Annotated[Sequence[BaseMessage], add_messages]
```

**å…³é”®ç‚¹**ï¼š
- `Annotated` æ˜¯ Python çš„ç±»å‹æ³¨è§£è¯­æ³•
- `add_messages` æ˜¯ LangGraph çš„ reducer å‡½æ•°ï¼Œè‡ªåŠ¨è¿½åŠ æ¶ˆæ¯åˆ°åˆ—è¡¨

### èŠ‚ç‚¹å®ç°

#### 1. Agent èŠ‚ç‚¹ï¼ˆå†³ç­–ä¸­å¿ƒï¼‰

```python
def _agent_node(self, state):
    """
    Agent èŠ‚ç‚¹ - LLM å†³ç­–ä¸­å¿ƒ

    ä½œç”¨:
        - æå–ç”¨æˆ·æŸ¥è¯¢çš„å…³é”®è¯
        - è°ƒç”¨ LLM å†³ç­–æ˜¯å¦éœ€è¦ä½¿ç”¨å·¥å…·
        - è¿”å› LLM çš„å“åº”æ¶ˆæ¯
    """
    messages = state["messages"]

    # å…³é”®è¯æå–ä¸æ¶ˆæ¯å¢å¼º
    if len(messages) > 0 and isinstance(messages[-1], HumanMessage):
        query = messages[-1].content
        keywords = self._extract_keywords(query)  # å­ç±»å®ç°

        if keywords:
            # åˆ›å»ºå¸¦å…³é”®è¯å…ƒæ•°æ®çš„å¢å¼ºæ¶ˆæ¯
            enhanced_message = HumanMessage(
                content=query,
                additional_kwargs={"keywords": keywords}
            )
            messages = messages[:-1] + [enhanced_message]

    # ç»‘å®šå·¥å…·åˆ° LLM
    model = self.llm.bind_tools(self.tools)

    # è°ƒç”¨ LLM è¿›è¡Œæ¨ç†
    response = model.invoke(messages)

    return {"messages": [response]}
```

**æµç¨‹è¯¦è§£**ï¼š
1. æå–æœ€åä¸€æ¡æ¶ˆæ¯ï¼ˆç”¨æˆ·æŸ¥è¯¢ï¼‰
2. ä½¿ç”¨å­ç±»çš„ `_extract_keywords()` æå–å…³é”®è¯
3. å°†å…³é”®è¯é™„åŠ åˆ°æ¶ˆæ¯å…ƒæ•°æ®ä¸­
4. ç»‘å®šå·¥å…·åˆ° LLMï¼ˆå‘Šè¯‰ LLM å¯ç”¨çš„å·¥å…·ï¼‰
5. LLM å†³ç­–æ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·

#### 2. Retrieve èŠ‚ç‚¹ï¼ˆå·¥å…·æ‰§è¡Œï¼‰

ä½¿ç”¨ LangGraph å†…ç½®çš„ `ToolNode`ï¼š

```python
workflow.add_node("retrieve", ToolNode(self.tools))
```

**ToolNode è‡ªåŠ¨å¤„ç†é€»è¾‘**ï¼š
1. ä»æœ€åä¸€æ¡æ¶ˆæ¯æå– `tool_calls`
2. æ ¹æ® `tool_calls[i].name` æ‰¾åˆ°å¯¹åº”å·¥å…·
3. æ‰§è¡Œ `tool.invoke(tool_calls[i].args)`
4. å°†ç»“æœåŒ…è£…æˆ `ToolMessage`
5. è¿½åŠ åˆ° `messages` åˆ—è¡¨

#### 3. Generate èŠ‚ç‚¹ï¼ˆç­”æ¡ˆç”Ÿæˆï¼‰

```python
def _generate_node(self, state):
    """ç”Ÿæˆå›ç­”èŠ‚ç‚¹é€»è¾‘"""
    messages = state["messages"]
    question = messages[-3].content  # ç”¨æˆ·é—®é¢˜
    docs = messages[-1].content      # å·¥å…·è¿”å›çš„æ–‡æ¡£

    # ç¼“å­˜æ£€æŸ¥
    cached_result = self.cache_manager.get(question, thread_id=thread_id)
    if cached_result:
        return {"messages": [AIMessage(content=cached_result)]}

    # ä½¿ç”¨ LLM ç”Ÿæˆç­”æ¡ˆ
    prompt = ChatPromptTemplate.from_messages([
        ("system", LC_SYSTEM_PROMPT),
        ("human", GENERATE_PROMPT),
    ])

    chain = prompt | self.llm | StrOutputParser()
    response = chain.invoke({
        "context": docs,
        "question": question
    })

    # æ›´æ–°ç¼“å­˜
    self.cache_manager.set(question, response, thread_id=thread_id)
    self.global_cache_manager.set(question, response)

    return {"messages": [AIMessage(content=response)]}
```

### æ¡ä»¶è¾¹ï¼štools_condition

```python
workflow.add_conditional_edges(
    source="agent",           # ä» agent èŠ‚ç‚¹å‡ºå‘
    path=tools_condition,     # ä½¿ç”¨å†…ç½®æ¡ä»¶å‡½æ•°
    path_map={
        "tools": "retrieve",  # æœ‰ tool_calls â†’ å» retrieve
        END: END,             # æ—  tool_calls â†’ ç›´æ¥ç»“æŸ
    },
)
```

**tools_condition åˆ¤æ–­é€»è¾‘**ï¼š
```python
def tools_condition(state) -> str:
    last_message = state["messages"][-1]
    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:
        return "tools"  # éœ€è¦è°ƒç”¨å·¥å…·
    else:
        return END      # ä¸éœ€è¦å·¥å…·ï¼Œç›´æ¥ç»“æŸ
```

---

## äº”ç§ Agent è¯¦è§£

### 1. NaiveRagAgentï¼ˆåŸºç¡€å‘é‡æ£€ç´¢ï¼‰

**ç‰¹ç‚¹**ï¼šæœ€ç®€å•çš„ RAG å®ç°ï¼Œä»…ä½¿ç”¨å‘é‡æ£€ç´¢ã€‚

**å·¥ä½œæµç¨‹**ï¼š
```mermaid
sequenceDiagram
    participant U as ç”¨æˆ·
    participant A as NaiveRagAgent
    participant V as VectorStore
    participant L as LLM

    U->>A: æé—®
    A->>V: å‘é‡ç›¸ä¼¼åº¦æœç´¢
    V-->>A: è¿”å› Top-K æ–‡æ¡£
    A->>L: æ–‡æ¡£ + é—®é¢˜
    L-->>A: ç”Ÿæˆç­”æ¡ˆ
    A-->>U: è¿”å›ç­”æ¡ˆ
```

**æ ¸å¿ƒä»£ç **ï¼š
```python
class NaiveRagAgent(BaseAgent):
    def __init__(self):
        self.search_tool = NaiveSearchTool()  # å‘é‡æœç´¢å·¥å…·
        super().__init__(cache_dir="./cache/naive_agent")

    def _setup_tools(self) -> List:
        return [self.search_tool.get_tool()]

    def _add_retrieval_edges(self, workflow):
        # ç®€å•çš„ä»æ£€ç´¢ç›´æ¥åˆ°ç”Ÿæˆ
        workflow.add_edge("retrieve", "generate")

    def _extract_keywords(self, query: str) -> Dict[str, List[str]]:
        # ä¸åšå…³é”®è¯æå–
        return {"low_level": [], "high_level": []}
```

**é€‚ç”¨åœºæ™¯**ï¼š
- ç®€å•é—®ç­”
- æ–‡æ¡£æ•°é‡è¾ƒå°‘
- ä¸éœ€è¦å¤æ‚æ¨ç†

---

### 2. GraphAgentï¼ˆå›¾ç»“æ„æ¨ç†ï¼‰

**ç‰¹ç‚¹**ï¼šåŸºäºçŸ¥è¯†å›¾è°±çš„æœ¬åœ°å’Œå…¨å±€æœç´¢ã€‚

**å·¥ä½œæµç¨‹**ï¼š
```mermaid
graph TB
    Q[ç”¨æˆ·æŸ¥è¯¢] --> Agent[AgentèŠ‚ç‚¹]
    Agent --> ToolDecision{å·¥å…·å†³ç­–}

    ToolDecision -->|æœ¬åœ°æœç´¢| Local[LocalSearchTool<br/>å®ä½“é‚»å±…æ‰©å±•]
    ToolDecision -->|å…¨å±€æœç´¢| Global[GlobalSearchTool<br/>ç¤¾åŒºæ‘˜è¦èšåˆ]

    Local --> Grade{æ–‡æ¡£è¯„åˆ†}
    Global --> Reduce[ReduceèŠ‚ç‚¹<br/>Map-Reduceæ•´åˆ]

    Grade -->|è´¨é‡é«˜| Generate[GenerateèŠ‚ç‚¹]
    Grade -->|è´¨é‡ä½| Retry[é‡è¯•æœ¬åœ°æœç´¢]

    Generate --> End([ç»“æŸ])
    Reduce --> End

    style Local fill:#e3f2fd
    style Global fill:#fff3e0
    style Generate fill:#e8f5e9
```

**æ ¸å¿ƒä»£ç **ï¼š
```python
class GraphAgent(BaseAgent):
    def __init__(self):
        self.local_tool = LocalSearchTool()   # æœ¬åœ°æœç´¢
        self.global_tool = GlobalSearchTool() # å…¨å±€æœç´¢
        super().__init__(cache_dir="./cache/graph_agent")

    def _setup_tools(self) -> List:
        return [
            self.local_tool.get_tool(),
            self.global_tool.search,
        ]

    def _add_retrieval_edges(self, workflow):
        # æ·»åŠ  reduce èŠ‚ç‚¹ï¼ˆç”¨äºå…¨å±€æœç´¢ï¼‰
        workflow.add_node("reduce", self._reduce_node)

        # æ¡ä»¶è¾¹ï¼šæ ¹æ®æ–‡æ¡£è¯„åˆ†å†³å®šè·¯ç”±
        workflow.add_conditional_edges(
            "retrieve",
            self._grade_documents,  # è¯„åˆ†å‡½æ•°
            {
                "generate": "generate",
                "reduce": "reduce"      # å…¨å±€æœç´¢èµ° reduce
            }
        )

        workflow.add_edge("reduce", END)

    def _grade_documents(self, state) -> str:
        """è¯„ä¼°æ–‡æ¡£ç›¸å…³æ€§"""
        messages = state["messages"]
        retrieve_message = messages[-2]

        # æ£€æŸ¥æ˜¯å¦ä¸ºå…¨å±€æ£€ç´¢å·¥å…·è°ƒç”¨
        tool_calls = retrieve_message.additional_kwargs.get("tool_calls", [])
        if tool_calls and tool_calls[0].get("function", {}).get("name") == "global_retriever":
            return "reduce"  # å…¨å±€æœç´¢éœ€è¦ reduce

        # å…¶ä»–æƒ…å†µä½¿ç”¨ generate
        return "generate"
```

**å…³é”®è¯æå–**ï¼š
```python
def _extract_keywords(self, query: str) -> Dict[str, List[str]]:
    # ä½¿ç”¨ LLM æå–å…³é”®è¯
    prompt = GRAPH_AGENT_KEYWORD_PROMPT.format(query=query)
    result = self.llm.invoke(prompt)

    # è§£æ JSON æ ¼å¼çš„å…³é”®è¯
    keywords = json.loads(result.content)
    return keywords
    # è¿”å›æ ¼å¼: {"low_level": [...], "high_level": [...]}
```

**é€‚ç”¨åœºæ™¯**ï¼š
- éœ€è¦ç†è§£å®ä½“å…³ç³»
- å¤šè·³æ¨ç†
- å…¨å±€æ€§é—®é¢˜ï¼ˆå¦‚ç»Ÿè®¡ã€æ’åï¼‰

---

### 3. HybridAgentï¼ˆæ··åˆæœç´¢ï¼‰

**ç‰¹ç‚¹**ï¼šç»“åˆå‘é‡æ£€ç´¢å’Œå›¾æœç´¢ï¼ŒåŠ¨æ€é€‰æ‹©æœ€ä½³ç­–ç•¥ã€‚

**å·¥ä½œæµç¨‹**ï¼š
```mermaid
graph TB
    Q[ç”¨æˆ·æŸ¥è¯¢] --> KW[å…³é”®è¯æå–<br/>HybridSearchTool]
    KW --> Strategy{æœç´¢ç­–ç•¥}

    Strategy -->|å‘é‡åŒ¹é…| V[å‘é‡æ£€ç´¢<br/>TOP-K chunks]
    Strategy -->|å›¾ç»“æ„| G[å›¾è°±æ£€ç´¢<br/>å®ä½“+å…³ç³»]
    Strategy -->|æ··åˆ| H[åŠ æƒèåˆ<br/>Î±Â·å‘é‡ + Î²Â·å›¾è°±]

    V --> Merge[ç»“æœåˆå¹¶]
    G --> Merge
    H --> Merge

    Merge --> Generate[ç”Ÿæˆç­”æ¡ˆ]
    Generate --> End([ç»“æŸ])

    style Strategy fill:#fce4ec
    style Merge fill:#e8f5e9
```

**æ ¸å¿ƒä»£ç **ï¼š
```python
class HybridAgent(BaseAgent):
    def __init__(self):
        self.search_tool = HybridSearchTool()  # æ··åˆæœç´¢å·¥å…·
        super().__init__(cache_dir="./cache/hybrid_agent")

    def _setup_tools(self) -> List:
        return [
            self.search_tool.get_tool(),
            self.search_tool.get_global_tool(),
        ]

    def _extract_keywords(self, query: str) -> Dict[str, List[str]]:
        # ä½¿ç”¨æœç´¢å·¥å…·çš„å…³é”®è¯æå–åŠŸèƒ½
        return self.search_tool.extract_keywords(query)
```

**HybridSearchTool å·¥ä½œåŸç†**ï¼š
```python
class HybridSearchTool:
    def search(self, query: str) -> str:
        # 1. å…³é”®è¯æå–
        keywords = self.extract_keywords(query)

        # 2. å‘é‡æ£€ç´¢
        vector_results = self._vector_search(query, top_k=5)

        # 3. å›¾è°±æ£€ç´¢
        graph_results = self._graph_search(keywords, max_hops=2)

        # 4. åŠ æƒèåˆ
        final_results = self._merge_results(
            vector_results,
            graph_results,
            alpha=0.6,  # å‘é‡æƒé‡
            beta=0.4    # å›¾è°±æƒé‡
        )

        return final_results
```

**é€‚ç”¨åœºæ™¯**ï¼š
- å¤æ‚æŸ¥è¯¢ï¼ˆæ—¢éœ€è¦è¯­ä¹‰åŒ¹é…åˆéœ€è¦ç»“æ„æ¨ç†ï¼‰
- ä¸­ç­‰è§„æ¨¡çŸ¥è¯†åº“
- å¹³è¡¡å‡†ç¡®ç‡å’Œå¬å›ç‡

---

### 4. DeepResearchAgentï¼ˆå¤šæ­¥è¿­ä»£æœç´¢ï¼‰

**ç‰¹ç‚¹**ï¼šå¤šè½® Think-Search-Reason å¾ªç¯ï¼Œæ·±åº¦æ¢ç´¢çŸ¥è¯†å›¾è°±ã€‚

**å·¥ä½œæµç¨‹**ï¼š
```mermaid
sequenceDiagram
    participant U as ç”¨æˆ·
    participant A as DeepResearchAgent
    participant T as ThinkingModule
    participant S as SearchModule
    participant R as ReasoningModule

    U->>A: å¤æ‚æŸ¥è¯¢

    loop è¿­ä»£æ¢ç´¢ï¼ˆæœ€å¤š3è½®ï¼‰
        A->>T: åˆ†æå½“å‰çŠ¶æ€
        T-->>A: ç”Ÿæˆå­é—®é¢˜

        A->>S: æ‰§è¡Œå­é—®é¢˜æœç´¢
        S-->>A: è¿”å›è¯æ®

        A->>R: æ¨ç†æ•´åˆ
        R-->>A: ä¸­é—´ç»“è®º

        Note over A: æ£€æŸ¥æ˜¯å¦éœ€è¦ç»§ç»­æ¢ç´¢
    end

    A->>A: æœ€ç»ˆæ•´åˆ
    A-->>U: è¿”å›å®Œæ•´ç­”æ¡ˆ + æ¨ç†é“¾
```

**æ ¸å¿ƒä»£ç **ï¼š
```python
class DeepResearchAgent(BaseAgent):
    def __init__(self, use_deeper_tool=True):
        if use_deeper_tool:
            self.research_tool = DeeperResearchTool()
            self.exploration_tool = self.research_tool.get_exploration_tool()
            self.reasoning_analysis_tool = self.research_tool.get_reasoning_analysis_tool()
        else:
            self.research_tool = DeepResearchTool()

        super().__init__(cache_dir="./cache/enhanced_research_agent")

    def _setup_tools(self) -> List:
        tools = [self.research_tool.get_tool()]

        if self.use_deeper_tool:
            tools.append(self.exploration_tool)
            tools.append(self.reasoning_analysis_tool)

        return tools

    def ask_with_thinking(self, query: str, thread_id: str = "default"):
        """è¿”å›å¸¦æ€è€ƒè¿‡ç¨‹çš„ç­”æ¡ˆ"""
        result = self.research_tool.thinking(query)
        return result
        # è¿”å›æ ¼å¼:
        # {
        #     "answer": "æœ€ç»ˆç­”æ¡ˆ",
        #     "thinking_steps": [...],
        #     "evidence": [...],
        #     "reasoning_chain": [...]
        # }
```

**æ€è€ƒè¿‡ç¨‹æ ¼å¼**ï¼š
```xml
<think>
æ­¥éª¤1: é—®é¢˜åˆ†è§£
- å­é—®é¢˜1: ...
- å­é—®é¢˜2: ...

æ­¥éª¤2: è¯æ®æ”¶é›†
- è¯æ®1: ... [æ¥æº: ...]
- è¯æ®2: ... [æ¥æº: ...]

æ­¥éª¤3: æ¨ç†æ•´åˆ
- æ¨è®º1: ...
- æ¨è®º2: ...
</think>

æœ€ç»ˆç­”æ¡ˆ: ...
```

**é€‚ç”¨åœºæ™¯**ï¼š
- å¤æ‚å¤šè·³æ¨ç†
- éœ€è¦å±•ç¤ºæ¨ç†è¿‡ç¨‹
- ç ”ç©¶æ€§æŸ¥è¯¢

---

### 5. FusionGraphRAGAgentï¼ˆå¤šæ™ºèƒ½ä½“åä½œï¼‰

**ç‰¹ç‚¹**ï¼šPlan-Execute-Report ä¸‰é˜¶æ®µï¼Œå¤š Agent å¹¶è¡Œåä½œã€‚

**æ¶æ„å›¾**ï¼š
```mermaid
graph TB
    subgraph Plané˜¶æ®µ[Plan é˜¶æ®µ]
        C[Clarifier<br/>æ„å›¾æ¾„æ¸…]
        TD[TaskDecomposer<br/>ä»»åŠ¡åˆ†è§£]
        PR[PlanReviewer<br/>è®¡åˆ’å®¡æ ¸]
    end

    subgraph Executeé˜¶æ®µ[Execute é˜¶æ®µ]
        WC[WorkerCoordinator<br/>ä»»åŠ¡è°ƒåº¦]

        subgraph Workers[Worker Pool]
            W1[Retrieval Worker<br/>æ£€ç´¢ä»»åŠ¡]
            W2[Research Worker<br/>ç ”ç©¶ä»»åŠ¡]
            W3[Reflection Worker<br/>åæ€ä»»åŠ¡]
        end
    end

    subgraph Reporté˜¶æ®µ[Report é˜¶æ®µ]
        OB[OutlineBuilder<br/>å¤§çº²æ„å»º]
        SW[SectionWriter<br/>åˆ†æ®µæ’°å†™]
        CC[ConsistencyChecker<br/>ä¸€è‡´æ€§æ£€æŸ¥]
    end

    Q[ç”¨æˆ·æŸ¥è¯¢] --> C
    C --> TD
    TD --> PR
    PR --> WC

    WC --> W1
    WC --> W2
    WC --> W3

    W1 --> OB
    W2 --> OB
    W3 --> OB

    OB --> SW
    SW --> CC
    CC --> Result[æœ€ç»ˆæŠ¥å‘Š]

    style Plané˜¶æ®µ fill:#e3f2fd
    style Executeé˜¶æ®µ fill:#fff3e0
    style Reporté˜¶æ®µ fill:#e8f5e9
```

**æ ¸å¿ƒä»£ç **ï¼š
```python
class FusionGraphRAGAgent(BaseAgent):
    def __init__(self):
        # åˆå§‹åŒ–ä¸‰é˜¶æ®µç»„ä»¶
        self.planner = Planner()
        self.executor = Executor()
        self.reporter = Reporter()

        super().__init__(cache_dir="./cache/fusion_agent")

    def ask(self, query: str, thread_id: str = "default"):
        # 1. Plan é˜¶æ®µ
        plan_spec = self.planner.plan(query)
        # plan_spec = {
        #     "tasks": [...],
        #     "dependencies": {...},
        #     "priority": [...]
        # }

        # 2. Execute é˜¶æ®µ
        execution_records = self.executor.execute(plan_spec)
        # execution_records = [
        #     {"task_id": 1, "result": ..., "evidence": ...},
        #     ...
        # ]

        # 3. Report é˜¶æ®µ
        final_report = self.reporter.generate_report(
            query=query,
            executions=execution_records
        )

        return final_report
```

**Plan é˜¶æ®µè¯¦è§£**ï¼š
```python
class Planner:
    def plan(self, query: str) -> PlanSpec:
        # 1. Clarifier - æ¾„æ¸…ç”¨æˆ·æ„å›¾
        clarified_query = self.clarifier.clarify(query)

        # 2. TaskDecomposer - ä»»åŠ¡åˆ†è§£
        tasks = self.decomposer.decompose(clarified_query)
        # tasks = [
        #     {"id": 1, "type": "retrieval", "query": "..."},
        #     {"id": 2, "type": "research", "query": "..."},
        # ]

        # 3. PlanReviewer - å®¡æ ¸ä¼˜åŒ–
        optimized_plan = self.reviewer.review(tasks)

        return optimized_plan
```

**é€‚ç”¨åœºæ™¯**ï¼š
- è¶…å¤æ‚æŸ¥è¯¢ï¼ˆå¦‚æ’°å†™æŠ¥å‘Šï¼‰
- éœ€è¦å¤šç»´åº¦åˆ†æ
- é«˜è´¨é‡è¾“å‡ºè¦æ±‚

---

## å·¥å…·æ³¨å†Œæœºåˆ¶

### å·¥å…·æ³¨å†Œæµç¨‹

```mermaid
graph LR
    Tool[æœç´¢å·¥å…·ç±»] --> Wrapper[LangChain Tool Wrapper]
    Wrapper --> Registry[å·¥å…·æ³¨å†Œè¡¨]
    Registry --> Agent[Agent _setup_tools]
    Agent --> LLM[ç»‘å®šåˆ° LLM]

    style Tool fill:#e3f2fd
    style Registry fill:#fff3e0
    style LLM fill:#e8f5e9
```

### å·¥å…·å®šä¹‰ç¤ºä¾‹

```python
from langchain.tools import tool

class LocalSearchTool:
    def __init__(self):
        self.searcher = LocalSearch(llm, embeddings)

    @tool
    def search(self, query: str) -> str:
        """
        åœ¨çŸ¥è¯†å›¾è°±ä¸­è¿›è¡Œæœ¬åœ°æœç´¢ã€‚

        å‚æ•°:
            query: æœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²

        è¿”å›:
            str: æ£€ç´¢åˆ°çš„ç›¸å…³ä¿¡æ¯
        """
        return self.searcher.search(query)

    def get_tool(self):
        """è¿”å› LangChain å·¥å…·å¯¹è±¡"""
        return self.search
```

**å·¥å…·æè¿°çš„é‡è¦æ€§**ï¼š
- LLM é€šè¿‡å·¥å…·çš„ `docstring` ç†è§£å·¥å…·åŠŸèƒ½
- æè¿°è¶Šæ¸…æ™°ï¼ŒLLM é€‰æ‹©è¶Šå‡†ç¡®

### å¤šå·¥å…·åä½œ

```python
class HybridAgent(BaseAgent):
    def _setup_tools(self) -> List:
        return [
            self.search_tool.get_tool(),          # æ··åˆæœç´¢
            self.search_tool.get_global_tool(),   # å…¨å±€æœç´¢
        ]

# LLM ä¼šæ ¹æ®æŸ¥è¯¢ç‰¹ç‚¹è‡ªåŠ¨é€‰æ‹©å·¥å…·
# ä¾‹å¦‚ï¼š
# "å¥–å­¦é‡‘ç”³è¯·æ¡ä»¶ï¼Ÿ" â†’ ä½¿ç”¨ hybrid search
# "æ‰€æœ‰å¥–å­¦é‡‘ç±»å‹ç»Ÿè®¡ï¼Ÿ" â†’ ä½¿ç”¨ global search
```

---

## æµå¼å“åº”å®ç°

### ä¼ªæµå¼ vs çœŸæµå¼

**å½“å‰å®ç°**ï¼šä¼ªæµå¼ï¼ˆç”±äº LangChain ç‰ˆæœ¬é™åˆ¶ï¼‰

```mermaid
graph LR
    Q[ç”¨æˆ·æŸ¥è¯¢] --> LLM[LLMç”Ÿæˆ<br/>å®Œæ•´ç­”æ¡ˆ]
    LLM --> Buffer[ç¼“å†²åŒº]
    Buffer --> Chunk1[åˆ†å—1]
    Buffer --> Chunk2[åˆ†å—2]
    Buffer --> Chunk3[åˆ†å—3]

    Chunk1 --> U[ç”¨æˆ·çœ‹åˆ°]
    Chunk2 --> U
    Chunk3 --> U

    style LLM fill:#ffebee
    style Buffer fill:#fff3e0
```

**çœŸæµå¼ï¼ˆæœªæ¥å‡çº§ï¼‰**ï¼š
```mermaid
graph LR
    Q[ç”¨æˆ·æŸ¥è¯¢] --> LLM[LLMæµå¼ç”Ÿæˆ]
    LLM --> Token1[Token 1]
    LLM --> Token2[Token 2]
    LLM --> Token3[Token 3]

    Token1 --> U[ç”¨æˆ·å®æ—¶çœ‹åˆ°]
    Token2 --> U
    Token3 --> U

    style LLM fill:#e8f5e9
```

### æµå¼ä»£ç å®ç°

```python
async def _stream_process(self, inputs, config):
    """å®ç°æµå¼å¤„ç†è¿‡ç¨‹"""
    query = inputs["messages"][-1].content
    thread_id = config.get("configurable", {}).get("thread_id", "default")

    # 1. ç¼“å­˜æ£€æŸ¥
    cached_response = self.cache_manager.get(query.strip(), thread_id=thread_id)
    if cached_response:
        # åˆ†å—è¿”å›ç¼“å­˜ç»“æœ
        chunks = re.split(r'([.!?ã€‚ï¼ï¼Ÿ]\s*)', cached_response)
        buffer = ""

        for i in range(0, len(chunks)):
            buffer += chunks[i]

            # å½“ç¼“å†²åŒºåŒ…å«å®Œæ•´å¥å­æˆ–è¾¾åˆ°é˜ˆå€¼æ—¶è¾“å‡º
            if (i % 2 == 1) or len(buffer) >= self.stream_flush_threshold:
                yield buffer
                buffer = ""
                await asyncio.sleep(0.01)

        if buffer:
            yield buffer
        return

    # 2. æ‰§è¡Œå·¥ä½œæµ
    workflow_state = {"messages": [HumanMessage(content=query)]}

    # æç¤ºç”¨æˆ·æ­£åœ¨å¤„ç†
    yield "**æ­£åœ¨åˆ†æé—®é¢˜**...\n\n"

    # Agent èŠ‚ç‚¹
    agent_output = self._agent_node(workflow_state)
    workflow_state = {"messages": workflow_state["messages"] + agent_output["messages"]}

    # æ£€æŸ¥æ˜¯å¦éœ€è¦å·¥å…·
    tool_decision = tools_condition(workflow_state)
    if tool_decision == "tools":
        yield "**æ­£åœ¨æ£€ç´¢ç›¸å…³ä¿¡æ¯**...\n\n"

        # æ£€ç´¢èŠ‚ç‚¹
        retrieve_output = await self._retrieve_node_async(workflow_state)
        workflow_state = {"messages": workflow_state["messages"] + retrieve_output["messages"]}

        yield "**æ­£åœ¨ç”Ÿæˆå›ç­”**...\n\n"

        # æµå¼ç”Ÿæˆ
        async for token in self._generate_node_stream(workflow_state):
            yield token
```

### åˆ†å—ç­–ç•¥

```python
# æŒ‰å¥å­åˆ†å—ï¼ˆæ›´è‡ªç„¶ï¼‰
sentences = re.split(r'([.!?ã€‚ï¼ï¼Ÿ]\s*)', response)
buffer = ""

for i in range(0, len(sentences)):
    buffer += sentences[i]

    # å®Œæ•´å¥å­ OR è¾¾åˆ°é˜ˆå€¼
    if (i % 2 == 1) or len(buffer) >= self.stream_flush_threshold:
        yield buffer
        buffer = ""
        await asyncio.sleep(0.01)  # é¿å…è¿‡å¿«åˆ·æ–°

if buffer:
    yield buffer
```

**é…ç½®å‚æ•°**ï¼š
```python
# .env æˆ– settings.py
STREAM_FLUSH_THRESHOLD = 100          # æ™®é€š Agent
DEEP_STREAM_FLUSH_THRESHOLD = 150     # Deep Research Agent
FUSION_STREAM_FLUSH_THRESHOLD = 200   # Fusion Agent
```

---

## ç¼“å­˜ç³»ç»Ÿ

### ç¼“å­˜æ¶æ„

```mermaid
graph TB
    subgraph ä¼šè¯ç¼“å­˜[ä¼šè¯ç¼“å­˜ Session Cache]
        SK[ç¼“å­˜é”®ç­–ç•¥<br/>ContextAwareCacheKeyStrategy]
        SM[å†…å­˜ç¼“å­˜<br/>200æ¡]
        SD[ç£ç›˜ç¼“å­˜<br/>2000æ¡]
    end

    subgraph å…¨å±€ç¼“å­˜[å…¨å±€ç¼“å­˜ Global Cache]
        GK[ç¼“å­˜é”®ç­–ç•¥<br/>GlobalCacheKeyStrategy]
        GM[å†…å­˜ç¼“å­˜<br/>500æ¡]
        GD[ç£ç›˜ç¼“å­˜<br/>5000æ¡]
    end

    Q[æŸ¥è¯¢] --> SK
    SK --> SM
    SM -->|æœªå‘½ä¸­| SD
    SD -->|æœªå‘½ä¸­| GK
    GK --> GM
    GM -->|æœªå‘½ä¸­| GD

    style SM fill:#e8f5e9
    style SD fill:#fff3e0
    style GM fill:#e3f2fd
    style GD fill:#fce4ec
```

### ç¼“å­˜é”®ç”Ÿæˆç­–ç•¥

**ä¼šè¯ç¼“å­˜ï¼ˆä¸Šä¸‹æ–‡æ„ŸçŸ¥ï¼‰**ï¼š
```python
class ContextAwareCacheKeyStrategy:
    def generate_key(self, query: str, thread_id: str, **kwargs) -> str:
        # è€ƒè™‘å¯¹è¯å†å²
        context_hash = self._hash_context(thread_id)
        query_hash = self._hash_query(query)

        # ç»„åˆé”®
        return f"{thread_id}:{context_hash}:{query_hash}"
```

**å…¨å±€ç¼“å­˜ï¼ˆä»…åŸºäºæŸ¥è¯¢ï¼‰**ï¼š
```python
class GlobalCacheKeyStrategy:
    def generate_key(self, query: str, **kwargs) -> str:
        # ä»…åŸºäºæŸ¥è¯¢æ–‡æœ¬
        return self._hash_query(query)
```

### ç¼“å­˜è´¨é‡æ§åˆ¶

```python
def mark_answer_quality(self, query: str, is_positive: bool, thread_id: str = "default"):
    """æ ‡è®°å›ç­”è´¨é‡ï¼Œç”¨äºç¼“å­˜è´¨é‡æ§åˆ¶"""
    keywords = self._extract_keywords(query)

    self.cache_manager.mark_quality(
        query.strip(),
        is_positive,
        thread_id=thread_id,
        low_level_keywords=keywords.get("low_level", []),
        high_level_keywords=keywords.get("high_level", [])
    )

# é«˜è´¨é‡ç¼“å­˜ä¼šè¿›å…¥"å¿«é€Ÿè·¯å¾„"ï¼Œè·³è¿‡éªŒè¯
```

### ç¼“å­˜å¤±æ•ˆ

```python
def clear_cache_for_query(self, query: str, thread_id: str = "default"):
    """æ¸…é™¤ç‰¹å®šæŸ¥è¯¢çš„ç¼“å­˜"""
    # æ¸…é™¤ä¼šè¯ç¼“å­˜
    self.cache_manager.delete(query.strip(), thread_id=thread_id)

    # æ¸…é™¤å…¨å±€ç¼“å­˜
    self.global_cache_manager.delete(query.strip())

    # å¼ºåˆ¶åˆ·æ–°å†™é˜Ÿåˆ—
    self.cache_manager.storage._flush_write_queue()
    self.global_cache_manager.storage._flush_write_queue()
```

---

## Agent å¯¹æ¯”

### åŠŸèƒ½å¯¹æ¯”è¡¨

| ç‰¹æ€§ | NaiveRagAgent | GraphAgent | HybridAgent | DeepResearchAgent | FusionGraphRAGAgent |
|------|---------------|------------|-------------|-------------------|---------------------|
| **æœç´¢ç­–ç•¥** | å‘é‡æ£€ç´¢ | å›¾è°±æ£€ç´¢ï¼ˆæœ¬åœ°+å…¨å±€ï¼‰ | å‘é‡+å›¾è°±æ··åˆ | å¤šæ­¥è¿­ä»£æœç´¢ | å¤šæ™ºèƒ½ä½“åä½œ |
| **æ¨ç†èƒ½åŠ›** | ä½ | ä¸­ | ä¸­é«˜ | é«˜ | æé«˜ |
| **å…³é”®è¯æå–** | æ—  | LLMæå– | å·¥å…·æå– | LLMæå– | å¤šé˜¶æ®µæå– |
| **å¤šè½®å¯¹è¯** | æ”¯æŒ | æ”¯æŒ | æ”¯æŒ | æ”¯æŒ | æ”¯æŒ |
| **æµå¼å“åº”** | æ”¯æŒ | æ”¯æŒ | æ”¯æŒ | æ”¯æŒ | æ”¯æŒ |
| **ç¼“å­˜æœºåˆ¶** | åŒå±‚ç¼“å­˜ | åŒå±‚ç¼“å­˜ | åŒå±‚ç¼“å­˜ | åŒå±‚ç¼“å­˜ | åŒå±‚ç¼“å­˜ |
| **å“åº”é€Ÿåº¦** | å¿« | ä¸­ | ä¸­ | æ…¢ | æ…¢ |
| **å‡†ç¡®ç‡** | ä½ | ä¸­é«˜ | é«˜ | é«˜ | æé«˜ |
| **èµ„æºæ¶ˆè€—** | ä½ | ä¸­ | ä¸­ | é«˜ | é«˜ |
| **é€‚ç”¨åœºæ™¯** | ç®€å•é—®ç­” | å®ä½“å…³ç³»æ¨ç† | é€šç”¨æŸ¥è¯¢ | å¤æ‚æ¨ç† | æŠ¥å‘Šç”Ÿæˆ |

### æ€§èƒ½å¯¹æ¯”

```mermaid
graph LR
    subgraph å“åº”é€Ÿåº¦[å“åº”é€Ÿåº¦ï¼ˆè¶ŠçŸ­è¶Šå¥½ï¼‰]
        N1[NaiveRag: 2s]
        G1[Graph: 4s]
        H1[Hybrid: 3s]
        D1[DeepResearch: 10s]
        F1[Fusion: 15s]
    end

    subgraph å‡†ç¡®ç‡[å‡†ç¡®ç‡ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰]
        N2[NaiveRag: 70%]
        G2[Graph: 85%]
        H2[Hybrid: 88%]
        D2[DeepResearch: 92%]
        F2[Fusion: 95%]
    end

    style N1 fill:#e8f5e9
    style F1 fill:#ffebee
    style N2 fill:#ffebee
    style F2 fill:#e8f5e9
```

### é€‰æ‹©å»ºè®®

```mermaid
graph TD
    Q{æŸ¥è¯¢ç±»å‹?}

    Q -->|ç®€å•äº‹å®æŸ¥è¯¢| N[NaiveRagAgent<br/>å¿«é€Ÿå“åº”]
    Q -->|å®ä½“å…³ç³»æŸ¥è¯¢| G[GraphAgent<br/>å›¾è°±æ¨ç†]
    Q -->|é€šç”¨æŸ¥è¯¢| H[HybridAgent<br/>å¹³è¡¡æ€§èƒ½]
    Q -->|å¤æ‚å¤šè·³æ¨ç†| D[DeepResearchAgent<br/>æ·±åº¦æ¢ç´¢]
    Q -->|æŠ¥å‘Šç”Ÿæˆ| F[FusionGraphRAGAgent<br/>å¤šæ™ºèƒ½ä½“åä½œ]

    style N fill:#e8f5e9
    style G fill:#e3f2fd
    style H fill:#fff3e0
    style D fill:#fce4ec
    style F fill:#ffe1f5
```

---

## å¼€å‘æŒ‡å—

### å¦‚ä½•åˆ›å»ºè‡ªå®šä¹‰ Agent

**æ­¥éª¤1ï¼šåˆ›å»º Agent ç±»**
```python
from graphrag_agent.agents.base import BaseAgent

class CustomAgent(BaseAgent):
    def __init__(self):
        # åˆå§‹åŒ–è‡ªå®šä¹‰å·¥å…·
        self.my_tool = MyCustomTool()

        # è°ƒç”¨çˆ¶ç±»æ„é€ å‡½æ•°
        super().__init__(cache_dir="./cache/custom_agent")
```

**æ­¥éª¤2ï¼šå®ç°å¿…éœ€æ–¹æ³•**
```python
    def _setup_tools(self) -> List:
        """è®¾ç½®å·¥å…·ï¼ˆå¿…éœ€ï¼‰"""
        return [
            self.my_tool.get_tool(),
        ]

    def _add_retrieval_edges(self, workflow):
        """æ·»åŠ æ£€ç´¢è¾¹ï¼ˆå¿…éœ€ï¼‰"""
        # ç®€å•å®ç°ï¼šç›´æ¥ä» retrieve åˆ° generate
        workflow.add_edge("retrieve", "generate")

        # å¤æ‚å®ç°ï¼šæ·»åŠ è‡ªå®šä¹‰èŠ‚ç‚¹
        # workflow.add_node("custom", self._custom_node)
        # workflow.add_edge("retrieve", "custom")
        # workflow.add_edge("custom", "generate")

    def _extract_keywords(self, query: str) -> Dict[str, List[str]]:
        """æå–å…³é”®è¯ï¼ˆå¿…éœ€ï¼‰"""
        # ä½¿ç”¨ LLM æˆ–å…¶ä»–æ–¹æ³•æå–
        return {"low_level": [...], "high_level": [...]}

    def _generate_node(self, state):
        """ç”Ÿæˆç­”æ¡ˆï¼ˆå¿…éœ€ï¼‰"""
        messages = state["messages"]
        question = messages[-3].content
        docs = messages[-1].content

        # ç¼“å­˜æ£€æŸ¥
        thread_id = state.get("configurable", {}).get("thread_id", "default")
        cached_result = self.cache_manager.get(question, thread_id=thread_id)
        if cached_result:
            return {"messages": [AIMessage(content=cached_result)]}

        # ä½¿ç”¨ LLM ç”Ÿæˆ
        prompt = ChatPromptTemplate.from_messages([
            ("system", SYSTEM_PROMPT),
            ("human", HUMAN_PROMPT),
        ])

        chain = prompt | self.llm | StrOutputParser()
        response = chain.invoke({"context": docs, "question": question})

        # æ›´æ–°ç¼“å­˜
        self.cache_manager.set(question, response, thread_id=thread_id)

        return {"messages": [AIMessage(content=response)]}
```

**æ­¥éª¤3ï¼šå®ç°æµå¼æ–¹æ³•ï¼ˆå¯é€‰ï¼‰**
```python
    async def _stream_process(self, inputs, config):
        """æµå¼å¤„ç†ï¼ˆå¯é€‰ï¼‰"""
        # å‚è€ƒ HybridAgent._stream_process å®ç°
        pass
```

**æ­¥éª¤4ï¼šä½¿ç”¨è‡ªå®šä¹‰ Agent**
```python
agent = CustomAgent()

# åŒæ­¥è°ƒç”¨
answer = agent.ask("ä½ çš„é—®é¢˜", thread_id="session_123")

# æµå¼è°ƒç”¨
async for chunk in agent.ask_stream("ä½ çš„é—®é¢˜", thread_id="session_123"):
    print(chunk, end="", flush=True)
```

### æ·»åŠ è‡ªå®šä¹‰èŠ‚ç‚¹

```python
def _add_retrieval_edges(self, workflow):
    # æ·»åŠ è‡ªå®šä¹‰èŠ‚ç‚¹
    workflow.add_node("preprocess", self._preprocess_node)
    workflow.add_node("postprocess", self._postprocess_node)

    # å®šä¹‰è¾¹
    workflow.add_edge("retrieve", "preprocess")
    workflow.add_edge("preprocess", "generate")
    workflow.add_edge("generate", "postprocess")
    workflow.add_edge("postprocess", END)

def _preprocess_node(self, state):
    """é¢„å¤„ç†èŠ‚ç‚¹"""
    messages = state["messages"]
    # å¯¹æ£€ç´¢ç»“æœè¿›è¡Œæ¸…æ´—ã€å»é‡ç­‰
    processed_docs = self._clean_documents(messages[-1].content)

    # æ›´æ–°æœ€åä¸€æ¡æ¶ˆæ¯
    messages[-1].content = processed_docs
    return {"messages": messages}

def _postprocess_node(self, state):
    """åå¤„ç†èŠ‚ç‚¹"""
    messages = state["messages"]
    # å¯¹ç”Ÿæˆçš„ç­”æ¡ˆè¿›è¡Œæ ¼å¼åŒ–
    formatted_answer = self._format_answer(messages[-1].content)

    messages[-1].content = formatted_answer
    return {"messages": messages}
```

### è°ƒè¯•æŠ€å·§

**1. å¯ç”¨æ‰§è¡Œæ—¥å¿—**
```python
result = agent.ask_with_trace("é—®é¢˜", thread_id="debug")
print(result["execution_log"])
# [
#     {"node": "extract_keywords", "input": "...", "output": {...}},
#     {"node": "agent", "input": [...], "output": ...},
#     ...
# ]
```

**2. å¯è§†åŒ–å·¥ä½œæµ**
```python
from IPython.display import Image, display

# ç”Ÿæˆå·¥ä½œæµå›¾
display(Image(agent.graph.get_graph().draw_mermaid_png()))
```

**3. æ€§èƒ½ç›‘æ§**
```python
print(agent.performance_metrics)
# {
#     "ask": {"total_duration": 5.2, "processing": 4.8},
#     "cache_check": {"duration": 0.05, "type": "miss"},
#     ...
# }
```

### å¸¸è§é—®é¢˜

**Q1: å¦‚ä½•ç¦ç”¨ç¼“å­˜ï¼Ÿ**
```python
agent = CustomAgent()
agent.cache_manager.enable_cache = False
agent.global_cache_manager.enable_cache = False
```

**Q2: å¦‚ä½•è°ƒæ•´é€’å½’é™åˆ¶ï¼Ÿ**
```python
# æ–¹å¼1ï¼šç¯å¢ƒå˜é‡
os.environ["AGENT_DEFAULT_RECURSION_LIMIT"] = "50"

# æ–¹å¼2ï¼šè°ƒç”¨æ—¶æŒ‡å®š
agent.ask("é—®é¢˜", recursion_limit=50)
```

**Q3: å¦‚ä½•å¤„ç†è¶…é•¿å¯¹è¯å†å²ï¼Ÿ**
```python
# å®šæœŸæ¸…ç†ä¼šè¯çŠ¶æ€
agent.memory.clear(config)

# æˆ–ä½¿ç”¨æ»‘åŠ¨çª—å£ï¼ˆä¿ç•™æœ€è¿‘ N æ¡æ¶ˆæ¯ï¼‰
```

---

## ç›¸å…³æ–‡æ¡£

- [çŸ¥è¯†å›¾è°±æ„å»º](./çŸ¥è¯†å›¾è°±æ„å»º.md) - äº†è§£çŸ¥è¯†å›¾è°±çš„æ„å»ºæµç¨‹
- [æœç´¢å¼•æ“](./æœç´¢å¼•æ“.md) - äº†è§£å„ç§æœç´¢ç­–ç•¥çš„å®ç°
- [ç³»ç»Ÿæ¶æ„æ€»è§ˆ](../01-æ•´ä½“æ¶æ„/ç³»ç»Ÿæ¶æ„æ€»è§ˆ.md) - äº†è§£æ•´ä½“æ¶æ„
- [LangGraph å®˜æ–¹æ–‡æ¡£](https://langchain-ai.github.io/langgraph/) - LangGraph æ¡†æ¶æ–‡æ¡£

---

## æ›´æ–°æ—¥å¿—

| ç‰ˆæœ¬ | æ—¥æœŸ | æ›´æ–°å†…å®¹ | ä½œè€… |
|------|------|----------|------|
| 1.0 | 2026-01-04 | åˆå§‹ç‰ˆæœ¬ï¼Œå®Œæ•´è¦†ç›–5ç§Agent | Claude |
| - | - | - | - |
