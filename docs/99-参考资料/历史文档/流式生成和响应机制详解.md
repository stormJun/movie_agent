# æµå¼ç”Ÿæˆå’Œå“åº”æœºåˆ¶è¯¦è§£

> æ³¨æ„ï¼šæœ¬æ–‡åŸºäºæ—§ç‰ˆæ¥å£ä¸ legacy æœåŠ¡ï¼ˆ`backend/application/services/chat_service.py` ç­‰ï¼‰ï¼Œå½“å‰å·²ä¸‹çº¿ï¼›ä»…ä¾›å†å²å‚è€ƒã€‚

**é—®é¢˜**: ç³»ç»Ÿå¦‚ä½•å®ç°æµå¼ç”Ÿæˆå’Œå“åº”ï¼Ÿ

---

## ğŸ¯ æ ¸å¿ƒæ¦‚å¿µ

### ä»€ä¹ˆæ˜¯æµå¼å“åº”ï¼Ÿ

æµå¼å“åº”æ˜¯æŒ‡**æœåŠ¡å™¨é€æ­¥å‘é€æ•°æ®**ï¼Œå®¢æˆ·ç«¯**å®æ—¶æ¥æ”¶å¹¶æ¸²æŸ“**ï¼Œè€Œä¸æ˜¯ç­‰å¾…å®Œæ•´å“åº”åä¸€æ¬¡æ€§å±•ç¤ºã€‚

**å¯¹æ¯”**:

```
ã€éæµå¼ã€‘
ç”¨æˆ·æé—® â†’ ç­‰å¾… 10 ç§’ â†’ ä¸€æ¬¡æ€§å±•ç¤ºå®Œæ•´ç­”æ¡ˆ
                  â†‘
            ç”¨æˆ·ç„¦è™‘ç­‰å¾…

ã€æµå¼ã€‘
ç”¨æˆ·æé—® â†’ 1ç§’åçœ‹åˆ°ç¬¬ä¸€å¥ â†’ 2ç§’çœ‹åˆ°ç¬¬äºŒå¥ â†’ ... â†’ 10ç§’çœ‹åˆ°å®Œæ•´ç­”æ¡ˆ
                  â†‘              â†‘              â†‘
              é€æ­¥æ¸²æŸ“ï¼Œç”¨æˆ·ä½“éªŒæ›´å¥½
```

### ä¸ºä»€ä¹ˆéœ€è¦æµå¼å“åº”ï¼Ÿ

âœ… **æ›´å¥½çš„ç”¨æˆ·ä½“éªŒ**: ç”¨æˆ·èƒ½ç«‹å³çœ‹åˆ°å›ç­”å¼€å§‹ç”Ÿæˆ
âœ… **é™ä½ç„¦è™‘æ„Ÿ**: æµå¼è¾“å‡ºè®©ç”¨æˆ·çŸ¥é“ç³»ç»Ÿåœ¨å·¥ä½œ
âœ… **æ”¯æŒé•¿å›ç­”**: å¯ä»¥æ¸è¿›å¼å±•ç¤ºé•¿ç¯‡å›ç­”
âœ… **å®æ—¶åé¦ˆ**: ç”¨æˆ·å¯ä»¥æå‰åˆ¤æ–­å›ç­”æ–¹å‘

---

## ğŸ“Š æŠ€æœ¯æ ˆ

| å±‚çº§ | æŠ€æœ¯ | ä½œç”¨ |
|------|------|------|
| **åè®®** | SSE (Server-Sent Events) | æœåŠ¡å™¨å‘å®¢æˆ·ç«¯æ¨é€äº‹ä»¶æµ |
| **åç«¯æ¡†æ¶** | FastAPI + StreamingResponse | ç”Ÿæˆæµå¼ HTTP å“åº” |
| **Python è¯­æ³•** | AsyncGenerator | å¼‚æ­¥ç”Ÿæˆå™¨ï¼Œé€æ­¥ yield æ•°æ® |
| **å‰ç«¯ï¼ˆReactï¼‰** | Fetch API + ReadableStream | è¯»å–æµå¼å“åº” |
| **å‰ç«¯ï¼ˆStreamlitï¼‰** | sseclient + requests | è§£æ SSE äº‹ä»¶æµ |
| **Agent å±‚** | LangGraph + MemorySaver | å·¥ä½œæµå¼•æ“ + çŠ¶æ€ç®¡ç† |

---

## ğŸ”„ å®Œæ•´æ•°æ®æµå‘

```
ã€ç”¨æˆ·ç‚¹å‡»å‘é€ã€‘
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ã€å‰ç«¯ã€‘React / Streamlit                               â”‚
â”‚                                                         â”‚
â”‚  React: fetch() + ReadableStream                       â”‚
â”‚  Streamlit: requests.post(..., stream=True)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â”‚ POST /api/v1/chat/stream
    â”‚ Content-Type: backend/application/json
    â”‚ Accept: text/event-stream
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ã€è·¯ç”±å±‚ã€‘FastAPI Router                                â”‚
â”‚                                                         â”‚
â”‚  @router.post("/chat/stream")                          â”‚
â”‚  async def chat_stream(request: Request):              â”‚
â”‚      async def event_generator():                      â”‚
â”‚          yield "data: {...}\n\n"  â† SSE æ ¼å¼           â”‚
â”‚      return StreamingResponse(                         â”‚
â”‚          event_generator(),                            â”‚
â”‚          media_type="text/event-stream"                â”‚
â”‚      )                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â”‚ async for chunk in process_chat_stream(...)
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ã€æœåŠ¡å±‚ã€‘Chat Service                                  â”‚
â”‚                                                         â”‚
â”‚  async def process_chat_stream(...):                   â”‚
â”‚      async for chunk in agent.ask_stream(...):         â”‚
â”‚          yield json.dumps({"status": "token",          â”‚
â”‚                           "content": chunk})           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â”‚ async for chunk in self._stream_process(...)
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ã€Agent å±‚ã€‘BaseAgent                                   â”‚
â”‚                                                         â”‚
â”‚  async def ask_stream(query, thread_id):               â”‚
â”‚      # 1. æ£€æŸ¥ç¼“å­˜                                      â”‚
â”‚      cached_result = self.check_fast_cache(...)        â”‚
â”‚      if cached_result:                                 â”‚
â”‚          # åˆ†å—è¿”å›ç¼“å­˜ç»“æœ                             â”‚
â”‚          chunks = re.split(r'([.!?ã€‚ï¼ï¼Ÿ]\s*)', ...)   â”‚
â”‚          for chunk in chunks:                          â”‚
â”‚              yield chunk                               â”‚
â”‚              await asyncio.sleep(0.01)                 â”‚
â”‚          return                                        â”‚
â”‚                                                         â”‚
â”‚      # 2. æœªå‘½ä¸­ç¼“å­˜ï¼Œæ‰§è¡Œæµå¼å¤„ç†                       â”‚
â”‚      async for chunk in self._stream_process(...):     â”‚
â”‚          yield chunk                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â”‚ å®é™…ä¸Šæ˜¯æ¨¡æ‹Ÿæµå¼ï¼ˆä¼ªæµå¼ï¼‰
    â”‚ å› ä¸º LangGraph ç›®å‰ä¸æ”¯æŒçœŸæ­£çš„æµå¼ç”Ÿæˆ
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ã€æ¨¡æ‹Ÿæµå¼ã€‘åˆ†å—è¾“å‡ºå®Œæ•´ç­”æ¡ˆ                             â”‚
â”‚                                                         â”‚
â”‚  1. æ‰§è¡Œå®Œæ•´çš„ graph.stream(...)                        â”‚
â”‚  2. è·å–å®Œæ•´ç­”æ¡ˆ                                         â”‚
â”‚  3. æŒ‰å­—ç¬¦/å¥å­åˆ†å—                                      â”‚
â”‚  4. é€å— yield                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â”‚ yield chunk (æ¯æ¬¡ 10-50 ä¸ªå­—ç¬¦)
    â”‚
    â–¼
ã€å‰ç«¯é€æ­¥æ¸²æŸ“ã€‘
    â”œâ”€ ç¬¬ 1 æ¬¡ yield: "æ ¹æ®"
    â”œâ”€ ç¬¬ 2 æ¬¡ yield: "è§„å®šï¼Œ"
    â”œâ”€ ç¬¬ 3 æ¬¡ yield: "æ—·è¯¾ç´¯è®¡"
    â”œâ”€ ç¬¬ 4 æ¬¡ yield: "è¾¾åˆ°40å­¦æ—¶..."
    â””â”€ ...
```

---

## ğŸ” è¯¦ç»†ä»£ç åˆ†æ

### 1ï¸âƒ£ è·¯ç”±å±‚ï¼šFastAPI StreamingResponse

**æ–‡ä»¶**: `backend/server/api/rest/v1/chat_stream.py`

```python
@router.post("/chat/stream")
async def chat_stream(request: Request):
    """æµå¼å“åº”èŠå¤©è¯·æ±‚"""

    # 1. è§£æè¯·æ±‚å‚æ•°
    data = await request.json()
    message = data.get("message")
    session_id = data.get("session_id")
    agent_type = data.get("agent_type", "hybrid_agent")
    # ...

    # 2. ğŸ”‘ å®šä¹‰å¼‚æ­¥ç”Ÿæˆå™¨å‡½æ•°
    async def event_generator():
        """
        è¿™ä¸ªå‡½æ•°æ˜¯ä¸€ä¸ªå¼‚æ­¥ç”Ÿæˆå™¨ (AsyncGenerator)
        é€šè¿‡ yield é€æ­¥è¿”å›æ•°æ®
        """
        try:
            # å‘é€å¼€å§‹äº‹ä»¶
            yield "data: " + json.dumps({"status": "start"}) + "\n\n"
            #      â†‘ SSE æ ¼å¼: data: <JSON>\n\n

            # 3. å¾ªç¯å¤„ç†æµå¼è¾“å‡º
            async for chunk in process_chat_stream(
                message=message,
                session_id=session_id,
                agent_type=agent_type,
                # ...
            ):
                # æ£€æŸ¥ chunk ç±»å‹
                if isinstance(chunk, dict):
                    # å­—å…¸ç±»å‹ï¼ˆåŒ…å« status ä¿¡æ¯ï¼‰
                    yield "data: " + json.dumps(chunk) + "\n\n"
                else:
                    # æ™®é€šæ–‡æœ¬å—
                    yield "data: " + json.dumps({
                        "status": "token",
                        "content": chunk
                    }) + "\n\n"

            # å‘é€å®Œæˆäº‹ä»¶
            yield "data: " + json.dumps({"status": "done"}) + "\n\n"

        except Exception as e:
            # å‘é€é”™è¯¯äº‹ä»¶
            yield "data: " + json.dumps({
                "status": "error",
                "message": str(e)
            }) + "\n\n"

    # 4. ğŸ”‘ è¿”å› StreamingResponse
    return StreamingResponse(
        event_generator(),  # â† ä¼ å…¥å¼‚æ­¥ç”Ÿæˆå™¨
        media_type="text/event-stream",  # â† SSE åª’ä½“ç±»å‹
        headers={
            "Cache-Control": "no-cache",  # â† ç¦ç”¨ç¼“å­˜
            "Connection": "keep-alive",   # â† ä¿æŒè¿æ¥
            "X-Accel-Buffering": "no"     # â† é˜»æ­¢ Nginx ç¼“å†²
        }
    )
```

**å…³é”®ç‚¹**:
- âœ… **AsyncGenerator**: `async def event_generator()` + `yield`
- âœ… **SSE æ ¼å¼**: `data: <JSON>\n\n`
- âœ… **StreamingResponse**: FastAPI çš„æµå¼å“åº”ç±»
- âœ… **HTTP Headers**: è®¾ç½®æ­£ç¡®çš„ç¼“å­˜å’Œè¿æ¥å¤´

---

### 2ï¸âƒ£ æœåŠ¡å±‚ï¼šprocess_chat_stream

**æ–‡ä»¶**: `backend/application/services/chat_service.py:202-359`

```python
async def process_chat_stream(
    message: str,
    session_id: str,
    agent_type: str,
    # ...
) -> AsyncGenerator[str, None]:
    """å¤„ç†èŠå¤©è¯·æ±‚ï¼Œè¿”å›æµå¼è¾“å‡º"""

    # 1. å¹¶å‘æ§åˆ¶ï¼ˆè·å–é”ï¼‰
    lock_key = f"{session_id}_chat"
    lock_acquired = chat_manager.try_acquire_lock(lock_key)
    if not lock_acquired:
        yield json.dumps({"status": "error", "message": "è¯·æ±‚æ­£åœ¨å¤„ç†"})
        return

    try:
        # 2. è·å– Agent å®ä¾‹
        selected_agent = agent_manager.get_agent(agent_type, session_id)

        # 3. å¿«é€Ÿç¼“å­˜æ£€æŸ¥
        fast_result = selected_agent.check_fast_cache(message, session_id)
        if fast_result:
            # ç¼“å­˜å‘½ä¸­ï¼Œç›´æ¥è¿”å›
            yield json.dumps({"status": "token", "content": fast_result})
            yield json.dumps({"status": "done"})
            return

        # 4. ğŸ”‘ è°ƒç”¨ Agent çš„æµå¼æ–¹æ³•
        async for chunk in selected_agent.ask_stream(
            message,
            thread_id=session_id
        ):
            # é€å— yield ç»™è·¯ç”±å±‚
            yield json.dumps({"status": "token", "content": chunk})

        # 5. å‘é€å®Œæˆä¿¡å·
        yield json.dumps({"status": "done"})

    finally:
        # 6. é‡Šæ”¾é”
        chat_manager.release_lock(lock_key)
```

**å…³é”®ç‚¹**:
- âœ… **AsyncGenerator ç±»å‹æ³¨è§£**: `-> AsyncGenerator[str, None]`
- âœ… **async for**: å¼‚æ­¥è¿­ä»£ Agent çš„è¾“å‡º
- âœ… **yield**: é€å—è¿”å›æ•°æ®
- âœ… **finally**: ç¡®ä¿é”è¢«é‡Šæ”¾

---

### 3ï¸âƒ£ Agent å±‚ï¼šask_stream

**æ–‡ä»¶**: `backend/graphrag_agent/agents/base.py:936-1068`

```python
async def ask_stream(
    self,
    query: str,
    thread_id: str = "default",
    recursion_limit: Optional[int] = None
) -> AsyncGenerator[str, None]:
    """å‘ Agent æé—®ï¼Œè¿”å›æµå¼å“åº”"""

    safe_query = query.strip()

    # 1. æ£€æŸ¥å…¨å±€ç¼“å­˜
    global_result = self.global_cache_manager.get(safe_query)
    if global_result:
        # ğŸ”‘ åˆ†å—è¿”å›ç¼“å­˜ç»“æœï¼ˆæ¨¡æ‹Ÿæµå¼ï¼‰
        import re
        chunks = re.split(r'([.!?ã€‚ï¼ï¼Ÿ]\s*)', global_result)
        #                â†‘ æŒ‰å¥å­åˆ†å‰²ï¼ˆåŒ…å«æ ‡ç‚¹ç¬¦å·ï¼‰
        buffer = ""

        for i in range(0, len(chunks)):
            buffer += chunks[i]

            # å½“ç¼“å†²åŒºåŒ…å«å®Œæ•´å¥å­æˆ–è¾¾åˆ°é˜ˆå€¼æ—¶è¾“å‡º
            if (i % 2 == 1) or len(buffer) >= self.stream_flush_threshold:
                yield buffer  # â† è¿”å›ä¸€å—æ–‡æœ¬
                buffer = ""
                await asyncio.sleep(0.01)  # â† æ¨¡æ‹Ÿå»¶è¿Ÿ

        # è¾“å‡ºå‰©ä½™å†…å®¹
        if buffer:
            yield buffer
        return

    # 2. æ£€æŸ¥å¿«é€Ÿç¼“å­˜ï¼ˆåŒæ ·åˆ†å—è¿”å›ï¼‰
    fast_result = self.check_fast_cache(safe_query, thread_id)
    if fast_result:
        # åŒæ ·çš„åˆ†å—é€»è¾‘
        # ...
        return

    # 3. æ£€æŸ¥ä¼šè¯ç¼“å­˜
    cached_response = self.cache_manager.get(safe_query, thread_id=thread_id)
    if cached_response:
        # åŒæ ·çš„åˆ†å—é€»è¾‘
        # ...
        return

    # 4. æœªå‘½ä¸­ç¼“å­˜ï¼Œæ‰§è¡Œæµå¼å¤„ç†
    config = {
        "configurable": {
            "thread_id": thread_id,
            "recursion_limit": recursion_value,
            "stream_mode": True  # æ ‡è®°ä¸ºæµå¼æ¨¡å¼
        }
    }

    inputs = {"messages": [HumanMessage(content=query)]}
    answer = ""

    try:
        # 5. ğŸ”‘ è°ƒç”¨å­ç±»çš„ _stream_process æ–¹æ³•
        async for chunk in self._stream_process(inputs, config):
            yield chunk  # â† é€å—è¿”å›
            answer += chunk  # â† åŒæ—¶æ”¶é›†å®Œæ•´ç­”æ¡ˆ

        # 6. ç¼“å­˜å®Œæ•´ç­”æ¡ˆ
        if answer and len(answer) > 10:
            self.cache_manager.set(safe_query, answer, thread_id=thread_id)
            self.global_cache_manager.set(safe_query, answer)

    except Exception as e:
        error_msg = f"å¤„ç†æŸ¥è¯¢æ—¶å‡ºé”™: {str(e)}"
        yield error_msg
```

**å…³é”®ç‚¹**:
- âœ… **ç¼“å­˜å‘½ä¸­åˆ†å—è¿”å›**: æŒ‰å¥å­åˆ†å‰²ï¼Œæ¨¡æ‹Ÿæµå¼
- âœ… **re.split()**: ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŒ‰æ ‡ç‚¹ç¬¦å·åˆ†å‰²
- âœ… **asyncio.sleep()**: æ·»åŠ å°å»¶è¿Ÿï¼Œæ¨¡æ‹Ÿæµå¼æ•ˆæœ
- âœ… **buffer ç®¡ç†**: æ§åˆ¶æ¯æ¬¡ yield çš„æ•°æ®é‡

---

### 4ï¸âƒ£ ä¼ªæµå¼å®ç°ï¼š_stream_process

**æ–‡ä»¶**: `backend/graphrag_agent/agents/base.py:309-358`

```python
async def _stream_process(
    self,
    inputs: Dict[str, Any],
    config: Dict[str, Any]
) -> AsyncGenerator[str, None]:
    """
    æ‰§è¡Œæµå¼å¤„ç†çš„é»˜è®¤å®ç°

    âš ï¸ æ³¨æ„ï¼šè¿™æ˜¯ä¼ªæµå¼å®ç°
    ç”±äº LangGraph ç›®å‰ä¸æ”¯æŒçœŸæ­£çš„æµå¼ç”Ÿæˆï¼Œ
    è¿™é‡Œå…ˆæ‰§è¡Œå®Œæ•´æµç¨‹ï¼Œç„¶ååˆ†å—è¿”å›
    """

    # 1. è·å–æŸ¥è¯¢
    messages = inputs.get("messages", [])
    query = messages[-1].content if messages else ""

    # 2. ğŸ”‘ æ‰§è¡Œå®Œæ•´çš„å·¥ä½œæµï¼ˆéæµå¼ï¼‰
    result = ""
    for output in self.graph.stream(inputs, config=config):
        # LangGraph é€æ­¥æ‰§è¡Œæ¯ä¸ªèŠ‚ç‚¹
        # ä½†æ¯ä¸ªèŠ‚ç‚¹æ˜¯å®Œæ•´æ‰§è¡Œçš„ï¼Œä¸æ˜¯é€ token ç”Ÿæˆ
        pass

    # 3. ä» memory è·å–å®Œæ•´ç­”æ¡ˆ
    chat_history = self.memory.get(config)["channel_values"]["messages"]
    result = chat_history[-1].content

    # 4. ğŸ”‘ åˆ†å—è¿”å›ï¼ˆæ¨¡æ‹Ÿæµå¼ï¼‰
    chunk_size = 10  # æ¯å— 10 ä¸ªå­—ç¬¦

    for i in range(0, len(result), chunk_size):
        chunk = result[i:i+chunk_size]
        yield chunk  # â† é€å—è¿”å›
        await asyncio.sleep(0.01)  # â† æ¨¡æ‹Ÿå»¶è¿Ÿ

    # å¯é€‰ï¼šå‰©ä½™éƒ¨åˆ†
    if result:
        yield ""  # ç¡®ä¿å®Œæˆ
```

**ä¸ºä»€ä¹ˆæ˜¯ä¼ªæµå¼ï¼Ÿ**

```python
# çœŸæ­£çš„æµå¼ï¼ˆç†æƒ³æƒ…å†µï¼ŒLangGraph æœªæ¥å¯èƒ½æ”¯æŒï¼‰
async for token in llm.astream(prompt):
    yield token  # â† é€ token ç”Ÿæˆ
    # "æ ¹" â†’ "æ®" â†’ "è§„" â†’ "å®š" â†’ ...

# å½“å‰çš„ä¼ªæµå¼ï¼ˆå®é™…å®ç°ï¼‰
answer = llm.invoke(prompt)  # â† å…ˆç”Ÿæˆå®Œæ•´ç­”æ¡ˆ
for chunk in split_into_chunks(answer):
    yield chunk  # â† ååˆ†å—è¿”å›
    # "æ ¹æ®è§„å®š" â†’ "ï¼Œæ—·è¯¾" â†’ "ç´¯è®¡è¾¾åˆ°" â†’ ...
```

**é™åˆ¶**:
- âŒ ç”¨æˆ·ä»éœ€ç­‰å¾…å®Œæ•´ç­”æ¡ˆç”Ÿæˆå®Œæ¯•
- âŒ æ— æ³•ä¸­é€”åœæ­¢ç”Ÿæˆ
- âœ… ä½†å‰ç«¯ä»èƒ½è·å¾—æµå¼æ¸²æŸ“çš„ä½“éªŒ

---

## ğŸŒ å‰ç«¯å®ç°

### React å‰ç«¯ï¼šFetch API + ReadableStream

**æ–‡ä»¶**: `frontend-react/src/utils/sse.ts:16-63`

```typescript
export async function postSseJson(
  url: string,
  body: unknown,
  onEvent: (event: StreamEvent) => void,
  options?: { signal?: AbortSignal; baseUrl?: string },
): Promise<void> {
  const baseUrl = options?.baseUrl ?? getApiBaseUrl();

  // 1. å‘èµ· Fetch è¯·æ±‚
  const response = await fetch(`${baseUrl}${url}`, {
    method: "POST",
    headers: {
      "Content-Type": "backend/application/json",
      Accept: "text/event-stream",  // â† æŒ‡å®šæ¥å— SSE
    },
    body: JSON.stringify(body),
    signal: options?.signal,  // â† æ”¯æŒä¸­æ–­
  });

  if (!response.ok || !response.body) {
    throw new Error(`SSE request failed: ${response.status}`);
  }

  // 2. ğŸ”‘ è·å– ReadableStream
  const reader = response.body.getReader();
  const decoder = new TextDecoder("utf-8");
  let buffer = "";

  // 3. å¾ªç¯è¯»å–æµ
  while (true) {
    const { done, value } = await reader.read();
    if (done) break;  // â† æµç»“æŸ

    // 4. è§£ç å­—èŠ‚æµ
    buffer += decoder.decode(value, { stream: true });

    // 5. è§£æ SSE å¸§
    const { frames, rest } = parseSseFrames(buffer);
    buffer = rest;

    // 6. å¤„ç†æ¯ä¸€å¸§
    for (const frame of frames) {
      const dataLines = extractDataLines(frame);
      for (const data of dataLines) {
        if (!data) continue;
        try {
          // 7. è§£æ JSON å¹¶è§¦å‘å›è°ƒ
          onEvent(JSON.parse(data) as StreamEvent);
        } catch {
          onEvent({ status: "error", message: "Invalid JSON" });
        }
      }
    }
  }
}

// è¾…åŠ©å‡½æ•°ï¼šè§£æ SSE å¸§
function parseSseFrames(buffer: string): { frames: string[]; rest: string } {
  // SSE æ ¼å¼: æ¯ä¸ªäº‹ä»¶ä»¥ "\n\n" åˆ†éš”
  const parts = buffer.split("\n\n");
  return {
    frames: parts.slice(0, -1),  // å®Œæ•´çš„å¸§
    rest: parts[parts.length - 1] || ""  // ä¸å®Œæ•´çš„å¸§ï¼ˆç•™å¾…ä¸‹æ¬¡ï¼‰
  };
}

// è¾…åŠ©å‡½æ•°ï¼šæå– data: è¡Œ
function extractDataLines(frame: string): string[] {
  return frame
    .split("\n")
    .filter((line) => line.startsWith("data:"))
    .map((line) => line.slice("data:".length).trimStart());
}
```

**ä½¿ç”¨ç¤ºä¾‹**:

```typescript
// åœ¨ React ç»„ä»¶ä¸­
const handleSendMessage = async () => {
  let fullResponse = "";

  await postSseJson(
    "/api/v1/chat/stream",
    {
      message: "ä»€ä¹ˆæ˜¯ GraphRAG?",
      session_id: "abc123",
      agent_type: "hybrid_agent",
    },
    (event) => {
      // å¤„ç†æ¯ä¸ªäº‹ä»¶
      if (event.status === "token") {
        fullResponse += event.content;
        setMessages((prev) => [...prev, { role: "ai", content: fullResponse }]);
      } else if (event.status === "done") {
        console.log("å®Œæˆ");
      } else if (event.status === "error") {
        console.error(event.message);
      }
    },
    { signal: abortController.signal }  // æ”¯æŒä¸­æ–­
  );
};
```

**å…³é”®ç‚¹**:
- âœ… **ReadableStream**: æµè§ˆå™¨åŸç”Ÿæµå¼è¯»å– API
- âœ… **TextDecoder**: å­—èŠ‚æµè§£ç ä¸ºæ–‡æœ¬
- âœ… **å¸§è§£æ**: å¤„ç† SSE çš„ `\n\n` åˆ†éš”ç¬¦
- âœ… **AbortController**: æ”¯æŒç”¨æˆ·ä¸­æ–­è¯·æ±‚

---

### Streamlit å‰ç«¯ï¼šsseclient + requests

**æ–‡ä»¶**: `frontend/utils/api.py:74-149`

```python
def send_message_stream(message: str, on_token: Callable[[str, bool], None]) -> str:
    """å‘ FastAPI åç«¯å‘é€èŠå¤©æ¶ˆæ¯ï¼Œè·å–æµå¼å“åº”"""

    # 1. æ„å»ºè¯·æ±‚å‚æ•°
    params = {
        "message": message,
        "session_id": st.session_state.session_id,
        "agent_type": st.session_state.agent_type,
        # ...
    }

    # 2. å¯¼å…¥ SSE å®¢æˆ·ç«¯
    import sseclient
    import requests
    import json

    # 3. ğŸ”‘ å‘èµ·æµå¼è¯·æ±‚
    response = requests.post(
        f"{API_URL}/api/v1/chat/stream",
        json=params,
        stream=True,  # â† å…³é”®ï¼šå¯ç”¨æµå¼æ¨¡å¼
        headers={"Accept": "text/event-stream"}
    )

    # 4. åˆ›å»º SSE å®¢æˆ·ç«¯
    client = sseclient.SSEClient(response)

    # 5. å¤„ç†æ¯ä¸ªäº‹ä»¶
    thinking_content = ""

    for event in client.events():
        try:
            # 6. è§£æ JSON
            data = json.loads(event.data)

            # 7. å¤„ç†ä¸åŒäº‹ä»¶ç±»å‹
            if data.get("status") == "token":
                # æ™®é€š token
                on_token(data.get("content", ""))

            elif data.get("status") == "thinking":
                # æ€è€ƒè¿‡ç¨‹
                chunk = data.get("content", "")
                thinking_content += chunk
                on_token(chunk, is_thinking=True)

            elif data.get("status") == "done":
                # å®Œæˆ
                break

            elif data.get("status") == "error":
                # é”™è¯¯
                st.error(data.get("message", "æœªçŸ¥é”™è¯¯"))
                break

        except json.JSONDecodeError as e:
            print(f"JSON è§£æé”™è¯¯: {e}")
            continue

    return thinking_content
```

**ä½¿ç”¨ç¤ºä¾‹**:

```python
# åœ¨ Streamlit ç»„ä»¶ä¸­
with st.chat_message("assistant"):
    message_placeholder = st.empty()
    full_response = ""

    # å®šä¹‰ token å¤„ç†å›è°ƒ
    def on_token(content: str, is_thinking: bool = False):
        nonlocal full_response
        full_response += content
        message_placeholder.markdown(full_response + "â–Œ")

    # å‘èµ·æµå¼è¯·æ±‚
    thinking = send_message_stream(prompt, on_token)

    # æœ€ç»ˆæ¸²æŸ“
    message_placeholder.markdown(full_response)
```

**å…³é”®ç‚¹**:
- âœ… **requests.post(stream=True)**: å¯ç”¨æµå¼æ¨¡å¼
- âœ… **sseclient**: ç¬¬ä¸‰æ–¹åº“ï¼Œä¸“é—¨è§£æ SSE
- âœ… **å›è°ƒå‡½æ•°**: `on_token` é€æ­¥æ›´æ–° UI
- âœ… **Streamlit placeholder**: ç”¨äºå®æ—¶æ›´æ–°æ˜¾ç¤º

---

## ğŸ“¡ SSE (Server-Sent Events) åè®®

### åè®®æ ¼å¼

```
data: {"status": "start"}\n\n
data: {"status": "token", "content": "æ ¹æ®"}\n\n
data: {"status": "token", "content": "è§„å®š"}\n\n
data: {"status": "token", "content": "ï¼Œæ—·è¯¾"}\n\n
data: {"status": "done"}\n\n
```

**è§„åˆ™**:
- âœ… æ¯ä¸ªäº‹ä»¶ä»¥ `data:` å¼€å¤´
- âœ… æ¯ä¸ªäº‹ä»¶ä»¥ `\n\n`ï¼ˆä¸¤ä¸ªæ¢è¡Œç¬¦ï¼‰ç»“å°¾
- âœ… å¯ä»¥æœ‰å¤šè¡Œ `data:`
- âœ… æ”¯æŒå…¶ä»–å­—æ®µï¼š`event:`, `id:`, `retry:`

**å®Œæ•´ç¤ºä¾‹**:

```
event: message
id: 1
data: {"status": "token", "content": "ä½ å¥½"}

event: message
id: 2
data: {"status": "token", "content": "ä¸–ç•Œ"}

event: done
data: {"status": "done"}
```

### HTTP å“åº”å¤´

```http
HTTP/1.1 200 OK
Content-Type: text/event-stream
Cache-Control: no-cache
Connection: keep-alive
X-Accel-Buffering: no
```

**å…³é”®å¤´éƒ¨**:
- âœ… `Content-Type: text/event-stream`: SSE åª’ä½“ç±»å‹
- âœ… `Cache-Control: no-cache`: ç¦ç”¨ç¼“å­˜
- âœ… `Connection: keep-alive`: ä¿æŒ HTTP è¿æ¥
- âœ… `X-Accel-Buffering: no`: ç¦æ­¢ Nginx ç¼“å†²

---

## ğŸ Python AsyncGenerator è¯¦è§£

### ä»€ä¹ˆæ˜¯ AsyncGeneratorï¼Ÿ

**AsyncGenerator** æ˜¯ Python çš„**å¼‚æ­¥ç”Ÿæˆå™¨**ï¼Œç»“åˆäº†ï¼š
- **ç”Ÿæˆå™¨ (Generator)**: ä½¿ç”¨ `yield` é€æ­¥è¿”å›å€¼
- **å¼‚æ­¥ (Async)**: æ”¯æŒ `await` å’Œ `async for`

### æ™®é€šç”Ÿæˆå™¨ vs å¼‚æ­¥ç”Ÿæˆå™¨

```python
# æ™®é€šç”Ÿæˆå™¨ (Generator)
def normal_gen():
    yield 1
    yield 2
    yield 3

for item in normal_gen():
    print(item)  # 1, 2, 3

# å¼‚æ­¥ç”Ÿæˆå™¨ (AsyncGenerator)
async def async_gen():
    yield 1
    await asyncio.sleep(0.1)
    yield 2
    await asyncio.sleep(0.1)
    yield 3

async for item in async_gen():
    print(item)  # 1, 2, 3 (å¸¦å»¶è¿Ÿ)
```

### ç±»å‹æ³¨è§£

```python
from typing import AsyncGenerator

async def stream_data() -> AsyncGenerator[str, None]:
    #                      â†‘ ç”Ÿæˆçš„ç±»å‹ â†‘ Send ç±»å‹ï¼ˆé€šå¸¸æ˜¯ Noneï¼‰
    yield "ç¬¬ä¸€å—"
    yield "ç¬¬äºŒå—"
    yield "ç¬¬ä¸‰å—"
```

### å®é™…åº”ç”¨

```python
async def event_generator() -> AsyncGenerator[str, None]:
    """SSE äº‹ä»¶ç”Ÿæˆå™¨"""

    # å‘é€å¼€å§‹äº‹ä»¶
    yield "data: " + json.dumps({"status": "start"}) + "\n\n"

    # å¤„ç†æµå¼è¾“å‡º
    async for chunk in process_stream():
        yield "data: " + json.dumps({
            "status": "token",
            "content": chunk
        }) + "\n\n"
        await asyncio.sleep(0.01)  # æ¨¡æ‹Ÿå»¶è¿Ÿ

    # å‘é€ç»“æŸäº‹ä»¶
    yield "data: " + json.dumps({"status": "done"}) + "\n\n"

# FastAPI ä½¿ç”¨
return StreamingResponse(
    event_generator(),  # â† ä¼ å…¥å¼‚æ­¥ç”Ÿæˆå™¨
    media_type="text/event-stream"
)
```

---

## âš™ï¸ æµå¼å¤„ç†æµç¨‹å›¾

### å®Œæ•´æ—¶åºå›¾

```
ã€ç”¨æˆ·ã€‘         ã€å‰ç«¯ã€‘       ã€è·¯ç”±ã€‘      ã€æœåŠ¡ã€‘      ã€Agentã€‘
  â”‚               â”‚             â”‚             â”‚             â”‚
  â”‚ ç‚¹å‡»å‘é€      â”‚             â”‚             â”‚             â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚             â”‚             â”‚             â”‚
  â”‚               â”‚ POST /api/v1/chat/stream         â”‚             â”‚
  â”‚               â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚             â”‚             â”‚
  â”‚               â”‚             â”‚ async for  â”‚             â”‚
  â”‚               â”‚             â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚             â”‚
  â”‚               â”‚             â”‚             â”‚ ask_stream â”‚
  â”‚               â”‚             â”‚             â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚
  â”‚               â”‚             â”‚             â”‚             â”‚
  â”‚               â”‚             â”‚             â”‚ æ£€æŸ¥ç¼“å­˜   â”‚
  â”‚               â”‚             â”‚             â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚               â”‚             â”‚             â”‚ æœªå‘½ä¸­     â”‚
  â”‚               â”‚             â”‚             â”‚             â”‚
  â”‚               â”‚             â”‚             â”‚ graph.stream()
  â”‚               â”‚             â”‚             â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚               â”‚             â”‚             â”‚ å®Œæ•´ç­”æ¡ˆ   â”‚
  â”‚               â”‚             â”‚             â”‚             â”‚
  â”‚               â”‚             â”‚             â”‚ åˆ†å—å¤„ç†   â”‚
  â”‚               â”‚             â”‚             â”‚             â”‚
  â”‚               â”‚             â”‚             â”‚ yield "æ ¹æ®"
  â”‚               â”‚             â”‚             â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚               â”‚             â”‚ yield token â”‚             â”‚
  â”‚               â”‚             â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤             â”‚
  â”‚               â”‚ SSE: {"status":"token","content":"æ ¹æ®"}
  â”‚               â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤             â”‚             â”‚
  â”‚ æ˜¾ç¤º "æ ¹æ®"   â”‚             â”‚             â”‚             â”‚
  â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤             â”‚             â”‚             â”‚
  â”‚               â”‚             â”‚             â”‚ yield "è§„å®š"
  â”‚               â”‚             â”‚             â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚               â”‚             â”‚ yield token â”‚             â”‚
  â”‚               â”‚             â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤             â”‚
  â”‚               â”‚ SSE: {"status":"token","content":"è§„å®š"}
  â”‚               â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤             â”‚             â”‚
  â”‚ æ˜¾ç¤º "è§„å®š"   â”‚             â”‚             â”‚             â”‚
  â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤             â”‚             â”‚             â”‚
  â”‚               â”‚             â”‚             â”‚             â”‚
  â”‚               â”‚ ... ç»§ç»­æµå¼è¾“å‡º ...      â”‚             â”‚
  â”‚               â”‚             â”‚             â”‚             â”‚
  â”‚               â”‚             â”‚             â”‚ yield done â”‚
  â”‚               â”‚             â”‚ yield done  â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚               â”‚ SSE: {"status":"done"}    â”‚             â”‚
  â”‚ å®Œæˆ          â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤             â”‚             â”‚
  â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤             â”‚             â”‚             â”‚
```

---

## ğŸ”§ å…³é”®ä»£ç ç‰‡æ®µæ€»ç»“

### åç«¯ï¼šFastAPI StreamingResponse

```python
@router.post("/chat/stream")
async def chat_stream(request: Request):
    async def event_generator():
        async for chunk in process_chat_stream(...):
            yield "data: " + json.dumps(chunk) + "\n\n"

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
        }
    )
```

### åç«¯ï¼šAsyncGenerator

```python
async def process_chat_stream(...) -> AsyncGenerator[str, None]:
    async for chunk in agent.ask_stream(message, thread_id=session_id):
        yield json.dumps({"status": "token", "content": chunk})
    yield json.dumps({"status": "done"})
```

### Agentï¼šä¼ªæµå¼åˆ†å—

```python
async def ask_stream(self, query, thread_id) -> AsyncGenerator[str, None]:
    # è·å–å®Œæ•´ç­”æ¡ˆ
    answer = await self._get_full_answer(query, thread_id)

    # åˆ†å—è¿”å›
    chunks = re.split(r'([.!?ã€‚ï¼ï¼Ÿ]\s*)', answer)
    for chunk in chunks:
        yield chunk
        await asyncio.sleep(0.01)
```

### å‰ç«¯ï¼šReact Fetch

```typescript
const response = await fetch("/api/v1/chat/stream", {
  method: "POST",
  headers: { "Accept": "text/event-stream" },
  body: JSON.stringify({ message, session_id }),
});

const reader = response.body.getReader();
while (true) {
  const { done, value } = await reader.read();
  if (done) break;
  // å¤„ç† value...
}
```

### å‰ç«¯ï¼šStreamlit SSE

```python
response = requests.post(
    f"{API_URL}/api/v1/chat/stream",
    json=params,
    stream=True
)

client = sseclient.SSEClient(response)
for event in client.events():
    data = json.loads(event.data)
    on_token(data.get("content", ""))
```

---

## âš ï¸ å½“å‰é™åˆ¶ä¸æœªæ¥æ”¹è¿›

### å½“å‰é™åˆ¶

1. **ä¼ªæµå¼å®ç°**
   - é—®é¢˜ï¼šå¿…é¡»ç­‰å¾…å®Œæ•´ç­”æ¡ˆç”Ÿæˆå®Œæ¯•ï¼Œæ‰å¼€å§‹åˆ†å—è¿”å›
   - å½±å“ï¼šæ— æ³•ä¸­é€”åœæ­¢ç”Ÿæˆï¼Œä»éœ€ç­‰å¾…å®Œæ•´å¤„ç†æ—¶é—´

2. **LangGraph ä¸æ”¯æŒçœŸæµå¼**
   - é—®é¢˜ï¼š`graph.stream()` æ˜¯é€èŠ‚ç‚¹æ‰§è¡Œï¼Œä¸æ˜¯é€ token ç”Ÿæˆ
   - åŸå› ï¼šLangChain æ¡†æ¶é™åˆ¶

3. **æ¨¡æ‹Ÿå»¶è¿Ÿ**
   - é—®é¢˜ï¼šä½¿ç”¨ `asyncio.sleep()` æ¨¡æ‹Ÿæµå¼æ•ˆæœ
   - å½±å“ï¼šå¢åŠ äº†ä¸å¿…è¦çš„æ€»è€—æ—¶

### æœªæ¥æ”¹è¿›æ–¹å‘

#### æ–¹æ¡ˆ 1: LangGraph çœŸæµå¼ï¼ˆç­‰å¾…å®˜æ–¹æ”¯æŒï¼‰

```python
# ç†æƒ³æƒ…å†µï¼ˆLangGraph æœªæ¥å¯èƒ½æ”¯æŒï¼‰
async for event in graph.astream_events(inputs, config):
    if event["event"] == "on_llm_stream":
        yield event["data"]["chunk"].content
```

#### æ–¹æ¡ˆ 2: ç›´æ¥ä½¿ç”¨ LLM æµå¼ API

```python
async def ask_stream(self, query, thread_id):
    # è·³è¿‡ LangGraphï¼Œç›´æ¥è°ƒç”¨ LLM æµå¼ API
    async for chunk in self.stream_llm.astream(query):
        yield chunk.content
```

#### æ–¹æ¡ˆ 3: æ··åˆæ¨¡å¼

```python
async def ask_stream(self, query, thread_id):
    # 1. å·¥å…·è°ƒç”¨éƒ¨åˆ†ï¼šéæµå¼
    if needs_tool_call:
        tool_result = await self._execute_tools(query)

    # 2. ç­”æ¡ˆç”Ÿæˆéƒ¨åˆ†ï¼šçœŸæµå¼
    prompt = build_prompt_with_context(query, tool_result)
    async for chunk in self.stream_llm.astream(prompt):
        yield chunk.content
```

---

## ğŸ“ æ€»ç»“

### æ ¸å¿ƒæŠ€æœ¯æ ˆ

| ç»„ä»¶ | æŠ€æœ¯ |
|------|------|
| **åè®®** | SSE (Server-Sent Events) |
| **åç«¯æ¡†æ¶** | FastAPI + StreamingResponse |
| **Python è¯­æ³•** | AsyncGenerator + yield |
| **å‰ç«¯ï¼ˆReactï¼‰** | Fetch API + ReadableStream |
| **å‰ç«¯ï¼ˆStreamlitï¼‰** | requests + sseclient |

### æ•°æ®æµå‘

```
ç”¨æˆ·ç‚¹å‡»
    â†“
å‰ç«¯å‘èµ· POST /api/v1/chat/stream
    â†“
è·¯ç”±å±‚ event_generator() yield SSE äº‹ä»¶
    â†“
æœåŠ¡å±‚ process_chat_stream() async for agent.ask_stream()
    â†“
Agent å±‚ ask_stream() åˆ†å— yield ç­”æ¡ˆ
    â†“
å‰ç«¯é€æ­¥æ¸²æŸ“
```

### å…³é”®ä»£ç 

**åç«¯**:
```python
async def event_generator():
    async for chunk in process_stream():
        yield f"data: {json.dumps(chunk)}\n\n"

return StreamingResponse(event_generator(), media_type="text/event-stream")
```

**å‰ç«¯**:
```typescript
const reader = response.body.getReader();
while (true) {
  const { done, value } = await reader.read();
  if (done) break;
  // å¤„ç† value
}
```

### å½“å‰å®ç°

âœ… **ä¼˜ç‚¹**: æä¾›æµå¼æ¸²æŸ“ä½“éªŒï¼Œæ”¹å–„ç”¨æˆ·æ„ŸçŸ¥
âŒ **é™åˆ¶**: ä¼ªæµå¼å®ç°ï¼Œéœ€ç­‰å¾…å®Œæ•´ç­”æ¡ˆç”Ÿæˆ

### æœªæ¥æ–¹å‘

ç­‰å¾… LangGraph å®˜æ–¹æ”¯æŒçœŸæ­£çš„æµå¼ç”Ÿæˆï¼Œæˆ–ç›´æ¥ä½¿ç”¨ LLM çš„æµå¼ APIã€‚

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**åˆ›å»ºæ—¶é—´**: 2025-12-29
**ä½œè€…**: Claude Code
**ç›¸å…³æ–‡æ¡£**:
- `docs/Chatå·¥ä½œå°å®Œæ•´è°ƒç”¨æµç¨‹.md`
- `docs/Pythoné¢å‘å¯¹è±¡_Agentè°ƒç”¨æœºåˆ¶è¯¦è§£.md`
