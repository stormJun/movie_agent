# ä»£ç èµ°è¯»

# âš ï¸ æ³¨æ„ï¼šç¼“å­˜ç›®å½•/CacheManager æè¿°å·²è¿‡æ—¶

æœ¬æ–‡æ¡£é‡Œå‡ºç°çš„ `cache_manager/CacheManager` å±äºå†å²å®ç°ï¼›v3 strict å·²å°†ç¼“å­˜ç³»ç»Ÿ**ç‰©ç†ä¸‹çº¿**ã€‚
åç»­é˜…è¯»è¯·ä»¥ä»£ç ç°çŠ¶ä¸ºå‡†ï¼ˆPostgres æŒä¹…åŒ– + mem0 è®°å¿†ï¼Œæ— ç¼“å­˜æ¨¡å—ï¼‰ã€‚

---

## ğŸ“‹ å…ƒä¿¡æ¯

- **ç›®æ ‡è¯»è€…**ï¼šäºŒæ¬¡å¼€å‘è€…ã€è´¡çŒ®è€…
- **é˜…è¯»æ—¶é—´**ï¼š90åˆ†é’Ÿ
- **éš¾åº¦**ï¼šâ­â­â­â­
- **å‰ç½®çŸ¥è¯†**ï¼šPython 3.10+ã€Neo4jã€LangChainã€é¢å‘å¯¹è±¡ç¼–ç¨‹
- **æœ€åæ›´æ–°**ï¼š2026-01-04

---

## ğŸ“– æœ¬æ–‡å¤§çº²

- [é¡¹ç›®ç»“æ„æ€»è§ˆ](#é¡¹ç›®ç»“æ„æ€»è§ˆ)
- [æ ¸å¿ƒæ¨¡å—è§£æ](#æ ¸å¿ƒæ¨¡å—è§£æ)
- [å…³é”®ç±»è¯¦è§£](#å…³é”®ç±»è¯¦è§£)
- [è®¾è®¡æ¨¡å¼åº”ç”¨](#è®¾è®¡æ¨¡å¼åº”ç”¨)
- [æ•°æ®æµè¿½è¸ª](#æ•°æ®æµè¿½è¸ª)
- [æ‰©å±•ç‚¹è¯†åˆ«](#æ‰©å±•ç‚¹è¯†åˆ«)
- [ä»£ç è§„èŒƒ](#ä»£ç è§„èŒƒ)
- [è°ƒè¯•æŠ€å·§](#è°ƒè¯•æŠ€å·§)
- [å¸¸è§é™·é˜±](#å¸¸è§é™·é˜±)
- [ç›¸å…³æ–‡æ¡£](#ç›¸å…³æ–‡æ¡£)

---

## é¡¹ç›®ç»“æ„æ€»è§ˆ

### ç›®å½•æ ‘

```
backend/graphrag_agent/
â”œâ”€â”€ __init__.py                    # åŒ…åˆå§‹åŒ–
â”œâ”€â”€ readme.md                      # æ¨¡å—è¯´æ˜
â”‚
â”œâ”€â”€ agents/                        # Agent ç³»ç»Ÿ
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py                    # BaseAgent åŸºç±»
â”‚   â”œâ”€â”€ naive_rag_agent.py         # NaiveRAG Agent
â”‚   â”œâ”€â”€ graph_agent.py             # Graph Agent
â”‚   â”œâ”€â”€ hybrid_agent.py            # Hybrid Agent
â”‚   â”œâ”€â”€ deep_research_agent.py     # Deep Research Agent
â”‚   â”œâ”€â”€ fusion_agent.py            # Fusion GraphRAG Agent
â”‚   â””â”€â”€ multi_agent/               # å¤š Agent åä½œ
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ orchestrator.py        # åè°ƒå™¨ï¼ˆå·²åºŸå¼ƒï¼‰
â”‚       â”œâ”€â”€ planner/               # è§„åˆ’é˜¶æ®µ
â”‚       â”œâ”€â”€ executor/              # æ‰§è¡Œé˜¶æ®µ
â”‚       â”œâ”€â”€ reporter/              # æŠ¥å‘Šé˜¶æ®µ
â”‚       â””â”€â”€ integration/           # é›†æˆå±‚
â”‚
â”œâ”€â”€ cache_manager/                 # ç¼“å­˜ç®¡ç†
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ manager.py                 # ç»Ÿä¸€ç¼“å­˜ç®¡ç†å™¨ï¼ˆç²¾ç¡® key å‘½ä¸­ï¼‰
â”‚   â”œâ”€â”€ backends/                  # å­˜å‚¨åç«¯
â”‚   â”œâ”€â”€ models/                    # CacheItem å…ƒæ•°æ®
â”‚   â””â”€â”€ strategies/                # Key ç­–ç•¥
â”‚
â”œâ”€â”€ community/                     # ç¤¾åŒºæ£€æµ‹
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ detector/                  # æ£€æµ‹å™¨
â”‚   â”‚   â”œâ”€â”€ base.py                # BaseCommunityDetector
â”‚   â”‚   â”œâ”€â”€ leiden.py              # Leiden ç®—æ³•
â”‚   â”‚   â”œâ”€â”€ sllpa.py               # SLLPA ç®—æ³•
â”‚   â”‚   â””â”€â”€ projections.py         # å›¾æŠ•å½±æ··å…¥
â”‚   â””â”€â”€ summary/                   # ç¤¾åŒºæ‘˜è¦
â”‚       â”œâ”€â”€ base.py                # BaseSummarizer
â”‚       â”œâ”€â”€ leiden.py              # Leiden æ‘˜è¦ç”Ÿæˆ
â”‚       â””â”€â”€ sllpa.py               # SLLPA æ‘˜è¦ç”Ÿæˆ
â”‚
â”œâ”€â”€ config/                       # é…ç½®ç®¡ç†
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ prompts/                  # Prompt æ¨¡æ¿
â”‚   â””â”€â”€ settings.py               # æ ¸å¿ƒé»˜è®¤é…ç½®
â”‚
â”œâ”€â”€ evaluation/                    # è¯„ä¼°ç³»ç»Ÿ
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ metrics/                   # è¯„ä¼°æŒ‡æ ‡
â”‚   â”œâ”€â”€ preprocessing/             # æ•°æ®é¢„å¤„ç†
â”‚   â””â”€â”€ test/                      # æµ‹è¯•è„šæœ¬
â”‚
â”œâ”€â”€ graph/                         # çŸ¥è¯†å›¾è°±
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ core/                      # æ ¸å¿ƒç»„ä»¶
â”‚   â”‚   â”œâ”€â”€ graph_ops.py           # å›¾è®¿é—®è¾…åŠ©æ–¹æ³•
â”‚   â”‚   â””â”€â”€ utils.py               # å·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ extraction/                # å®ä½“å…³ç³»æå–ä¸å†™å…¥
â”‚   â”‚   â”œâ”€â”€ entity_extractor.py    # å®ä½“æå–å™¨
â”‚   â”‚   â”œâ”€â”€ prompt_builder.py      # Prompt æ„å»º
â”‚   â”‚   â””â”€â”€ graph_writer.py        # å›¾å†™å…¥å™¨ï¼ˆGraphWriterï¼‰
â”‚   â”œâ”€â”€ processing/                # å®ä½“å¤„ç†
â”‚   â”‚   â”œâ”€â”€ disambiguator.py       # å®ä½“æ¶ˆæ­§
â”‚   â”‚   â””â”€â”€ aligner.py             # å®ä½“å¯¹é½
â”‚   â”œâ”€â”€ indexing/                  # å‘é‡ç´¢å¼•
â”‚   â”‚   â”œâ”€â”€ entity_index.py        # å®ä½“ç´¢å¼•
â”‚   â”‚   â””â”€â”€ chunk_index.py         # Chunk ç´¢å¼•
â”‚   â””â”€â”€ graph_consistency_validator.py  # ä¸€è‡´æ€§éªŒè¯
â”‚
â”œâ”€â”€ integrations/                  # é›†æˆå…¥å£
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ build/                     # æ„å»ºæµç¨‹
â”‚       â”œâ”€â”€ main.py                # ä¸»æ„å»ºè„šæœ¬
â”‚       â”œâ”€â”€ build_graph.py         # å›¾è°±æ„å»ºå™¨
â”‚       â”œâ”€â”€ incremental_update.py  # å¢é‡æ›´æ–°
â”‚       â””â”€â”€ incremental/           # å¢é‡æ›´æ–°å®ç°
â”‚
â”œâ”€â”€ models/                        # æ¨¡å‹ç®¡ç†
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ get_models.py              # LLM/åµŒå…¥æ¨¡å‹è·å–
â”‚   â””â”€â”€ schemas/                   # æ•°æ®æ¨¡å¼
â”‚
â”œâ”€â”€ pipelines/                     # æ•°æ®ç®¡é“
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ ingestion/                 # æ•°æ®æ‘„å–
â”‚       â”œâ”€â”€ document_processor.py  # æ–‡æ¡£å¤„ç†å™¨
â”‚       â””â”€â”€ text_chunker.py        # æ–‡æœ¬åˆ†å—å™¨
â”‚
â”œâ”€â”€ search/                        # æœç´¢å¼•æ“
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ local_search.py            # æœ¬åœ°æœç´¢
â”‚   â”œâ”€â”€ global_search.py           # å…¨å±€æœç´¢
â”‚   â”œâ”€â”€ tool/                      # æœç´¢å·¥å…·
â”‚   â”‚   â”œâ”€â”€ base.py                # BaseSearchTool
â”‚   â”‚   â”œâ”€â”€ naive_tool.py          # Naive Search Tool
â”‚   â”‚   â”œâ”€â”€ hybrid_tool.py         # Hybrid Search Tool
â”‚   â”‚   â””â”€â”€ deep_research_tool.py  # Deep Research Tool
â”‚   â””â”€â”€ tool_registry.py           # å·¥å…·æ³¨å†Œè¡¨
â”‚
â””â”€â”€ utils/                         # å·¥å…·å‡½æ•°
    â”œâ”€â”€ __init__.py
    â””â”€â”€ logging_context.py         # æ—¥å¿—ä¸Šä¸‹æ–‡
```

### æ¨¡å—èŒè´£

```mermaid
graph TB
    subgraph å…¥å£å±‚[å…¥å£å±‚]
        Main[integrations/build/main.py<br/>ä¸»æ„å»ºè„šæœ¬]
        Server[backend/server/main.py<br/>FastAPIæœåŠ¡å™¨]
        Frontend[frontend/app.py<br/>Streamlitå‰ç«¯]
    end

    subgraph ä¸šåŠ¡å±‚[ä¸šåŠ¡å±‚]
        Agents[agents/<br/>Agentç³»ç»Ÿ]
        Search[search/<br/>æœç´¢å¼•æ“]
        Build[integrations/build/<br/>æ„å»ºæµç¨‹]
    end

    subgraph æ ¸å¿ƒå±‚[æ ¸å¿ƒå±‚]
        Graph[graph/<br/>çŸ¥è¯†å›¾è°±]
        Community[community/<br/>ç¤¾åŒºæ£€æµ‹]
        Cache[cache_manager/<br/>ç¼“å­˜ç®¡ç†]
        Pipelines[pipelines/<br/>æ•°æ®ç®¡é“]
    end

    subgraph åŸºç¡€å±‚[åŸºç¡€å±‚]
        Config[backend/config/<br/>é…ç½®ç®¡ç†]
        Models[models/<br/>æ¨¡å‹ç®¡ç†]
        Utils[utils/<br/>å·¥å…·å‡½æ•°]
    end

    Main --> Build
    Server --> Agents
    Frontend --> Server

    Agents --> Search
    Agents --> Cache
    Build --> Graph
    Build --> Community
    Build --> Pipelines

    Graph --> Config
    Search --> Graph
    Community --> Graph

    Graph --> Models
    Cache --> Models
    Search --> Models

    style å…¥å£å±‚ fill:#e3f2fd
    style ä¸šåŠ¡å±‚ fill:#fff3e0
    style æ ¸å¿ƒå±‚ fill:#e8f5e9
    style åŸºç¡€å±‚ fill:#fce4ec
```

---

## æ ¸å¿ƒæ¨¡å—è§£æ

### 1. agents/ - Agent ç³»ç»Ÿ

**æ ¸å¿ƒæ–‡ä»¶**ï¼š`agents/base.py`

**å…³é”®ç±»**ï¼š`BaseAgent`

**èŒè´£**ï¼š
- å®šä¹‰æ‰€æœ‰ Agent çš„åŸºç¡€æ¥å£
- é›†æˆ LangGraph å·¥ä½œæµ
- ç®¡ç† LLM å®ä¾‹å’Œç¼“å­˜
- æä¾›ç»Ÿä¸€çš„ `ask()` å’Œ `ask_stream()` æ¥å£

**ç»§æ‰¿å…³ç³»**ï¼š

```mermaid
classDiagram
    class BaseAgent {
        +llm: ChatOpenAI
        +stream_llm: ChatOpenAI
        +cache_manager: SessionCacheManager
        +global_cache_manager: GlobalCacheManager
        +graph: StateGraph
        +ask(query, session_id) str
        +ask_stream(query, session_id) Generator
        #_setup_tools() List[Tool]
        #_setup_graph() StateGraph
    }

    class NaiveRagAgent {
        +ask(query, session_id) str
    }

    class GraphAgent {
        +ask(query, session_id) str
    }

    class HybridAgent {
        +ask(query, session_id) str
    }

    class DeepResearchAgent {
        +ask(query, session_id) str
    }

    class FusionGraphRAGAgent {
        +ask(query, session_id) str
    }

    BaseAgent <|-- NaiveRagAgent
    BaseAgent <|-- GraphAgent
    BaseAgent <|-- HybridAgent
    BaseAgent <|-- DeepResearchAgent
    BaseAgent <|-- FusionGraphRAGAgent
```

**ä»£ç ç¤ºä¾‹**ï¼š

```python
# backend/graphrag_agent/agents/base.py

from abc import ABC, abstractmethod
from typing import List, Dict, Any, Generator
from langgraph.graph import StateGraph, END
from langchain_core.tools import Tool
from langchain_openai import ChatOpenAI

class BaseAgent(ABC):
    """Agent åŸºç±»"""

    def __init__(
        self,
        session_id: str = "default",
        enable_cache: bool = True,
        enable_stream: bool = True
    ):
        """åˆå§‹åŒ– Agent"""
        # 1. LLM å®ä¾‹
        self.llm = get_llm_model(stream=False)
        self.stream_llm = get_llm_model(stream=True) if enable_stream else None

        # 2. ç¼“å­˜ç®¡ç†å™¨
        self.cache_manager = SessionCacheManager(session_id) if enable_cache else None
        self.global_cache_manager = GlobalCacheManager() if enable_cache else None

        # 3. å›¾æ•°æ®åº“è¿æ¥
        self.neo4j_graph = get_graph()

        # 4. å·¥å…·å’Œå›¾è®¾ç½®
        self.tools = self._setup_tools()
        self.graph = self._setup_graph()

        # 5. ç¼–è¯‘ LangGraph
        self.app = self.graph.compile(
            checkpointer=MemorySaver()  # ä¼šè¯çŠ¶æ€ä¿å­˜
        )

    @abstractmethod
    def _setup_tools(self) -> List[Tool]:
        """å­ç±»å®ç°ï¼šè®¾ç½®å·¥å…·åˆ—è¡¨"""
        pass

    def _setup_graph(self) -> StateGraph:
        """è®¾ç½® LangGraph å·¥ä½œæµï¼ˆå¯è¢«å­ç±»é‡å†™ï¼‰"""
        from langgraph.prebuilt import create_react_agent

        return create_react_agent(
            self.llm,
            tools=self.tools,
            state_modifier=self._get_system_prompt()
        )

    def ask(self, query: str, session_id: str = None) -> str:
        """åŒæ­¥é—®ç­”æ¥å£"""
        session_id = session_id or "default"

        # 1. æ£€æŸ¥ç¼“å­˜
        if self.cache_manager:
            cached_result = self.cache_manager.get(query)
            if cached_result:
                return cached_result

        # 2. è°ƒç”¨ LangGraph
        result = self.app.invoke(
            {"messages": [("user", query)]},
            config={"configurable": {"thread_id": session_id}}
        )

        # 3. æå–ç­”æ¡ˆ
        answer = result["messages"][-1].content

        # 4. å†™å…¥ç¼“å­˜
        if self.cache_manager:
            self.cache_manager.set(query, answer)

        return answer

    def ask_stream(self, query: str, session_id: str = None) -> Generator:
        """æµå¼é—®ç­”æ¥å£"""
        # å®ç°æµå¼ç”Ÿæˆé€»è¾‘
        pass
```

**æ–‡ä»¶ä½ç½®**ï¼š`backend/graphrag_agent/agents/base.py:1-150`

---

### 2. graph/ - çŸ¥è¯†å›¾è°±æ¨¡å—

**æ ¸å¿ƒæ–‡ä»¶**ï¼š`graph/extraction/entity_extractor.py`

**å…³é”®ç±»**ï¼š`EntityExtractor`

**èŒè´£**ï¼š
- ä½¿ç”¨ LLM ä»æ–‡æœ¬å—æå–å®ä½“å’Œå…³ç³»
- ç®¡ç†æå– Prompt
- æ”¯æŒæ‰¹é‡å¹¶è¡Œæå–
- é›†æˆå…¨å±€ç¼“å­˜

**ä»£ç ç¤ºä¾‹**ï¼š

```python
# backend/graphrag_agent/graph/extraction/entity_extractor.py

from typing import List, Dict, Tuple
from concurrent.futures import ThreadPoolExecutor
from infrastructure.providers.models import get_llm_model
from infrastructure.providers.cache import CacheManager
from graphrag_agent.config.prompts import ENTITY_EXTRACTION_PROMPT

class EntityExtractor:
    """å®ä½“å…³ç³»æå–å™¨"""

    def __init__(self, enable_cache: bool = True):
        self.llm = get_llm_model(stream=False)
        self.cache_manager = CacheManager() if enable_cache else None
        self.extraction_chain = self._build_extraction_chain()

    def _build_extraction_chain(self):
        """æ„å»ºæå–é“¾"""
        from langchain_core.prompts import ChatPromptTemplate
        from langchain_core.output_parsers import JsonOutputParser

        prompt = ChatPromptTemplate.from_messages([
            ("system", ENTITY_EXTRACTION_PROMPT),
            ("user", "æ–‡æœ¬å†…å®¹ï¼š\n{text}\n\nè¯·æå–å®ä½“å’Œå…³ç³»ï¼ˆJSONæ ¼å¼ï¼‰")
        ])

        return prompt | self.llm | JsonOutputParser()

    def extract_from_chunk(self, chunk_text: str) -> Dict[str, Any]:
        """ä»å•ä¸ªæ–‡æœ¬å—æå–å®ä½“å…³ç³»"""
        # 1. æ£€æŸ¥ç¼“å­˜
        if self.cache_manager:
            cache_key = f"extraction_{hash(chunk_text)}"
            cached = self.cache_manager.get(cache_key)
            if cached:
                return cached

        # 2. LLM æå–
        try:
            result = self.extraction_chain.invoke({"text": chunk_text})

            # 3. å†™å…¥ç¼“å­˜
            if self.cache_manager:
                self.cache_manager.set(cache_key, result)

            return result

        except Exception as e:
            print(f"æå–å¤±è´¥: {e}")
            return {"entities": [], "relationships": []}

    def extract_batch(
        self,
        chunks: List[Dict],
        max_workers: int = 4
    ) -> List[Dict]:
        """æ‰¹é‡å¹¶è¡Œæå–"""
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = [
                executor.submit(self.extract_from_chunk, chunk["text"])
                for chunk in chunks
            ]

            results = [future.result() for future in futures]

        return results
```

**æ–‡ä»¶ä½ç½®**ï¼š`backend/graphrag_agent/graph/extraction/entity_extractor.py:1-120`

---

### 3. search/ - æœç´¢å¼•æ“æ¨¡å—

**æ ¸å¿ƒæ–‡ä»¶**ï¼š`search/local_search.py`

**å…³é”®ç±»**ï¼š`LocalSearch`

**èŒè´£**ï¼š
- å®ç°æœ¬åœ°æœç´¢ï¼ˆEntity-centricï¼‰
- å‘é‡æ£€ç´¢ + å›¾éå†
- ç”Ÿæˆå¸¦è¯æ®çš„ç­”æ¡ˆ

**ä»£ç ç¤ºä¾‹**ï¼š

```python
# backend/graphrag_agent/search/local_search.py

from typing import List, Dict
from infrastructure.models import get_llm_model, get_embeddings_model
from langchain_community.graphs import Neo4jGraph

class LocalSearch:
    """æœ¬åœ°æœç´¢ï¼šåŸºäºå®ä½“çš„é‚»åŸŸæœç´¢"""

    def __init__(self):
        self.llm = get_llm_model()
        self.embeddings = get_embeddings_model()
        self.graph = Neo4jGraph(
            url=NEO4J_URI,
            username=NEO4J_USERNAME,
            password=NEO4J_PASSWORD
        )

    def search(
        self,
        query: str,
        top_k_entities: int = 5,
        neighborhood_depth: int = 2
    ) -> str:
        """æ‰§è¡Œæœ¬åœ°æœç´¢"""
        # 1. æŸ¥è¯¢å‘é‡åŒ–
        query_vector = self.embeddings.embed_query(query)

        # 2. å‘é‡æ£€ç´¢ç›¸å…³å®ä½“
        entities = self._retrieve_entities(query_vector, top_k_entities)

        # 3. æ‰©å±•å®ä½“é‚»åŸŸ
        context = self._expand_neighborhood(entities, neighborhood_depth)

        # 4. LLM ç”Ÿæˆç­”æ¡ˆ
        answer = self._generate_answer(query, context)

        return answer

    def _retrieve_entities(
        self,
        query_vector: List[float],
        top_k: int
    ) -> List[Dict]:
        """å‘é‡æ£€ç´¢å®ä½“"""
        result = self.graph.query("""
            CALL db.index.vector.queryNodes('entity_index', $top_k, $query_vector)
            YIELD node, score
            RETURN node.id AS id,
                   node.name AS name,
                   node.description AS description,
                   score
            ORDER BY score DESC
        """, params={
            "query_vector": query_vector,
            "top_k": top_k
        })

        return result

    def _expand_neighborhood(
        self,
        entities: List[Dict],
        depth: int
    ) -> Dict[str, Any]:
        """æ‰©å±•å®ä½“é‚»åŸŸ"""
        entity_ids = [e["id"] for e in entities]

        result = self.graph.query(f"""
            MATCH (e:__Entity__)
            WHERE e.id IN $entity_ids

            // æ‰©å±• {depth} è·³é‚»å±…
            CALL {{
                WITH e
                MATCH path = (e)-[r*1..{depth}]-(neighbor:__Entity__)
                RETURN
                    collect(DISTINCT neighbor) AS neighbors,
                    collect(DISTINCT r) AS relationships
            }}

            RETURN
                collect(e) AS center_entities,
                neighbors,
                relationships
        """, params={"entity_ids": entity_ids})

        return result[0] if result else {}

    def _generate_answer(self, query: str, context: Dict) -> str:
        """åŸºäºä¸Šä¸‹æ–‡ç”Ÿæˆç­”æ¡ˆ"""
        from langchain_core.prompts import ChatPromptTemplate

        prompt = ChatPromptTemplate.from_messages([
            ("system", "ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†å›¾è°±é—®ç­”åŠ©æ‰‹ã€‚åŸºäºæä¾›çš„å®ä½“å’Œå…³ç³»å›ç­”ç”¨æˆ·é—®é¢˜ã€‚"),
            ("user", "é—®é¢˜ï¼š{query}\n\nä¸Šä¸‹æ–‡ï¼š\n{context}\n\nè¯·å›ç­”ï¼š")
        ])

        # æ ¼å¼åŒ–ä¸Šä¸‹æ–‡
        context_str = self._format_context(context)

        # è°ƒç”¨ LLM
        chain = prompt | self.llm
        response = chain.invoke({
            "query": query,
            "context": context_str
        })

        return response.content
```

**æ–‡ä»¶ä½ç½®**ï¼š`backend/graphrag_agent/search/local_search.py:1-180`

---

## å…³é”®ç±»è¯¦è§£

### 1. KnowledgeGraphBuilder

**ä½ç½®**ï¼š`backend/infrastructure/integrations/build/build_graph.py`

**èŒè´£**ï¼šç¼–æ’çŸ¥è¯†å›¾è°±æ„å»ºçš„å®Œæ•´æµç¨‹

**ç±»å›¾**ï¼š

```mermaid
classDiagram
    class KnowledgeGraphBuilder {
        +document_processor: DocumentProcessor
        +text_chunker: TextChunker
        +entity_extractor: EntityExtractor
        +disambiguator: EntityDisambiguator
        +aligner: EntityAligner
        +graph_writer: GraphWriter
        +process() void
        -_process_documents() List[Document]
        -_extract_entities() List[Entity]
        -_disambiguate_entities() void
        -_align_entities() void
        -_write_to_graph() void
    }

    class DocumentProcessor {
        +process_directory(path) List[Document]
    }

    class EntityExtractor {
        +extract_batch(chunks) List[Extraction]
    }

    class EntityDisambiguator {
        +process() void
    }

    KnowledgeGraphBuilder --> DocumentProcessor
    KnowledgeGraphBuilder --> EntityExtractor
    KnowledgeGraphBuilder --> EntityDisambiguator
```

**å®Œæ•´ä»£ç è§£æ**ï¼š

```python
# backend/infrastructure/integrations/build/build_graph.py

from infrastructure.pipelines.ingestion import DocumentProcessor
from graphrag_agent.graph.extraction import EntityExtractor
from graphrag_agent.graph.processing import EntityDisambiguator, EntityAligner
from graphrag_agent.graph.core import GraphWriter

class KnowledgeGraphBuilder:
    """çŸ¥è¯†å›¾è°±æ„å»ºå™¨ï¼šç¼–æ’å®Œæ•´æ„å»ºæµç¨‹"""

    def __init__(self, source_dir: str = "files/"):
        """åˆå§‹åŒ–æ„å»ºå™¨"""
        self.source_dir = source_dir

        # åˆå§‹åŒ–å„ç»„ä»¶
        self.document_processor = DocumentProcessor(self.source_dir)
        self.text_chunker = self.document_processor.chunker
        self.entity_extractor = EntityExtractor(enable_cache=True)
        self.disambiguator = EntityDisambiguator()
        self.aligner = EntityAligner()
        self.graph_writer = GraphWriter()

    def process(self):
        """æ‰§è¡Œå®Œæ•´æ„å»ºæµç¨‹"""
        print("=" * 60)
        print("å¼€å§‹çŸ¥è¯†å›¾è°±æ„å»º")
        print("=" * 60)

        # æ­¥éª¤1ï¼šæ–‡æ¡£å¤„ç†
        print("\n[1/6] æ–‡æ¡£å¤„ç†...")
        documents = self._process_documents()
        print(f"âœ“ å¤„ç†äº† {len(documents)} ä¸ªæ–‡æ¡£")

        # æ­¥éª¤2ï¼šæ–‡æœ¬åˆ†å—
        print("\n[2/6] æ–‡æœ¬åˆ†å—...")
        chunks = self._chunk_documents(documents)
        print(f"âœ“ ç”Ÿæˆäº† {len(chunks)} ä¸ªæ–‡æœ¬å—")

        # æ­¥éª¤3ï¼šå®ä½“å…³ç³»æå–
        print("\n[3/6] å®ä½“å…³ç³»æå–...")
        extractions = self._extract_entities(chunks)
        print(f"âœ“ æå–äº† {len(extractions)} ä¸ªå®ä½“å…³ç³»å¯¹")

        # æ­¥éª¤4ï¼šå†™å…¥å›¾æ•°æ®åº“
        print("\n[4/6] å†™å…¥ Neo4j...")
        self._write_to_graph(documents, chunks, extractions)
        print("âœ“ å›¾è°±å†™å…¥å®Œæˆ")

        # æ­¥éª¤5ï¼šå®ä½“æ¶ˆæ­§
        print("\n[5/6] å®ä½“æ¶ˆæ­§...")
        self.disambiguator.process()
        print("âœ“ å®ä½“æ¶ˆæ­§å®Œæˆ")

        # æ­¥éª¤6ï¼šå®ä½“å¯¹é½
        print("\n[6/6] å®ä½“å¯¹é½...")
        self.aligner.process()
        print("âœ“ å®ä½“å¯¹é½å®Œæˆ")

        print("\n" + "=" * 60)
        print("çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆï¼")
        print("=" * 60)

    def _process_documents(self) -> List[Dict]:
        """å¤„ç†æ–‡æ¡£"""
        from pathlib import Path

        # è·å–æ‰€æœ‰æ–‡æ¡£
        doc_files = list(Path(self.source_dir).rglob("*"))
        doc_files = [f for f in doc_files if f.is_file()]

        # å¤„ç†æ¯ä¸ªæ–‡æ¡£
        documents = []
        for doc_file in doc_files:
            try:
                content = self.document_processor.process_file(str(doc_file))
                documents.append({
                    "id": f"doc_{hash(str(doc_file))}",
                    "file_name": doc_file.name,
                    "file_path": str(doc_file),
                    "content": content,
                    "file_type": doc_file.suffix[1:]
                })
            except Exception as e:
                print(f"å¤„ç†æ–‡æ¡£å¤±è´¥ {doc_file}: {e}")

        return documents

    def _chunk_documents(self, documents: List[Dict]) -> List[Dict]:
        """æ–‡æœ¬åˆ†å—"""
        all_chunks = []

        for doc in documents:
            chunks = self.text_chunker.chunk_text(doc["content"])

            for i, chunk_text in enumerate(chunks):
                all_chunks.append({
                    "id": f"{doc['id']}_chunk_{i}",
                    "document_id": doc["id"],
                    "text": chunk_text,
                    "chunk_index": i
                })

        return all_chunks

    def _extract_entities(self, chunks: List[Dict]) -> List[Dict]:
        """æå–å®ä½“å…³ç³»"""
        # æ‰¹é‡å¹¶è¡Œæå–
        extractions = self.entity_extractor.extract_batch(
            chunks,
            max_workers=MAX_WORKERS
        )

        # åˆå¹¶ç»“æœ
        all_entities = []
        all_relationships = []

        for i, extraction in enumerate(extractions):
            chunk_id = chunks[i]["id"]

            for entity in extraction.get("entities", []):
                entity["source_chunk"] = chunk_id
                all_entities.append(entity)

            for rel in extraction.get("relationships", []):
                rel["source_chunk"] = chunk_id
                all_relationships.append(rel)

        return {
            "entities": all_entities,
            "relationships": all_relationships
        }

    def _write_to_graph(
        self,
        documents: List[Dict],
        chunks: List[Dict],
        extractions: Dict
    ):
        """å†™å…¥å›¾æ•°æ®åº“"""
        # 1. å†™å…¥æ–‡æ¡£èŠ‚ç‚¹
        self.graph_writer.write_documents(documents)

        # 2. å†™å…¥ Chunk èŠ‚ç‚¹
        self.graph_writer.write_chunks(chunks)

        # 3. å†™å…¥å®ä½“èŠ‚ç‚¹
        self.graph_writer.write_entities(extractions["entities"])

        # 4. å†™å…¥å…³ç³»
        self.graph_writer.write_relationships(extractions["relationships"])
```

**æ–‡ä»¶ä½ç½®**ï¼š`backend/infrastructure/integrations/build/build_graph.py:1-250`

---

### 2. CacheManager

**ä½ç½®**ï¼š`backend/infrastructure/cache_manager/manager.py`

**èŒè´£**ï¼šç»Ÿä¸€ç¼“å­˜å…¥å£ï¼ˆç²¾ç¡® key å‘½ä¸­ï¼‰ï¼Œç®¡ç†å­˜å‚¨åç«¯ä¸ç¼“å­˜é”®ç­–ç•¥

**ç±»å›¾**ï¼š

```mermaid
classDiagram
    class CacheManager {
        +get(query, **kwargs) Any
        +set(query, result, **kwargs) void
        +delete(query, **kwargs) bool
        +clear() void
        +get_metrics() Dict
    }

    CacheManager --> CacheStorageBackend
    CacheManager --> CacheKeyStrategy
    CacheManager --> CacheItem
```

**ä»£ç ç¤ºä¾‹**ï¼š

```python
# backend/infrastructure/cache_manager/manager.py

from infrastructure.providers.cache import (
    CacheManager,
    MemoryCacheBackend,
    SimpleCacheKeyStrategy,
)

# å½“å‰å®ç°ä»¥ `CacheManager` ä¸ºç»Ÿä¸€å…¥å£ï¼ˆä»…ç²¾ç¡® key å‘½ä¸­ï¼‰ã€‚
cache = CacheManager(
    key_strategy=SimpleCacheKeyStrategy(),
    storage_backend=MemoryCacheBackend(),
)

cached = cache.get("What is GraphRAG?")
if cached is None:
    cache.set("What is GraphRAG?", "GraphRAG is ...")
    cached = cache.get("What is GraphRAG?")

print(cached)
```

**æ–‡ä»¶ä½ç½®**ï¼š`backend/infrastructure/cache_manager/manager.py`ï¼ˆå®ç°ä»¥ä»£ç ä¸ºå‡†ï¼›æ­¤å¤„ç¤ºä¾‹ä¸ºç®€åŒ–è°ƒç”¨ï¼‰

---

## è®¾è®¡æ¨¡å¼åº”ç”¨

### 1. å·¥å‚æ¨¡å¼ï¼ˆFactory Patternï¼‰

**åº”ç”¨åœºæ™¯**ï¼šåˆ›å»ºç¤¾åŒºæ£€æµ‹å™¨

```python
# backend/graphrag_agent/community/__init__.py

class CommunityDetectorFactory:
    """ç¤¾åŒºæ£€æµ‹å™¨å·¥å‚"""

    @staticmethod
    def create(
        algorithm: str,
        gds,
        graph
    ) -> BaseCommunityDetector:
        """æ ¹æ®ç®—æ³•ç±»å‹åˆ›å»ºæ£€æµ‹å™¨"""
        if algorithm == "leiden":
            from .detector.leiden import LeidenDetector
            return LeidenDetector(gds, graph)

        elif algorithm == "sllpa":
            from .detector.sllpa import SLLPADetector
            return SLLPADetector(gds, graph)

        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ç®—æ³•: {algorithm}")


# ä½¿ç”¨ç¤ºä¾‹
detector = CommunityDetectorFactory.create(
    algorithm="leiden",
    gds=gds,
    graph=graph
)
result = detector.process()
```

**ä¼˜åŠ¿**ï¼š
- è§£è€¦å¯¹è±¡åˆ›å»ºå’Œä½¿ç”¨
- æ–¹ä¾¿æ‰©å±•æ–°ç®—æ³•
- ç»Ÿä¸€åˆ›å»ºæ¥å£

---

### 2. ç­–ç•¥æ¨¡å¼ï¼ˆStrategy Patternï¼‰

**åº”ç”¨åœºæ™¯**ï¼šç¼“å­˜åç«¯é€‰æ‹©

```python
# backend/infrastructure/cache_manager/backends/base.py

from abc import ABC, abstractmethod

class CacheBackend(ABC):
    """ç¼“å­˜åç«¯æ¥å£"""

    @abstractmethod
    def get(self, key: str) -> Any:
        pass

    @abstractmethod
    def set(self, key: str, value: Any):
        pass

    @abstractmethod
    def clear(self):
        pass


# å†…å­˜åç«¯å®ç°
class MemoryCacheBackend(CacheBackend):
    def __init__(self):
        self.storage = {}

    def get(self, key: str) -> Any:
        return self.storage.get(key)

    def set(self, key: str, value: Any):
        self.storage[key] = value

    def clear(self):
        self.storage.clear()


# ç£ç›˜åç«¯å®ç°
class DiskCacheBackend(CacheBackend):
    def __init__(self, cache_dir: str):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)

    def get(self, key: str) -> Any:
        cache_file = self.cache_dir / f"{hash(key)}.json"
        if cache_file.exists():
            with open(cache_file, 'r') as f:
                return json.load(f)
        return None

    def set(self, key: str, value: Any):
        cache_file = self.cache_dir / f"{hash(key)}.json"
        with open(cache_file, 'w') as f:
            json.dump(value, f)

    def clear(self):
        shutil.rmtree(self.cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)


# ä½¿ç”¨ç¤ºä¾‹
# å¯ä»¥åŠ¨æ€åˆ‡æ¢åç«¯
backend = MemoryCacheBackend()  # æˆ– DiskCacheBackend("./cache")
cache_manager = SessionCacheManager(
    session_id="user_123",
    backend=backend
)
```

**ä¼˜åŠ¿**ï¼š
- è¿è¡Œæ—¶åŠ¨æ€é€‰æ‹©ç®—æ³•
- æ˜“äºæ‰©å±•æ–°åç«¯
- ç¬¦åˆå¼€é—­åŸåˆ™

---

### 3. æ¨¡æ¿æ–¹æ³•æ¨¡å¼ï¼ˆTemplate Method Patternï¼‰

**åº”ç”¨åœºæ™¯**ï¼šAgent åŸºç±»å®šä¹‰æµç¨‹æ¨¡æ¿

```python
# backend/graphrag_agent/agents/base.py

class BaseAgent(ABC):
    """Agent åŸºç±»ï¼šå®šä¹‰é—®ç­”æµç¨‹æ¨¡æ¿"""

    def ask(self, query: str, session_id: str = None) -> str:
        """é—®ç­”æµç¨‹æ¨¡æ¿ï¼ˆä¸å¯è¢«é‡å†™ï¼‰"""
        # 1. é¢„å¤„ç†ï¼ˆé’©å­æ–¹æ³•ï¼‰
        processed_query = self._preprocess_query(query)

        # 2. æ£€æŸ¥ç¼“å­˜
        cached_result = self._check_cache(processed_query, session_id)
        if cached_result:
            return cached_result

        # 3. æ ¸å¿ƒå¤„ç†ï¼ˆæŠ½è±¡æ–¹æ³•ï¼Œå­ç±»å¿…é¡»å®ç°ï¼‰
        answer = self._process_query(processed_query, session_id)

        # 4. åå¤„ç†ï¼ˆé’©å­æ–¹æ³•ï¼‰
        final_answer = self._postprocess_answer(answer)

        # 5. å†™å…¥ç¼“å­˜
        self._write_cache(processed_query, final_answer, session_id)

        return final_answer

    def _preprocess_query(self, query: str) -> str:
        """é¢„å¤„ç†æŸ¥è¯¢ï¼ˆé’©å­æ–¹æ³•ï¼Œå­ç±»å¯é€‰æ‹©é‡å†™ï¼‰"""
        return query.strip()

    def _check_cache(self, query: str, session_id: str) -> Optional[str]:
        """æ£€æŸ¥ç¼“å­˜ï¼ˆæ¨¡æ¿å†…éƒ¨æ–¹æ³•ï¼‰"""
        if self.cache_manager:
            return self.cache_manager.get(query)
        return None

    @abstractmethod
    def _process_query(self, query: str, session_id: str) -> str:
        """å¤„ç†æŸ¥è¯¢ï¼ˆæŠ½è±¡æ–¹æ³•ï¼Œå­ç±»å¿…é¡»å®ç°ï¼‰"""
        pass

    def _postprocess_answer(self, answer: str) -> str:
        """åå¤„ç†ç­”æ¡ˆï¼ˆé’©å­æ–¹æ³•ï¼Œå­ç±»å¯é€‰æ‹©é‡å†™ï¼‰"""
        return answer

    def _write_cache(self, query: str, answer: str, session_id: str):
        """å†™å…¥ç¼“å­˜ï¼ˆæ¨¡æ¿å†…éƒ¨æ–¹æ³•ï¼‰"""
        if self.cache_manager:
            self.cache_manager.set(query, answer)


# å­ç±»å®ç°
class GraphAgent(BaseAgent):
    def _process_query(self, query: str, session_id: str) -> str:
        """å®ç°å›¾æ£€ç´¢é€»è¾‘"""
        # å­ç±»å…·ä½“å®ç°
        pass

    def _postprocess_answer(self, answer: str) -> str:
        """é‡å†™åå¤„ç†ï¼ˆå¯é€‰ï¼‰"""
        # æ·»åŠ æ¥æºå¼•ç”¨
        return f"{answer}\n\næ¥æºï¼šçŸ¥è¯†å›¾è°±"
```

**ä¼˜åŠ¿**ï¼š
- å›ºå®šæµç¨‹éª¨æ¶ï¼Œçµæ´»æ‰©å±•ç»†èŠ‚
- å¤ç”¨å…¬å…±é€»è¾‘
- å¼ºåˆ¶å­ç±»å®ç°å…³é”®æ­¥éª¤

---

### 4. è£…é¥°å™¨æ¨¡å¼ï¼ˆDecorator Patternï¼‰

**åº”ç”¨åœºæ™¯**ï¼šç¼“å­˜è£…é¥°å™¨

```python
# backend/infrastructure/cache_manager/decorators.py

from functools import wraps
from typing import Callable

def with_global_cache(func: Callable) -> Callable:
    """å…¨å±€ç¼“å­˜è£…é¥°å™¨"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        from infrastructure.providers.cache import CacheManager

        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = f"{func.__name__}_{hash((args, frozenset(kwargs.items())))}"

        # æ£€æŸ¥ç¼“å­˜
        cache_manager = CacheManager()
        cached_result = cache_manager.get(cache_key)

        if cached_result:
            print(f"[Cache] å‘½ä¸­å…¨å±€ç¼“å­˜: {func.__name__}")
            return cached_result

        # æ‰§è¡Œå‡½æ•°
        result = func(*args, **kwargs)

        # å†™å…¥ç¼“å­˜
        cache_manager.set(cache_key, result)

        return result

    return wrapper


# ä½¿ç”¨ç¤ºä¾‹
@with_global_cache
def extract_entities(chunk_text: str) -> Dict:
    """å®ä½“æå–ï¼ˆå¸¦å…¨å±€ç¼“å­˜ï¼‰"""
    # è€—æ—¶çš„ LLM è°ƒç”¨
    result = llm.invoke(chunk_text)
    return result
```

**ä¼˜åŠ¿**ï¼š
- é€æ˜åœ°æ·»åŠ åŠŸèƒ½
- ä¸ä¿®æ”¹åŸæœ‰ä»£ç 
- å¯ç»„åˆå¤šä¸ªè£…é¥°å™¨

---

### 5. è§‚å¯Ÿè€…æ¨¡å¼ï¼ˆObserver Patternï¼‰

**åº”ç”¨åœºæ™¯**ï¼šè¿›åº¦ç›‘æ§

```python
# backend/infrastructure/utils/progress.py

from typing import List, Callable

class ProgressObserver:
    """è¿›åº¦è§‚å¯Ÿè€…æ¥å£"""
    def update(self, current: int, total: int, message: str):
        pass


class ConsoleProgressObserver(ProgressObserver):
    """æ§åˆ¶å°è¿›åº¦è§‚å¯Ÿè€…"""
    def update(self, current: int, total: int, message: str):
        percentage = (current / total) * 100
        print(f"[{percentage:.1f}%] {message} ({current}/{total})")


class ProgressSubject:
    """è¿›åº¦ä¸»é¢˜ï¼ˆè¢«è§‚å¯Ÿè€…ï¼‰"""
    def __init__(self):
        self._observers: List[ProgressObserver] = []

    def attach(self, observer: ProgressObserver):
        """æ·»åŠ è§‚å¯Ÿè€…"""
        self._observers.append(observer)

    def detach(self, observer: ProgressObserver):
        """ç§»é™¤è§‚å¯Ÿè€…"""
        self._observers.remove(observer)

    def notify(self, current: int, total: int, message: str):
        """é€šçŸ¥æ‰€æœ‰è§‚å¯Ÿè€…"""
        for observer in self._observers:
            observer.update(current, total, message)


# ä½¿ç”¨ç¤ºä¾‹
class KnowledgeGraphBuilder:
    def __init__(self):
        self.progress = ProgressSubject()

        # æ·»åŠ æ§åˆ¶å°è§‚å¯Ÿè€…
        self.progress.attach(ConsoleProgressObserver())

    def process(self):
        chunks = self._load_chunks()
        total = len(chunks)

        for i, chunk in enumerate(chunks):
            self._process_chunk(chunk)

            # é€šçŸ¥è¿›åº¦
            self.progress.notify(
                current=i + 1,
                total=total,
                message=f"å¤„ç†æ–‡æœ¬å— {chunk['id']}"
            )
```

**ä¼˜åŠ¿**ï¼š
- è§£è€¦è¿›åº¦æŠ¥å‘Šå’Œä¸šåŠ¡é€»è¾‘
- æ”¯æŒå¤šä¸ªè§‚å¯Ÿè€…
- æ˜“äºæ‰©å±•æ–°çš„è§‚å¯Ÿè€…ç±»å‹

---

## æ•°æ®æµè¿½è¸ª

### å®Œæ•´æŸ¥è¯¢æµç¨‹

**åœºæ™¯**ï¼šç”¨æˆ·é€šè¿‡ Streamlit å‰ç«¯æé—® "å›½å®¶å¥–å­¦é‡‘çš„ç”³è¯·æ¡ä»¶æ˜¯ä»€ä¹ˆï¼Ÿ"

```mermaid
sequenceDiagram
    participant U as ç”¨æˆ·
    participant FE as Streamlit å‰ç«¯
    participant API as FastAPI åç«¯
    participant Agent as HybridAgent
    participant Cache as CacheManager
    participant Search as HybridSearchTool
    participant Neo4j as Neo4j æ•°æ®åº“
    participant LLM as OpenAI LLM

    U->>FE: è¾“å…¥æŸ¥è¯¢
    FE->>API: POST /api/v1/chat
    API->>Agent: ask(query, session_id)

    Agent->>Cache: get(query)
    Cache-->>Agent: None (æœªå‘½ä¸­)

    Agent->>LLM: åˆ†ææŸ¥è¯¢æ„å›¾
    LLM-->>Agent: "éœ€è¦æ··åˆæ£€ç´¢"

    Agent->>Search: hybrid_search(query)

    par å¹¶è¡Œæ£€ç´¢
        Search->>Neo4j: å‘é‡æ£€ç´¢å®ä½“
        Search->>Neo4j: å›¾éå†é‚»åŸŸ
        Search->>Neo4j: æ£€ç´¢ç›¸å…³ç¤¾åŒº
    end

    Neo4j-->>Search: è¿”å›ä¸Šä¸‹æ–‡

    Search->>LLM: åŸºäºä¸Šä¸‹æ–‡ç”Ÿæˆç­”æ¡ˆ
    LLM-->>Search: ç­”æ¡ˆ

    Search-->>Agent: ç­”æ¡ˆ + è¯æ®

    Agent->>Cache: set(query, answer)
    Cache-->>Agent: OK

    Agent-->>API: è¿”å›ç­”æ¡ˆ
    API-->>FE: JSON å“åº”
    FE-->>U: æ˜¾ç¤ºç­”æ¡ˆ
```

**ä»£ç è¿½è¸ªè·¯å¾„**ï¼š

1. **å‰ç«¯**ï¼š`frontend/app.py`ï¼ˆ`chat_interface()`ï¼‰
   ```python
   def chat_interface():
       if user_input:
           response = requests.post(
               f"{API_URL}/api/v1/chat",
               json={"query": user_input, "session_id": session_id}
           )
           answer = response.json()["answer"]
   ```

2. **åç«¯ API**ï¼š`backend/server/api/rest/v1/chat.py`ï¼ˆ`chat_endpoint()`ï¼‰
   ```python
   @router.post("/chat")
   def chat_endpoint(request: ChatRequest):
       agent = get_agent(request.agent_type)
       answer = agent.ask(request.query, request.session_id)
       return {"answer": answer}
   ```

3. **Agent å±‚**ï¼š`backend/graphrag_agent/agents/hybrid_agent.py`ï¼ˆ`ask()`ï¼‰
   ```python
   def ask(self, query: str, session_id: str) -> str:
       # æ£€æŸ¥ç¼“å­˜
       cached = self.cache_manager.get(query)
       if cached:
           return cached

       # è°ƒç”¨ LangGraph
       result = self.app.invoke(
           {"messages": [("user", query)]},
           config={"configurable": {"thread_id": session_id}}
       )

       answer = result["messages"][-1].content

       # å†™å…¥ç¼“å­˜
       self.cache_manager.set(query, answer)

       return answer
   ```

4. **æœç´¢å·¥å…·**ï¼š`backend/graphrag_agent/search/tool/hybrid_tool.py`ï¼ˆ`_run()`ï¼‰
   ```python
   def _run(self, query: str) -> str:
       # å‘é‡æ£€ç´¢
       entities = self._vector_retrieve(query)

       # å›¾éå†
       context = self._graph_traverse(entities)

       # ç”Ÿæˆç­”æ¡ˆ
       answer = self._generate_answer(query, context)

       return answer
   ```

5. **æ•°æ®åº“æŸ¥è¯¢**ï¼š`backend/graphrag_agent/search/tool/hybrid_tool.py`ï¼ˆ`_vector_retrieve()`ï¼‰
   ```python
   def _vector_retrieve(self, query: str) -> List[Dict]:
       query_vector = self.embeddings.embed_query(query)

       result = self.graph.query("""
           CALL db.index.vector.queryNodes('entity_index', 10, $query_vector)
           YIELD node, score
           RETURN node.id, node.description, score
       """, params={"query_vector": query_vector})

       return result
   ```

---

## æ‰©å±•ç‚¹è¯†åˆ«

### 1. æ·»åŠ æ–° Agent

**æ‰©å±•ç‚¹**ï¼š`backend/graphrag_agent/agents/`

**æ­¥éª¤**ï¼š

```python
# 1. åˆ›å»ºæ–° Agent ç±»
# backend/graphrag_agent/agents/my_custom_agent.py

from graphrag_agent.agents.base import BaseAgent
from langchain_core.tools import Tool

class MyCustomAgent(BaseAgent):
    """è‡ªå®šä¹‰ Agent"""

    def _setup_tools(self) -> List[Tool]:
        """å®šä¹‰å·¥å…·åˆ—è¡¨"""
        return [
            Tool(
                name="custom_search",
                func=self._custom_search,
                description="è‡ªå®šä¹‰æœç´¢å·¥å…·"
            )
        ]

    def _custom_search(self, query: str) -> str:
        """è‡ªå®šä¹‰æœç´¢é€»è¾‘"""
        # å®ç°ä½ çš„é€»è¾‘
        pass

    def _get_system_prompt(self) -> str:
        """å®šä¹‰ç³»ç»Ÿæç¤º"""
        return "ä½ æ˜¯ä¸€ä¸ªè‡ªå®šä¹‰åŠ©æ‰‹..."


# 2. æ³¨å†Œåˆ° Agent å·¥å‚
# backend/graphrag_agent/agents/__init__.py

from .my_custom_agent import MyCustomAgent

AGENT_REGISTRY = {
    "naive": NaiveRagAgent,
    "graph": GraphAgent,
    "hybrid": HybridAgent,
    "deep_research": DeepResearchAgent,
    "fusion": FusionGraphRAGAgent,
    "custom": MyCustomAgent,  # æ–°å¢
}

def get_agent(agent_type: str) -> BaseAgent:
    agent_class = AGENT_REGISTRY.get(agent_type)
    if not agent_class:
        raise ValueError(f"æœªçŸ¥ Agent ç±»å‹: {agent_type}")
    return agent_class()
```

---

### 2. æ·»åŠ æ–°æœç´¢å·¥å…·

**æ‰©å±•ç‚¹**ï¼š`backend/graphrag_agent/search/tool/`

```python
# backend/graphrag_agent/search/tool/my_search_tool.py

from langchain_core.tools import BaseTool

class MySearchTool(BaseTool):
    """è‡ªå®šä¹‰æœç´¢å·¥å…·"""

    name = "my_search"
    description = "è‡ªå®šä¹‰æœç´¢ç­–ç•¥"

    def _run(self, query: str) -> str:
        """æ‰§è¡Œæœç´¢"""
        # å®ç°ä½ çš„æœç´¢é€»è¾‘
        pass

    def _arun(self, query: str):
        """å¼‚æ­¥ç‰ˆæœ¬ï¼ˆå¯é€‰ï¼‰"""
        raise NotImplementedError()


# æ³¨å†Œåˆ°å·¥å…·æ³¨å†Œè¡¨
# backend/graphrag_agent/search/tool_registry.py

from .my_search_tool import MySearchTool

TOOL_REGISTRY = {
    "naive": NaiveSearchTool,
    "hybrid": HybridSearchTool,
    "deep_research": DeepResearchTool,
    "my_search": MySearchTool,  # æ–°å¢
}
```

---

### 3. æ·»åŠ æ–°ç¤¾åŒºæ£€æµ‹ç®—æ³•

**æ‰©å±•ç‚¹**ï¼š`backend/graphrag_agent/community/detector/`

```python
# backend/graphrag_agent/community/detector/my_algorithm.py

from graphrag_agent.community.detector.base import BaseCommunityDetector

class MyAlgorithmDetector(BaseCommunityDetector):
    """è‡ªå®šä¹‰ç¤¾åŒºæ£€æµ‹ç®—æ³•"""

    def detect_communities(self) -> Dict[str, Any]:
        """å®ç°æ£€æµ‹é€»è¾‘"""
        # ä½ çš„ç®—æ³•å®ç°
        pass

    def save_communities(self) -> Dict[str, Any]:
        """ä¿å­˜ç¤¾åŒºç»“æœ"""
        # ä¿å­˜é€»è¾‘
        pass


# æ³¨å†Œåˆ°å·¥å‚
# backend/graphrag_agent/community/__init__.py

class CommunityDetectorFactory:
    @staticmethod
    def create(algorithm: str, gds, graph):
        if algorithm == "leiden":
            from .detector.leiden import LeidenDetector
            return LeidenDetector(gds, graph)
        elif algorithm == "sllpa":
            from .detector.sllpa import SLLPADetector
            return SLLPADetector(gds, graph)
        elif algorithm == "my_algorithm":
            from .detector.my_algorithm import MyAlgorithmDetector
            return MyAlgorithmDetector(gds, graph)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ç®—æ³•: {algorithm}")
```

---

## ä»£ç è§„èŒƒ

### 1. å‘½åè§„èŒƒ

**æ¨¡å—å’ŒåŒ…**ï¼šå°å†™+ä¸‹åˆ’çº¿
```python
# âœ… æ­£ç¡®
backend/infrastructure/cache_manager/manager.py

# âŒ é”™è¯¯
backend/graphrag_agent/CacheManager/SessionCache.py
```

**ç±»å**ï¼šå¤§é©¼å³°ï¼ˆPascalCaseï¼‰
```python
# âœ… æ­£ç¡®
class EntityExtractor:
    pass

# âŒ é”™è¯¯
class entity_extractor:
    pass
```

**å‡½æ•°å’Œå˜é‡**ï¼šå°å†™+ä¸‹åˆ’çº¿
```python
# âœ… æ­£ç¡®
def extract_entities(chunk_text: str):
    entity_count = 0

# âŒ é”™è¯¯
def ExtractEntities(chunkText: str):
    EntityCount = 0
```

**å¸¸é‡**ï¼šå…¨å¤§å†™+ä¸‹åˆ’çº¿
```python
# âœ… æ­£ç¡®
MAX_WORKERS = 4
CHUNK_SIZE = 512

# âŒ é”™è¯¯
maxWorkers = 4
chunkSize = 512
```

### 2. ç±»å‹æ³¨è§£

**å¿…é¡»ä½¿ç”¨ç±»å‹æ³¨è§£**ï¼š

```python
from typing import List, Dict, Optional, Any

# âœ… æ­£ç¡®
def extract_entities(
    chunk_text: str,
    enable_cache: bool = True
) -> Dict[str, List[str]]:
    """æå–å®ä½“"""
    pass

# âŒ é”™è¯¯
def extract_entities(chunk_text, enable_cache=True):
    pass
```

### 3. æ–‡æ¡£å­—ç¬¦ä¸²

**ä½¿ç”¨ Google é£æ ¼**ï¼š

```python
def hybrid_search(
    query: str,
    top_k_entities: int = 5,
    top_k_chunks: int = 3
) -> str:
    """æ‰§è¡Œæ··åˆæœç´¢

    Args:
        query: ç”¨æˆ·æŸ¥è¯¢
        top_k_entities: å®ä½“æ£€ç´¢æ•°é‡
        top_k_chunks: æ–‡æœ¬å—æ£€ç´¢æ•°é‡

    Returns:
        ç”Ÿæˆçš„ç­”æ¡ˆå­—ç¬¦ä¸²

    Raises:
        ValueError: å½“ top_k_entities < 1 æ—¶

    Example:
        >>> answer = hybrid_search("å›½å®¶å¥–å­¦é‡‘çš„æ¡ä»¶", top_k_entities=10)
        >>> print(answer)
        "å›½å®¶å¥–å­¦é‡‘çš„ç”³è¯·æ¡ä»¶æ˜¯..."
    """
    pass
```

### 4. å¯¼å…¥é¡ºåº

```python
# 1. æ ‡å‡†åº“
import os
import sys
from typing import List, Dict

# 2. ç¬¬ä¸‰æ–¹åº“
import neo4j
from langchain_core.tools import Tool
from langchain_openai import ChatOpenAI

# 3. æœ¬åœ°æ¨¡å—
from graphrag_agent.config import settings
from infrastructure.models import get_llm_model
```

---

## è°ƒè¯•æŠ€å·§

### 1. å¯ç”¨è°ƒè¯•æ—¥å¿—

```python
# backend/graphrag_agent/config/settings.py

import logging

# è®¾ç½®æ—¥å¿—çº§åˆ«
logging.basicConfig(
    level=logging.DEBUG,  # INFO / DEBUG / WARNING
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# ä½¿ç”¨ç¤ºä¾‹
logger = logging.getLogger(__name__)

def extract_entities(chunk_text: str):
    logger.debug(f"å¼€å§‹æå–å®ä½“ï¼Œæ–‡æœ¬é•¿åº¦: {len(chunk_text)}")

    result = llm.invoke(chunk_text)

    logger.debug(f"æå–ç»“æœ: {result}")

    return result
```

### 2. è¿½è¸ª LangGraph æ‰§è¡Œ

```python
# å¯ç”¨ LangSmith è¿½è¸ª
import os
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_API_KEY"] = "your_api_key"

# æˆ–ä½¿ç”¨æœ¬åœ°è¿½è¸ª
from langchain.callbacks import StdOutCallbackHandler

agent = HybridAgent()
answer = agent.ask(
    "å›½å®¶å¥–å­¦é‡‘çš„æ¡ä»¶",
    callbacks=[StdOutCallbackHandler()]
)
```

### 3. æ€§èƒ½åˆ†æ

```python
import cProfile
import pstats

def profile_function():
    """æ€§èƒ½åˆ†æ"""
    profiler = cProfile.Profile()
    profiler.enable()

    # æ‰§è¡Œä»£ç 
    agent = HybridAgent()
    agent.ask("å›½å®¶å¥–å­¦é‡‘çš„æ¡ä»¶")

    profiler.disable()

    # è¾“å‡ºç»Ÿè®¡
    stats = pstats.Stats(profiler)
    stats.sort_stats('cumulative')
    stats.print_stats(20)  # æ‰“å°å‰ 20 ä¸ªæœ€è€—æ—¶çš„å‡½æ•°
```

### 4. æ–­ç‚¹è°ƒè¯•

```python
# ä½¿ç”¨ pdb
import pdb

def extract_entities(chunk_text: str):
    # è®¾ç½®æ–­ç‚¹
    pdb.set_trace()

    result = llm.invoke(chunk_text)
    return result

# æˆ–ä½¿ç”¨ IPython
from IPython import embed

def extract_entities(chunk_text: str):
    # è¿›å…¥äº¤äº’å¼ shell
    embed()

    result = llm.invoke(chunk_text)
    return result
```

---

## å¸¸è§é™·é˜±

### 1. å¾ªç¯å¯¼å…¥

**é—®é¢˜**ï¼š
```python
# backend/graphrag_agent/agents/base.py
from graphrag_agent.search.tool import HybridSearchTool  # âŒ

# backend/graphrag_agent/search/tool/hybrid_tool.py
from graphrag_agent.agents.base import BaseAgent  # âŒ
```

**è§£å†³æ–¹æ¡ˆ**ï¼šä½¿ç”¨å»¶è¿Ÿå¯¼å…¥
```python
# backend/graphrag_agent/agents/base.py
def _setup_tools(self):
    from graphrag_agent.search.tool import HybridSearchTool  # âœ…
    return [HybridSearchTool()]
```

### 2. ç¼“å­˜é”®å†²çª

**é—®é¢˜**ï¼š
```python
# ä¸¤ä¸ªä¸åŒæŸ¥è¯¢ç”Ÿæˆç›¸åŒç¼“å­˜é”®
cache_key1 = hash("å›½å®¶å¥–å­¦é‡‘")  # å¯èƒ½å†²çª
cache_key2 = hash("å›½å®¶å¥–å­¦é‡‘")
```

**è§£å†³æ–¹æ¡ˆ**ï¼šä½¿ç”¨æ›´å¥å£®çš„é”®ç”Ÿæˆ
```python
import hashlib

def generate_cache_key(query: str, session_id: str) -> str:
    """ç”Ÿæˆå”¯ä¸€ç¼“å­˜é”®"""
    content = f"{session_id}:{query}"
    return hashlib.sha256(content.encode()).hexdigest()
```

### 3. Neo4j è¿æ¥æ³„æ¼

**é—®é¢˜**ï¼š
```python
def query_graph():
    driver = GraphDatabase.driver(uri, auth=auth)
    session = driver.session()
    result = session.run("MATCH (n) RETURN n")
    # âŒ æœªå…³é—­ session å’Œ driver
    return result
```

**è§£å†³æ–¹æ¡ˆ**ï¼šä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨
```python
def query_graph():
    with GraphDatabase.driver(uri, auth=auth) as driver:
        with driver.session() as session:
            result = session.run("MATCH (n) RETURN n")
            return list(result)  # âœ… åœ¨ session å…³é—­å‰æ¶ˆè´¹ç»“æœ
```

### 4. LLM è°ƒç”¨æœªæ•è·å¼‚å¸¸

**é—®é¢˜**ï¼š
```python
def extract_entities(chunk_text: str):
    result = llm.invoke(chunk_text)  # âŒ å¯èƒ½æŠ›å‡ºå¼‚å¸¸
    return result
```

**è§£å†³æ–¹æ¡ˆ**ï¼šæ·»åŠ å¼‚å¸¸å¤„ç†
```python
def extract_entities(chunk_text: str):
    try:
        result = llm.invoke(chunk_text)
        return result
    except Exception as e:
        logger.error(f"å®ä½“æå–å¤±è´¥: {e}")
        return {"entities": [], "relationships": []}  # è¿”å›é»˜è®¤å€¼
```

---

## ç›¸å…³æ–‡æ¡£

- [å­˜å‚¨æ¨¡å‹è¯¦è§£](./å­˜å‚¨æ¨¡å‹è¯¦è§£.md) - Neo4j æ•°æ®æ¨¡å‹
- [æ€§èƒ½è°ƒä¼˜](./æ€§èƒ½è°ƒä¼˜.md) - ç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–
- [Agent ç³»ç»Ÿ](../02-æ ¸å¿ƒå­ç³»ç»Ÿ/Agentç³»ç»Ÿ.md) - Agent æ¶æ„è®¾è®¡
- [çŸ¥è¯†å›¾è°±æ„å»º](../02-æ ¸å¿ƒå­ç³»ç»Ÿ/çŸ¥è¯†å›¾è°±æ„å»º.md) - æ„å»ºæµç¨‹è¯¦è§£
- [æ·»åŠ æ–° Agent](../../04-å¼€å‘æŒ‡å—/æ·»åŠ æ–°Agent.md) - Agent æ‰©å±•æŒ‡å—

---

## æ›´æ–°æ—¥å¿—

| ç‰ˆæœ¬ | æ—¥æœŸ | æ›´æ–°å†…å®¹ | ä½œè€… |
|------|------|----------|------|
| 1.0 | 2026-01-04 | åˆå§‹ç‰ˆæœ¬ï¼Œå®Œæ•´è¦†ç›–ä»£ç èµ°è¯» | Claude |
| - | - | - | - |
