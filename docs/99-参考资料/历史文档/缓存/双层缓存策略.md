# 双层缓存策略

# ⚠️ 已废弃：双层缓存策略已下线

本文件描述的“双层缓存（session/global）+ CacheManager”在 v3 strict 阶段已**物理下线**，仅保留为历史参考。

当前系统不再通过缓存提速；如需提升体验，请优先通过：
- 检索 fanout/降级策略与聚合策略调优
- LLM 生成超时/取消与流式协议优化
- Postgres/监控/评测闭环驱动改进

> 本文是“缓存管理”的关键特性视角总结；实现细节请参考 `docs/99-参考资料/历史文档/缓存/缓存管理.md`。

## 背景

在 GraphRAG/DeepSearch 场景中，LLM 调用成本高、延迟不稳定，且重复/相似问题大量出现。双层缓存用于：

- 降低重复问答的成本与延迟
- 提升高频问题的稳定性
- 支持“近似命中”（向量相似度）而不仅是精确命中

## 两层含义（概念层）

1. **会话级（Session-aware）**：同一线程/会话中，重复提问或轻微改写更容易复用结果。
2. **全局级（Global）**：跨会话复用高质量答案或中间产物（视配置与质量策略而定）。

## 入口与配置

- 缓存管理器：`backend/infrastructure/cache_manager/manager.py`
- 关键配置：`backend/infrastructure/config/settings.py`（`CACHE_SETTINGS`）与 `.env.example`
